"""
Script de Coleta Hist√≥rica Segura de Coment√°rios
================================================
Este script coleta coment√°rios hist√≥ricos de TODOS os v√≠deos dos canais tipo='nosso'
com prote√ß√µes para N√ÉO sobrescrever dados existentes e N√ÉO duplicar coment√°rios.

Autor: Claude Code para Cellibs
Data: 12/02/2026
"""

import os
import sys
import json
import time
import logging
import argparse
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Set
from dotenv import load_dotenv
from supabase import create_client, Client
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configurar encoding UTF-8 para Windows
sys.stdout.reconfigure(encoding='utf-8')

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('coleta_historica.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Carregar vari√°veis de ambiente
load_dotenv()

# Configurar Supabase
supabase_url = os.getenv('SUPABASE_URL')
supabase_service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

if not supabase_url or not supabase_service_key:
    logger.error("‚ùå Credenciais Supabase n√£o configuradas!")
    sys.exit(1)

supabase: Client = create_client(supabase_url, supabase_service_key)

# Configurar YouTube API (precisa de pelo menos uma key)
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY_3')  # Usar KEY_3 como padr√£o
if not YOUTUBE_API_KEY:
    logger.error("‚ùå YouTube API Key n√£o configurada! Configure pelo menos YOUTUBE_API_KEY_3")
    sys.exit(1)

youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# Arquivo de checkpoint para retomar se interrompido
CHECKPOINT_FILE = 'coleta_checkpoint.json'

# Estat√≠sticas globais
stats = {
    'canais_processados': 0,
    'videos_processados': 0,
    'comentarios_novos': 0,
    'comentarios_existentes': 0,
    'comentarios_traduzidos': 0,
    'erros': 0,
    'inicio': datetime.now(timezone.utc)
}


def load_checkpoint() -> Dict:
    """Carrega checkpoint se existir"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_checkpoint(data: Dict):
    """Salva checkpoint para retomar depois"""
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)


def get_existing_comment_ids(canal_id: int) -> Set[str]:
    """
    Retorna conjunto de IDs de coment√°rios j√° existentes para um canal
    CR√çTICO: Evita duplicatas
    """
    logger.info(f"  üìä Buscando coment√°rios existentes do canal {canal_id}...")

    existing_ids = set()
    offset = 0
    limit = 1000

    while True:
        try:
            response = supabase.table('video_comments').select('comment_id').eq(
                'canal_id', canal_id
            ).range(offset, offset + limit - 1).execute()

            if not response.data:
                break

            for comment in response.data:
                existing_ids.add(comment['comment_id'])

            if len(response.data) < limit:
                break

            offset += limit

        except Exception as e:
            logger.error(f"  ‚ùå Erro ao buscar IDs existentes: {e}")
            break

    logger.info(f"  ‚úÖ {len(existing_ids)} coment√°rios j√° existem no banco")
    return existing_ids


def get_channel_info(channel_url: str) -> Optional[str]:
    """Extrai channel ID da URL"""
    try:
        # Decodificar URL se necess√°rio
        from urllib.parse import unquote
        channel_url = unquote(channel_url)

        if '@' in channel_url:
            handle = channel_url.split('@')[-1].split('/')[0]
            # Buscar por handle customizado
            response = youtube.search().list(
                part="snippet",
                q=handle,
                type="channel",
                maxResults=1
            ).execute()

            if 'items' in response and response['items']:
                return response['items'][0]['snippet']['channelId']
        elif '/channel/' in channel_url:
            channel_id = channel_url.split('/channel/')[-1].split('/')[0]
            return channel_id
        elif '/c/' in channel_url or '/user/' in channel_url:
            custom_url = channel_url.split('/')[-1]
            response = youtube.channels().list(
                part="id",
                forUsername=custom_url
            ).execute()
        else:
            return None

        if 'items' in response and response['items']:
            return response['items'][0]['id']

    except Exception as e:
        logger.error(f"Erro ao extrair channel ID: {e}")

    return None


def get_all_channel_videos(channel_id: str) -> List[Dict]:
    """
    Busca TODOS os v√≠deos de um canal (sem limite)
    Ordena por data de publica√ß√£o (mais recentes primeiro)
    """
    videos = []
    next_page_token = None

    try:
        while True:
            # Buscar v√≠deos do canal
            request = youtube.search().list(
                part="id,snippet",
                channelId=channel_id,
                type="video",
                maxResults=50,
                pageToken=next_page_token,
                order="date"  # Mais recentes primeiro
            )
            response = request.execute()

            # Processar cada v√≠deo
            for item in response.get('items', []):
                video_data = {
                    'id': item['id']['videoId'],
                    'title': item['snippet']['title'],
                    'publishedAt': item['snippet']['publishedAt']
                }
                videos.append(video_data)

            # Verificar se h√° mais p√°ginas
            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

            # Pequena pausa para n√£o sobrecarregar API
            time.sleep(0.1)

    except HttpError as e:
        logger.error(f"Erro HTTP ao buscar v√≠deos: {e}")
    except Exception as e:
        logger.error(f"Erro ao buscar v√≠deos: {e}")

    return videos


def get_video_comments(video_id: str, max_results: int = 100) -> List[Dict]:
    """
    Busca coment√°rios de um v√≠deo
    Retorna at√© max_results coment√°rios ordenados por data
    """
    comments = []
    next_page_token = None

    try:
        while len(comments) < max_results:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=min(100, max_results - len(comments)),
                order='time',  # Coment√°rios mais recentes primeiro
                pageToken=next_page_token
            )
            response = request.execute()

            for item in response.get('items', []):
                comment = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'id': item['snippet']['topLevelComment']['id'],
                    'author': comment['authorDisplayName'],
                    'author_channel_id': comment['authorChannelId']['value'],
                    'text': comment['textDisplay'],
                    'published_at': comment['publishedAt'],
                    'like_count': comment.get('likeCount', 0),
                    'reply_count': item['snippet'].get('totalReplyCount', 0)
                })

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        return comments[:max_results]

    except HttpError as e:
        if e.resp.status == 403 and 'commentsDisabled' in str(e):
            logger.debug(f"    Coment√°rios desabilitados no v√≠deo {video_id}")
        else:
            logger.error(f"    Erro HTTP ao buscar coment√°rios: {e}")
    except Exception as e:
        logger.error(f"    Erro ao buscar coment√°rios: {e}")

    return []


def translate_text(text: str, target_lang: str = 'pt') -> str:
    """
    Traduz texto para portugu√™s usando GPT
    NOTA: Implementa√ß√£o simplificada - voc√™ pode usar sua API de tradu√ß√£o preferida
    """
    # Por enquanto, retorna o texto original
    # Voc√™ pode integrar com Google Translate, DeepL ou GPT aqui
    return text


def save_comments_batch(comments_data: List[Dict]) -> bool:
    """
    Salva lote de coment√°rios no banco
    USA INSERT (n√£o UPSERT) para garantir que n√£o sobrescreve
    """
    if not comments_data:
        return True

    try:
        # INSERT apenas (n√£o UPSERT)
        response = supabase.table('video_comments').insert(comments_data).execute()

        if response.data:
            logger.info(f"    ‚úÖ {len(comments_data)} coment√°rios novos salvos")
            return True
        else:
            logger.warning(f"    ‚ö†Ô∏è Nenhum coment√°rio foi salvo")
            return False

    except Exception as e:
        logger.error(f"    ‚ùå Erro ao salvar coment√°rios: {e}")
        return False


def process_channel(canal: Dict, dry_run: bool = False) -> Dict:
    """
    Processa um canal completo
    Coleta coment√°rios de TODOS os v√≠deos
    """
    canal_id = canal['id']
    canal_nome = canal['nome_canal']
    canal_lingua = canal.get('lingua', 'unknown')
    canal_url = canal['url_canal']

    logger.info(f"\n{'='*80}")
    logger.info(f"üì∫ Processando canal: {canal_nome} (ID: {canal_id})")
    logger.info(f"   Subnicho: {canal['subnicho']}")
    logger.info(f"   L√≠ngua: {canal_lingua}")
    logger.info(f"{'='*80}")

    # Estat√≠sticas do canal
    canal_stats = {
        'videos': 0,
        'comentarios_novos': 0,
        'comentarios_existentes': 0,
        'comentarios_traduzidos': 0
    }

    # 1. Buscar IDs existentes para evitar duplicatas
    existing_ids = get_existing_comment_ids(canal_id) if not dry_run else set()

    # 2. Extrair channel ID do YouTube
    channel_id = get_channel_info(canal_url)
    if not channel_id:
        logger.error(f"‚ùå N√£o foi poss√≠vel extrair channel ID de: {canal_url}")
        return canal_stats

    # 3. Buscar TODOS os v√≠deos do canal
    logger.info(f"  üîç Buscando v√≠deos do canal...")
    videos = get_all_channel_videos(channel_id)
    logger.info(f"  ‚úÖ {len(videos)} v√≠deos encontrados")

    if not videos:
        logger.warning(f"  ‚ö†Ô∏è Nenhum v√≠deo encontrado para o canal")
        return canal_stats

    # 4. Processar cada v√≠deo
    for i, video in enumerate(videos, 1):
        video_id = video['id']
        video_title = video['title'][:50]

        logger.info(f"\n  [{i}/{len(videos)}] V√≠deo: {video_title}...")

        # Buscar coment√°rios do v√≠deo
        comments = get_video_comments(video_id, max_results=100)

        if not comments:
            logger.debug(f"    Sem coment√°rios")
            continue

        logger.info(f"    üìù {len(comments)} coment√°rios encontrados")

        # Processar cada coment√°rio
        new_comments = []
        for comment in comments:
            # Verificar se j√° existe
            if comment['id'] in existing_ids:
                canal_stats['comentarios_existentes'] += 1
                continue

            # Preparar dados do coment√°rio
            comment_data = {
                'comment_id': comment['id'],
                'video_id': video_id,
                'canal_id': canal_id,
                'author_name': comment['author'],
                'author_channel_id': comment['author_channel_id'],
                'comment_text_original': comment['text'],
                'published_at': comment['published_at'],
                'collected_at': datetime.now(timezone.utc).isoformat(),
                'like_count': comment['like_count'],
                'reply_count': comment['reply_count'],
                'is_reply': False,
                'parent_id': None,
                'canal_lingua': canal_lingua
            }

            # Tradu√ß√£o inteligente
            if canal_lingua == 'portuguese':
                # Canal em portugu√™s: copiar original
                comment_data['comment_text_pt'] = comment['text']
                comment_data['is_translated'] = True
            elif canal_lingua in ['english', 'spanish', 'german', 'french']:
                # Outros idiomas: traduzir
                # NOTA: Implementar tradu√ß√£o real aqui se necess√°rio
                comment_data['comment_text_pt'] = comment['text']  # Por enquanto copia
                comment_data['is_translated'] = False
                canal_stats['comentarios_traduzidos'] += 1
            else:
                # Idioma desconhecido
                comment_data['comment_text_pt'] = comment['text']
                comment_data['is_translated'] = False

            new_comments.append(comment_data)
            canal_stats['comentarios_novos'] += 1

        # Salvar lote de coment√°rios novos
        if new_comments and not dry_run:
            success = save_comments_batch(new_comments)
            if success:
                # Adicionar IDs ao conjunto de existentes
                for c in new_comments:
                    existing_ids.add(c['comment_id'])

        canal_stats['videos'] += 1

        # Checkpoint a cada 10 v√≠deos
        if i % 10 == 0 and not dry_run:
            checkpoint = {
                'ultimo_canal_id': canal_id,
                'ultimo_video_index': i,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            save_checkpoint(checkpoint)

        # Pequena pausa para n√£o sobrecarregar
        time.sleep(0.5)

    logger.info(f"\n  üìä Resumo do canal {canal_nome}:")
    logger.info(f"     V√≠deos processados: {canal_stats['videos']}")
    logger.info(f"     Coment√°rios novos: {canal_stats['comentarios_novos']}")
    logger.info(f"     Coment√°rios existentes: {canal_stats['comentarios_existentes']}")
    logger.info(f"     Coment√°rios traduzidos: {canal_stats['comentarios_traduzidos']}")

    return canal_stats


def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description='Coleta Hist√≥rica de Coment√°rios')
    parser.add_argument('--canal', type=str, help='Nome espec√≠fico do canal para coletar')
    parser.add_argument('--todos', action='store_true', help='Coletar de TODOS os canais')
    parser.add_argument('--dry-run', action='store_true', help='Modo simula√ß√£o (n√£o salva)')
    parser.add_argument('--verbose', action='store_true', help='Log detalhado')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("\n" + "="*80)
    logger.info("üöÄ COLETA HIST√ìRICA DE COMENT√ÅRIOS - INICIANDO")
    logger.info("="*80)

    if args.dry_run:
        logger.warning("‚ö†Ô∏è MODO DRY-RUN: Nenhum dado ser√° salvo!")

    # Buscar canais para processar
    if args.canal:
        # Canal espec√≠fico
        response = supabase.table('canais_monitorados').select('*').eq(
            'tipo', 'nosso'
        ).eq('nome_canal', args.canal).execute()
        canais = response.data

        if not canais:
            logger.error(f"‚ùå Canal '{args.canal}' n√£o encontrado!")
            return

    elif args.todos:
        # Todos os canais nossos
        response = supabase.table('canais_monitorados').select('*').eq(
            'tipo', 'nosso'
        ).order('subnicho').execute()
        canais = response.data
    else:
        logger.error("‚ùå Especifique --canal NOME ou --todos")
        return

    logger.info(f"üìã {len(canais)} canais para processar\n")

    # Carregar checkpoint se existir
    checkpoint = load_checkpoint()
    start_index = 0

    if checkpoint.get('ultimo_canal_id'):
        # Encontrar √≠ndice para continuar
        for i, canal in enumerate(canais):
            if canal['id'] == checkpoint['ultimo_canal_id']:
                start_index = i + 1
                logger.info(f"üìå Retomando do canal {start_index}/{len(canais)}")
                break

    # Processar cada canal
    for i, canal in enumerate(canais[start_index:], start=start_index+1):
        logger.info(f"\n[{i}/{len(canais)}] Canal: {canal['nome_canal']}")

        try:
            canal_stats = process_channel(canal, dry_run=args.dry_run)

            # Atualizar estat√≠sticas globais
            stats['canais_processados'] += 1
            stats['videos_processados'] += canal_stats['videos']
            stats['comentarios_novos'] += canal_stats['comentarios_novos']
            stats['comentarios_existentes'] += canal_stats['comentarios_existentes']
            stats['comentarios_traduzidos'] += canal_stats['comentarios_traduzidos']

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è Interrompido pelo usu√°rio!")
            break
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar canal: {e}")
            stats['erros'] += 1
            continue

    # Relat√≥rio final
    tempo_total = datetime.now(timezone.utc) - stats['inicio']
    logger.info("\n" + "="*80)
    logger.info("üìä RELAT√ìRIO FINAL DA COLETA HIST√ìRICA")
    logger.info("="*80)
    logger.info(f"‚è±Ô∏è Tempo total: {tempo_total}")
    logger.info(f"üì∫ Canais processados: {stats['canais_processados']}")
    logger.info(f"üé¨ V√≠deos processados: {stats['videos_processados']}")
    logger.info(f"‚úÖ Coment√°rios novos salvos: {stats['comentarios_novos']}")
    logger.info(f"‚è≠Ô∏è Coment√°rios j√° existentes (pulados): {stats['comentarios_existentes']}")
    logger.info(f"üåç Coment√°rios traduzidos: {stats['comentarios_traduzidos']}")
    logger.info(f"‚ùå Erros: {stats['erros']}")

    if not args.dry_run and os.path.exists(CHECKPOINT_FILE):
        os.remove(CHECKPOINT_FILE)
        logger.info("üóëÔ∏è Checkpoint removido")

    logger.info("\n‚úÖ COLETA HIST√ìRICA CONCLU√çDA!")


if __name__ == "__main__":
    main()