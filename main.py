from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from pydantic import BaseModel
import uvicorn
import os
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Dict, Any
import asyncio
import logging
import uuid
import threading
import time
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

from database import SupabaseClient
from collector import YouTubeCollector
from notifier import NotificationChecker
from comments_logs import CommentsLogsManager
from monetization_endpoints import router as monetization_router
from financeiro import FinanceiroService
from analytics import ChannelAnalytics

# Sistema de Agentes Inteligentes
from agents_endpoints import init_agents_router

# Sistema de Calend√°rio Empresarial
from calendar_endpoints import init_calendar_router

# YouTube Uploader
from _features.yt_uploader.uploader import YouTubeUploader
from _features.yt_uploader.database import (
    create_upload,
    update_upload_status,
    get_upload_by_id,
    supabase
)
from _features.yt_uploader.sheets import update_upload_status_in_sheet

# Daily Upload Automation
from daily_uploader import schedule_daily_uploader, SPREADSHEET_CACHE

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="YouTube Dashboard API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========================================
# üíæ SISTEMA DE CACHE 24H PARA DASHBOARD
# ========================================
import hashlib
from functools import wraps
import json

# Cache global com TTL de 6 horas
dashboard_cache = {}
CACHE_DURATION = timedelta(hours=6)  # Cache de 6 horas (atualiza 4x por dia)

# Cache espec√≠fico para coment√°rios - 5 minutos (atualiza frequentemente)
comments_cache = {}
COMMENTS_CACHE_DURATION = timedelta(minutes=5)

def get_cache_key(endpoint: str, params: dict = None) -> str:
    """
    Gera chave √∫nica para o cache baseada no endpoint e par√¢metros.

    Args:
        endpoint: Nome do endpoint
        params: Par√¢metros da requisi√ß√£o

    Returns:
        Hash MD5 √∫nico para identificar o cache
    """
    params = params or {}
    # Ordenar par√¢metros para garantir consist√™ncia
    param_str = json.dumps(sorted(params.items()) if params else [], sort_keys=True)
    cache_str = f"{endpoint}:{param_str}"
    return hashlib.md5(cache_str.encode()).hexdigest()

def get_cached_response(cache_key: str) -> Optional[Any]:
    """
    Busca resposta no cache se ainda v√°lida.

    Args:
        cache_key: Chave do cache

    Returns:
        Dados do cache ou None se expirado/inexistente
    """
    if cache_key in dashboard_cache:
        cached_data, cached_time = dashboard_cache[cache_key]
        now = datetime.now(timezone.utc)

        # Verificar se cache ainda √© v√°lido (24h)
        if now - cached_time < CACHE_DURATION:
            age_minutes = int((now - cached_time).total_seconds() / 60)
            logger.info(f"‚ö° Cache hit! Servindo instant√¢neo (idade: {age_minutes} min)")
            return cached_data
        else:
            # Cache expirado, remover
            del dashboard_cache[cache_key]
            logger.info(f"‚è∞ Cache expirado (> 24h), buscando novo...")

    return None

def save_to_cache(cache_key: str, data: Any) -> None:
    """
    Salva dados no cache com timestamp.

    Args:
        cache_key: Chave do cache
        data: Dados a serem cacheados
    """
    dashboard_cache[cache_key] = (data, datetime.now(timezone.utc))
    logger.info(f"üíæ Dados salvos no cache por 24h (key: {cache_key[:8]}...)")

def clear_all_cache() -> dict:
    """
    Limpa todo o cache do dashboard.
    Chamado ap√≥s coleta di√°ria √†s 5h.

    Returns:
        Estat√≠sticas do cache limpo
    """
    cache_count = len(dashboard_cache)
    cache_size = sum(len(str(v[0])) for v in dashboard_cache.values())
    dashboard_cache.clear()

    logger.info(f"üßπ Cache limpo: {cache_count} entradas, ~{cache_size/1024:.1f}KB liberados")
    return {
        "entries_cleared": cache_count,
        "approx_size_kb": round(cache_size/1024, 1)
    }

def cache_endpoint(endpoint_name: str):
    """
    Decorator para adicionar cache autom√°tico a endpoints.

    Args:
        endpoint_name: Nome do endpoint para logging

    Usage:
        @cache_endpoint("canais")
        async def get_canais(...):
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Gerar chave do cache baseada nos par√¢metros
            cache_params = {k: str(v) for k, v in kwargs.items() if v is not None}
            cache_key = get_cache_key(endpoint_name, cache_params)

            # Tentar buscar do cache
            cached_data = get_cached_response(cache_key)
            if cached_data is not None:
                return cached_data

            # Cache miss - buscar dados frescos
            logger.info(f"üìä Cache miss para {endpoint_name} - buscando dados...")
            start_time = time.time()

            # Executar fun√ß√£o original
            result = await func(*args, **kwargs)

            # Salvar no cache
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"‚úÖ Dados obtidos em {elapsed_ms}ms")
            save_to_cache(cache_key, result)

            return result
        return wrapper
    return decorator

# ========================================
# üîí UPLOAD CONCURRENCY CONTROL
# ========================================
# Sem√°foro: m√°ximo 3 uploads simult√¢neos (protege Railway de overload)
upload_semaphore = asyncio.Semaphore(3)

# ========================================
# üí∞ MONETIZATION ROUTER
# ========================================
app.include_router(monetization_router)

# ========================================
# üÜï MODELOS PYDANTIC
# ========================================

class RegraNotificacaoCreate(BaseModel):
    nome_regra: str
    views_minimas: int
    periodo_dias: int
    tipo_canal: str = "ambos"
    subnichos: Optional[List[str]] = None
    ativa: bool = True

# ========================================
# MODELOS DO SISTEMA KANBAN
# ========================================

class KanbanMoveStatusRequest(BaseModel):
    new_status: str

class KanbanNoteRequest(BaseModel):
    note_text: str
    note_color: str = "yellow"
    coluna_id: Optional[str] = None  # Campo para permitir notas em qualquer coluna

class KanbanNoteUpdateRequest(BaseModel):
    note_text: Optional[str] = None
    note_color: Optional[str] = None
    coluna_id: Optional[str] = None  # Permite mover nota entre colunas

class KanbanReorderNotesRequest(BaseModel):
    note_positions: List[Dict[str, int]]

class KanbanMoveNoteRequest(BaseModel):
    """Request para mover uma nota para outra coluna"""
    stage_id: Optional[str] = None  # Compatibilidade com Lovable
    coluna_id: Optional[str] = None  # Nome usado no backend

    @property
    def target_column(self) -> Optional[str]:
        """Retorna o ID da coluna de destino (aceita ambos os nomes)"""
        return self.stage_id or self.coluna_id

# ========================================
# INICIALIZA√á√ÉO
# ========================================

db = SupabaseClient()
collector = YouTubeCollector()
notifier = NotificationChecker(db.supabase)
financeiro = FinanceiroService(db)
uploader = YouTubeUploader()

# Sistema de Agentes Inteligentes
try:
    agents_router = init_agents_router(db, collector)
    app.include_router(agents_router)
    logger.info("Sistema de Agentes Inteligentes inicializado com sucesso!")
except Exception as e:
    logger.warning(f"Sistema de Agentes nao inicializado: {e}")

# Sistema de Calend√°rio Empresarial
try:
    calendar_router = init_calendar_router(db)
    app.include_router(calendar_router)
    logger.info("‚úÖ Sistema de Calend√°rio Empresarial inicializado com sucesso!")
except Exception as e:
    logger.warning(f"‚ùå Sistema de Calend√°rio n√£o inicializado: {e}")

collection_in_progress = False
last_collection_time = None

# ========================================
# SISTEMA DE JOBS ASS√çNCRONOS
# ========================================

transcription_jobs: Dict[str, Dict[str, Any]] = {}
jobs_lock = threading.Lock()

def cleanup_old_jobs():
    """Remove jobs com mais de 1 hora"""
    with jobs_lock:
        now = datetime.now(timezone.utc)
        old_jobs = [
            job_id for job_id, job in transcription_jobs.items()
            if (now - job['created_at']).total_seconds() > 3600
        ]
        for job_id in old_jobs:
            logger.info(f"üßπ Removendo job antigo: {job_id}")
            del transcription_jobs[job_id]

def process_transcription_job(job_id: str, video_id: str):
    """Processa transcri√ß√£o usando servidor M5 local com polling"""
    try:
        logger.info(f"üé¨ [JOB {job_id}] Iniciando transcri√ß√£o: {video_id}")
        
        with jobs_lock:
            transcription_jobs[job_id]['status'] = 'processing'
            transcription_jobs[job_id]['message'] = 'Iniciando job no servidor M5...'
        
        import requests
        import time
        
        # PASSO 1: Criar job no M5
        logger.info(f"üì° [JOB {job_id}] Criando job no servidor M5...")
        
        response = requests.post(
            "https://transcription.2growai.com.br/transcribe",
            json={
                "video_id": video_id,
                "language": "en"
            },
            timeout=30  # S√≥ para criar o job
        )
        
        if response.status_code != 200:
            raise Exception(f"Servidor M5 retornou erro: {response.status_code}")
        
        data = response.json()
        m5_job_id = data.get('job_id')
        
        if not m5_job_id:
            raise Exception("Servidor M5 n√£o retornou job_id")
        
        logger.info(f"‚úÖ [JOB {job_id}] Job criado no M5: {m5_job_id}")
        
        # PASSO 2: Fazer polling at√© completar
        max_attempts = 360  # 30 minutos (360 * 5s)
        attempt = 0
        
        while attempt < max_attempts:
            time.sleep(5)  # Aguardar 5 segundos entre checks
            attempt += 1
            
            try:
                status_response = requests.get(
                    f"https://transcription.2growai.com.br/status/{m5_job_id}",
                    timeout=10
                )
                
                if status_response.status_code != 200:
                    continue
                
                status_data = status_response.json()
                m5_status = status_data.get('status')
                
                # Atualizar mensagem
                with jobs_lock:
                    transcription_jobs[job_id]['message'] = status_data.get('message', 'Processando...')
                
                logger.info(f"üìä [JOB {job_id}] Status M5: {m5_status} ({status_data.get('elapsed_seconds')}s)")
                
                # Verificar se completou
                if m5_status == 'completed':
                    result = status_data.get('result', {})
                    transcription = result.get('transcription', '')
                    
                    logger.info(f"‚úÖ [JOB {job_id}] Transcri√ß√£o completa: {len(transcription)} caracteres")
                    
                    # Salvar no cache
                    asyncio.run(db.save_transcription_cache(video_id, transcription))
                    
                    with jobs_lock:
                        transcription_jobs[job_id]['status'] = 'completed'
                        transcription_jobs[job_id]['message'] = 'Transcri√ß√£o conclu√≠da'
                        transcription_jobs[job_id]['result'] = {
                            'transcription': transcription,
                            'video_id': video_id
                        }
                        transcription_jobs[job_id]['completed_at'] = datetime.now(timezone.utc)
                    
                    logger.info(f"‚úÖ [JOB {job_id}] SUCESSO")
                    return
                
                # Verificar se falhou
                if m5_status == 'failed':
                    error_msg = status_data.get('error', 'Erro desconhecido no servidor M5')
                    raise Exception(error_msg)
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"‚ö†Ô∏è [JOB {job_id}] Erro no polling (tentativa {attempt}): {e}")
                continue
        
        # Timeout
        raise Exception(f"Timeout ap√≥s {max_attempts * 5} segundos aguardando servidor M5")
        
    except Exception as e:
        logger.error(f"‚ùå [JOB {job_id}] ERRO: {e}")
        
        with jobs_lock:
            transcription_jobs[job_id]['status'] = 'failed'
            transcription_jobs[job_id]['message'] = str(e)
            transcription_jobs[job_id]['error'] = str(e)
            transcription_jobs[job_id]['failed_at'] = datetime.now(timezone.utc)

# ========================================
# ENDPOINTS DE TRANSCRI√á√ÉO ASS√çNCRONA
# ========================================

@app.post("/api/transcribe")
async def transcribe_video_async(video_id: str):
    """Inicia transcri√ß√£o ass√≠ncrona - aceita query param"""
    try:
        logger.info(f"üé¨ Nova requisi√ß√£o de transcri√ß√£o: {video_id}")
        
        cleanup_old_jobs()
        
        # Verificar cache primeiro
        cached = await db.get_cached_transcription(video_id)
        if cached:
            logger.info(f"‚úÖ Cache hit para: {video_id}")
            return {
                "status": "completed",
                "from_cache": True,
                "result": {
                    "transcription": cached,
                    "video_id": video_id
                }
            }
        
        job_id = str(uuid.uuid4())
        
        with jobs_lock:
            transcription_jobs[job_id] = {
                'job_id': job_id,
                'video_id': video_id,
                'status': 'queued',
                'message': 'Iniciando processamento...',
                'created_at': datetime.now(timezone.utc),
                'result': None,
                'error': None
            }
        
        thread = threading.Thread(
            target=process_transcription_job,
            args=(job_id, video_id),
            daemon=True
        )
        thread.start()
        
        logger.info(f"üöÄ Job criado: {job_id} para v√≠deo {video_id}")
        
        return {
            "status": "processing",
            "job_id": job_id,
            "video_id": video_id,
            "message": "Transcri√ß√£o iniciada. Use /api/transcribe/status/{job_id} para verificar progresso."
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar job de transcri√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/transcribe/status/{job_id}")
async def get_transcription_status(job_id: str):
    """Verifica status do job de transcri√ß√£o"""
    try:
        with jobs_lock:
            if job_id not in transcription_jobs:
                raise HTTPException(
                    status_code=404, 
                    detail="Job n√£o encontrado. Pode ter expirado (>1h) ou n√£o existir."
                )
            
            job = transcription_jobs[job_id]
        
        elapsed = (datetime.now(timezone.utc) - job['created_at']).total_seconds()
        
        response = {
            "job_id": job['job_id'],
            "video_id": job['video_id'],
            "status": job['status'],
            "message": job['message'],
            "elapsed_seconds": int(elapsed)
        }
        
        if job['status'] == 'completed':
            response['result'] = job['result']
            response['completed_at'] = job['completed_at'].isoformat()
        
        if job['status'] == 'failed':
            response['error'] = job['error']
            response['failed_at'] = job['failed_at'].isoformat()
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar status do job: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/transcribe/jobs")
async def list_active_jobs():
    """Lista todos os jobs ativos"""
    try:
        with jobs_lock:
            jobs_list = []
            for job_id, job in transcription_jobs.items():
                jobs_list.append({
                    'job_id': job['job_id'],
                    'video_id': job['video_id'],
                    'status': job['status'],
                    'created_at': job['created_at'].isoformat(),
                    'elapsed_seconds': int((datetime.now(timezone.utc) - job['created_at']).total_seconds())
                })
        
        return {
            "total_jobs": len(jobs_list),
            "jobs": jobs_list
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao listar jobs: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========================================
# ENDPOINTS ORIGINAIS
# ========================================

@app.get("/")
async def root():
    return {"message": "YouTube Dashboard API is running", "status": "healthy", "version": "1.0"}

@app.get("/health")
async def health_check():
    try:
        await db.test_connection()
        
        quota_usada = await db.get_quota_diaria_usada()
        
        return {
            "status": "healthy", 
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "supabase": "connected",
            "youtube_api": "configured",
            "collection_in_progress": collection_in_progress,
            "last_collection": last_collection_time.isoformat() if last_collection_time else None,
            "quota_usada_hoje": quota_usada,
            "active_transcription_jobs": len(transcription_jobs)
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service unhealthy")

@app.get("/api/canais")
async def get_canais(
    nicho: Optional[str] = None,
    subnicho: Optional[str] = None,
    lingua: Optional[str] = None,
    tipo: Optional[str] = None,
    views_30d_min: Optional[int] = None,
    views_15d_min: Optional[int] = None,
    views_7d_min: Optional[int] = None,
    score_min: Optional[float] = None,
    growth_min: Optional[float] = None,
    limit: Optional[int] = 500,
    offset: Optional[int] = 0
):
    try:
        # Para requests simples (s√≥ tipo/subnicho/lingua), usar cache + MV
        if (not views_30d_min and not views_15d_min and not views_7d_min and
            not score_min and not growth_min and not nicho):

            # Gerar chave do cache
            cache_key = get_cache_key("canais", {
                "tipo": tipo,
                "subnicho": subnicho,
                "lingua": lingua,
                "limit": limit,
                "offset": offset
            })

            # Tentar buscar do cache
            cached_data = get_cached_response(cache_key)
            if cached_data is not None:
                return cached_data

            # Cache miss - buscar da MV otimizada
            logger.info(f"üìä Buscando dados da MV para /api/canais...")
            start_time = time.time()

            # Usar nova fun√ß√£o otimizada com MV
            canais = await db.get_dashboard_from_mv(
                tipo=tipo,
                subnicho=subnicho,
                lingua=lingua,
                limit=limit,
                offset=offset
            )

            result = {"canais": canais, "total": len(canais)}

            # Salvar no cache
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"‚úÖ Dados obtidos em {elapsed_ms}ms")
            save_to_cache(cache_key, result)

            return result

        else:
            # Para filtros complexos, usar m√©todo tradicional (sem cache por enquanto)
            logger.info("üîç Filtros complexos detectados, usando m√©todo tradicional...")
            canais = await db.get_canais_with_filters(
                nicho=nicho,
                subnicho=subnicho,
                lingua=lingua,
                tipo=tipo,
                views_30d_min=views_30d_min,
                views_15d_min=views_15d_min,
                views_7d_min=views_7d_min,
                score_min=score_min,
                growth_min=growth_min,
                limit=limit,
                offset=offset
            )
            return {"canais": canais, "total": len(canais)}

    except Exception as e:
        logger.error(f"Error fetching canais: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/nossos-canais")
async def get_nossos_canais(
    nicho: Optional[str] = None,
    subnicho: Optional[str] = None,
    lingua: Optional[str] = None,
    views_30d_min: Optional[int] = None,
    views_15d_min: Optional[int] = None,
    views_7d_min: Optional[int] = None,
    score_min: Optional[float] = None,
    growth_min: Optional[float] = None,
    limit: Optional[int] = 100,
    offset: Optional[int] = 0
):
    try:
        canais = await db.get_canais_with_filters(
            nicho=nicho,
            subnicho=subnicho,
            lingua=lingua,
            tipo="nosso",
            views_30d_min=views_30d_min,
            views_15d_min=views_15d_min,
            views_7d_min=views_7d_min,
            score_min=score_min,
            growth_min=growth_min,
            limit=limit,
            offset=offset
        )
        return {"canais": canais, "total": len(canais)}
    except Exception as e:
        logger.error(f"Error fetching nossos canais: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/canais-tabela")
async def get_canais_tabela():
    """
    Retorna nossos canais agrupados por subnicho para aba Tabela.
    Canais ordenados por desempenho (maior ganho de inscritos no topo).
    Subnichos ordenados alfabeticamente.

    üöÄ OTIMIZADO: Usa cache de 24h + Materialized View
    """
    try:
        # Gerar chave do cache
        cache_key = get_cache_key("canais-tabela", {})

        # Tentar buscar do cache
        cached_data = get_cached_response(cache_key)
        if cached_data is not None:
            return cached_data

        # Cache miss - buscar dados
        logger.info("üìä Buscando canais para aba Tabela...")
        start_time = time.time()

        # Buscar todos os nossos canais usando MV otimizada
        canais = await db.get_dashboard_from_mv(
            tipo="nosso",
            limit=1000,
            offset=0
        )

        logger.info(f"Total de canais encontrados: {len(canais)}")

        # Agrupar por subnicho
        grupos = {}
        for canal in canais:
            subnicho = canal.get('subnicho') or 'Sem Categoria'

            if subnicho not in grupos:
                grupos[subnicho] = []

            # Adicionar canal ao grupo
            grupos[subnicho].append({
                'id': canal['id'],
                'nome_canal': canal['nome_canal'],
                'url_canal': canal['url_canal'],
                'inscritos': canal.get('inscritos', 0),
                'inscritos_diff': canal.get('inscritos_diff'),
                'ultima_coleta': canal.get('ultima_coleta'),
                'subnicho': subnicho,
                'lingua': canal.get('lingua', 'N/A')
            })

        # Ordenar canais dentro de cada grupo por desempenho
        # Ordem desejada: melhor (positivos) -> menor (negativos) -> zero -> nulo
        # Ordem secund√°ria: maior n√∫mero de inscritos (tiebreaker)
        def sort_key(canal):
            diff = canal['inscritos_diff']
            inscritos = canal['inscritos'] or 0  # FIX: None ‚Üí 0

            # Estrat√©gia de ordena√ß√£o:
            # 1. null por √∫ltimo (categoria 3)
            if diff is None:
                return (3, 0, -inscritos)

            # 2. zero em pen√∫ltimo (categoria 2)
            if diff == 0:
                return (2, 0, -inscritos)

            # 3. negativos antes do zero (categoria 1, ordenados por valor)
            if diff < 0:
                return (1, diff, -inscritos)  # diff negativo = menor primeiro

            # 4. positivos no topo (categoria 0, ordenados DESC)
            return (0, -diff, -inscritos)

        for subnicho in grupos:
            grupos[subnicho].sort(key=sort_key)

        # Ordenar subnichos alfabeticamente
        grupos_ordenados = dict(sorted(grupos.items()))

        logger.info(f"Canais agrupados em {len(grupos_ordenados)} subnichos")

        result = {
            "grupos": grupos_ordenados,
            "total_canais": len(canais),
            "total_subnichos": len(grupos_ordenados)
        }

        # Salvar no cache
        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(f"‚úÖ Dados processados em {elapsed_ms}ms")
        save_to_cache(cache_key, result)

        return result

    except Exception as e:
        logger.error(f"Error fetching canais tabela: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

# =========================================================================
# ENDPOINT DE DIAGN√ìSTICO - Canais Problem√°ticos
# Added by Claude Code - 2026-01-17
# =========================================================================

@app.get("/api/canais/problematicos")
async def get_canais_problematicos():
    """
    üîç Retorna canais com falhas de coleta.

    √ötil para diagn√≥stico e identifica√ß√£o de canais que precisam de aten√ß√£o.
    Ordenados por quantidade de falhas consecutivas (mais problem√°ticos primeiro).

    Returns:
        - total: n√∫mero de canais com problemas
        - canais: lista com detalhes de cada canal problem√°tico
            - id, nome_canal, url_canal, subnicho, tipo
            - coleta_falhas_consecutivas: quantas vezes consecutivas falhou
            - coleta_ultimo_erro: mensagem do √∫ltimo erro
            - coleta_ultimo_sucesso: √∫ltima coleta bem-sucedida
            - ultima_coleta: timestamp da √∫ltima tentativa
    """
    try:
        canais = await db.get_canais_problematicos()
        return {
            "total": len(canais),
            "canais": canais
        }
    except Exception as e:
        logger.error(f"Error fetching canais problem√°ticos: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/canais/sem-coleta-recente")
async def get_canais_sem_coleta_recente(dias: int = 3):
    """
    üîç Retorna canais que n√£o tiveram coleta bem-sucedida nos √∫ltimos X dias.

    Args:
        dias: N√∫mero de dias para considerar "sem coleta recente" (default: 3)

    Returns:
        - total: n√∫mero de canais sem coleta recente
        - canais: lista com detalhes de cada canal
    """
    try:
        canais = await db.get_canais_sem_coleta_recente(dias)
        return {
            "total": len(canais),
            "dias_limite": dias,
            "canais": canais
        }
    except Exception as e:
        logger.error(f"Error fetching canais sem coleta recente: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/canais/{canal_id}/analytics")
async def get_canal_analytics(canal_id: int):
    """
    üìä Retorna an√°lise completa de um canal com insights inteligentes.

    Inclui:
    - Informa√ß√µes b√°sicas expandidas (data cria√ß√£o, custom URL, etc)
    - M√©tricas de performance (views, engagement, score)
    - Top 10 v√≠deos com thumbnails
    - Padr√µes de sucesso identificados estatisticamente
    - Clustering de conte√∫do por tema/performance
    - Detec√ß√£o de anomalias (outliers, tend√™ncias)
    - Melhor dia/hora para postar

    Returns:
        JSON com an√°lise completa do canal
    """
    try:
        # Criar inst√¢ncia do analisador
        analyzer = ChannelAnalytics(db)

        # Executar an√°lise completa
        analytics_data = await analyzer.analyze_channel(canal_id)

        if not analytics_data:
            raise HTTPException(
                status_code=404,
                detail=f"Canal {canal_id} n√£o encontrado ou sem dados para an√°lise"
            )

        # Atualizar campos de analytics no banco se houver dados novos
        if analytics_data.get('canal_info'):
            info = analytics_data['canal_info']
            if info.get('criado_em') or info.get('custom_url'):
                await db.update_canal_analytics_fields(canal_id, {
                    'published_at': info.get('criado_em'),
                    'custom_url': info.get('custom_url'),
                    'video_count': info.get('total_videos')
                })

        # Atualizar melhor momento no banco
        if analytics_data.get('melhor_momento'):
            momento = analytics_data['melhor_momento']
            if momento.get('dia_numero') is not None and momento.get('hora') is not None:
                await db.supabase.table('canais_monitorados').update({
                    'melhor_dia_semana': momento['dia_numero'],
                    'melhor_hora': momento['hora']
                }).eq('id', canal_id).execute()

        logger.info(f"‚úÖ Analytics gerado para canal {canal_id}")

        return analytics_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao gerar analytics para canal {canal_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def safe_days_diff(date_str: str) -> int:
    """
    Calcula diferen√ßa em dias entre agora e uma data, com tratamento seguro de timezone.

    Args:
        date_str: String de data em formato ISO (com ou sem timezone)

    Returns:
        N√∫mero de dias de diferen√ßa ou 0 se houver erro
    """
    try:
        if not date_str:
            return 0

        # Garantir que temos datetime.now com timezone UTC
        now_utc = datetime.now(timezone.utc)

        # Tentar parsear a data
        if 'T' in date_str:
            # Formato ISO com tempo
            if date_str.endswith('Z'):
                # Z significa UTC - converter para +00:00
                date_str = date_str.replace('Z', '+00:00')

            # Parse da data
            target_date = datetime.fromisoformat(date_str)

            # Se n√£o tem timezone (naive), assumir UTC
            if target_date.tzinfo is None:
                target_date = target_date.replace(tzinfo=timezone.utc)

        else:
            # Formato sem tempo - assumir in√≠cio do dia em UTC
            target_date = datetime.fromisoformat(date_str + 'T00:00:00+00:00')

        # Calcular diferen√ßa
        delta = now_utc - target_date
        return delta.days

    except Exception as e:
        logger.debug(f"Erro ao calcular diferen√ßa de dias para '{date_str}': {e}")
        return 0


@app.get("/api/canais/{canal_id}/engagement")
async def get_canal_engagement(canal_id: int, page: int = 1, limit: int = 10):
    """
    üí¨ Retorna an√°lise completa de engajamento (coment√°rios) de um canal.

    APENAS para canais tipo="nosso" (canais pr√≥prios).

    Organizado por v√≠deo com:
    - Coment√°rios traduzidos para PT-BR
    - An√°lise de sentimento
    - Detec√ß√£o de problemas (√°udio, v√≠deo, conte√∫do)
    - Insights acion√°veis
    - Separa√ß√£o entre positivos e negativos

    Returns:
        JSON com an√°lise de coment√°rios organizada por v√≠deo
    """
    try:
        # ========== VALIDA√á√ÉO: APENAS CANAIS "NOSSOS" ==========
        canal_response = db.supabase.table("canais_monitorados")\
            .select("id, tipo, nome_canal")\
            .eq("id", canal_id)\
            .execute()

        if not canal_response.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        canal = canal_response.data[0]

        # Verificar se √© tipo="nosso"
        if canal.get('tipo') != 'nosso':
            raise HTTPException(
                status_code=403,
                detail="An√°lise de coment√°rios dispon√≠vel apenas para canais pr√≥prios. Este √© um canal minerado de refer√™ncia."
            )

        logger.info(f"‚úÖ Buscando engagement do canal pr√≥prio: {canal.get('nome_canal')} (ID: {canal_id})")

        # PRIMEIRO: Tentar buscar do cache
        from engagement_preprocessor import EngagementPreprocessor
        preprocessor = EngagementPreprocessor(db)
        engagement_data = await preprocessor.get_cached_engagement(canal_id)

        # Se n√£o tem cache v√°lido, buscar dados em tempo real
        if not engagement_data:
            logger.info(f"‚ö†Ô∏è Cache miss para canal {canal_id}, buscando dados em tempo real...")
            engagement_data = await db.get_canal_engagement_data(canal_id)
        else:
            logger.info(f"‚úÖ Usando cache para canal {canal_id} (processado em {engagement_data.get('_cache_metadata', {}).get('processing_time_ms', 0)}ms)")

        # Se n√£o h√° dados ainda, organizar resposta vazia estruturada
        if not engagement_data or engagement_data['summary']['total_comments'] == 0:

            # Buscar v√≠deos do canal para estruturar resposta
            videos = await db.get_videos_by_canal(canal_id, limit=20)

            videos_data = []
            for video in videos:
                videos_data.append({
                    'video_id': video.get('video_id'),
                    'video_title': video.get('titulo', ''),
                    'published_days_ago': safe_days_diff(video.get('data_publicacao', '')),
                    'views': video.get('views_atuais', 0),
                    'total_comments': 0,
                    'positive_count': 0,
                    'negative_count': 0,
                    'has_problems': False,
                    'problem_count': 0,
                    'sentiment_score': 0,
                    'positive_comments': [],
                    'negative_comments': []
                })

            return {
                'summary': {
                    'total_comments': 0,
                    'positive_count': 0,
                    'negative_count': 0,
                    'positive_pct': 0,
                    'negative_pct': 0,
                    'actionable_count': 0,
                    'problems_count': 0
                },
                'videos': videos_data[:10],  # Top 10 v√≠deos
                'problems_grouped': {
                    'audio': [],
                    'video': [],
                    'content': [],
                    'technical': []
                },
                'message': 'Ainda n√£o h√° coment√°rios coletados para este canal. Execute a coleta de coment√°rios para ver an√°lises.'
            }

        # Processar dados dos v√≠deos com pagina√ß√£o
        videos_list = engagement_data.get('videos_summary', [])

        # Buscar dados completos dos v√≠deos (views, t√≠tulo, data de publica√ß√£o)
        # Isso garante que temos as informa√ß√µes reais mesmo quando os coment√°rios n√£o trazem tudo
        video_ids = [v.get('video_id') for v in videos_list if v.get('video_id')]
        videos_from_db = []
        if video_ids:
            # Buscar v√≠deos do canal para obter views, t√≠tulo e data de publica√ß√£o
            # Removido limite para garantir que pegamos TODOS os v√≠deos
            videos_from_db = await db.get_videos_by_canal(canal_id, limit=None)

        # Criar mapa de v√≠deos para acesso r√°pido (mapear TODOS, sem filtro)
        videos_map = {}
        for video in videos_from_db:
            video_id = video.get('video_id')
            if video_id:  # Apenas verificar se tem ID
                videos_map[video_id] = video

        # Aplicar pagina√ß√£o
        offset = (page - 1) * limit
        videos_paginated = videos_list[offset:offset + limit]

        # Formatar dados dos v√≠deos para o frontend
        videos_data = []
        for video_data in videos_paginated:
            # Obter dados reais do v√≠deo do mapa
            video_id = video_data.get('video_id')
            video_info = videos_map.get(video_id, {})

            # Separar coment√°rios positivos e negativos do v√≠deo
            video_comments = video_data.get('comments', [])

            # Garantir que cada coment√°rio tenha os campos obrigat√≥rios
            formatted_comments = []
            for comment in video_comments:
                # Priorizar texto traduzido, depois original
                comment_text = (
                    comment.get('comment_text_pt') or  # Primeiro: tradu√ß√£o
                    comment.get('comment_text_original') or  # Segundo: original
                    comment.get('comment_text', '') or  # Terceiro: fallback antigo
                    ''  # √öltimo: string vazia
                )

                formatted_comment = {
                    'comment_id': comment.get('comment_id', ''),
                    'author_name': comment.get('author_name', ''),
                    'comment_text_pt': comment_text,  # Sempre enviar texto (traduzido ou original)
                    'comment_text_original': comment.get('comment_text_original', ''),
                    'is_translated': comment.get('is_translated', False),
                    'like_count': comment.get('like_count', 0),
                    'insight_text': comment.get('insight_text', ''),
                    'suggested_action': comment.get('suggested_action'),
                    'sentiment_category': comment.get('sentiment_category', ''),
                    'suggested_response': comment.get('suggested_response', '')  # Resposta sugerida do banco
                }
                formatted_comments.append(formatted_comment)

            # Ordenar todos os coment√°rios por like_count (mais likes primeiro)
            formatted_comments.sort(key=lambda x: x.get('like_count', 0), reverse=True)

            # Arrays vazios para compatibilidade (Lovable pode estar esperando)
            positive_comments = []
            negative_comments = []
            neutral_comments = []

            # Log para debug
            if video_comments:
                logger.info(f"üîç Engagement - Video {video_id}: {len(video_comments)} coment√°rios totais")

            # Garantir que sempre tem um t√≠tulo v√°lido
            # Prioridade: 1) videos_historico, 2) video_comments (se houver), 3) fallback gen√©rico
            video_title = video_info.get('titulo', '').strip()

            if not video_title and video_comments:
                # Tentar buscar t√≠tulo do primeiro coment√°rio (que tem video_title)
                first_comment_with_title = next((c for c in video_comments if c.get('video_title')), None)
                if first_comment_with_title:
                    video_title = first_comment_with_title.get('video_title', '').strip()

            if not video_title:
                video_title = video_data.get('video_title', '').strip()

            if not video_title and video_id:
                video_title = f"V√≠deo {video_id[:8]}..."

            if not video_title:
                video_title = "V√≠deo sem t√≠tulo"

            # UNIFICA√á√ÉO DE CONTAGENS: Buscar contagem do YouTube para compara√ß√£o
            youtube_comment_count = video_info.get('comentarios', 0)  # Da tabela videos_historico
            analyzed_comment_count = len(formatted_comments)  # Coment√°rios que analisamos
            coverage_pct = (analyzed_comment_count / youtube_comment_count * 100) if youtube_comment_count > 0 else 0

            videos_data.append({
                'video_id': video_id,
                'video_title': video_title,  # Sempre ter√° um t√≠tulo
                'published_days_ago': safe_days_diff(video_info.get('data_publicacao', '')),  # Calcula dias desde publica√ß√£o
                'views': video_info.get('views_atuais', 0),  # Views reais do banco
                'total_comments': video_data.get('total_comments', 0),  # Mantido para compatibilidade
                # NOVO: Campos unificados de contagem
                'total_comments_youtube': youtube_comment_count,  # Contagem do YouTube (videos_historico)
                'total_comments_analyzed': analyzed_comment_count,  # Contagem analisada (video_comments)
                'coverage_pct': round(coverage_pct, 1),  # Porcentagem de cobertura
                'positive_count': 0,  # Removido an√°lise de sentimentos
                'negative_count': 0,  # Removido an√°lise de sentimentos
                'has_problems': False,  # Removido an√°lise de sentimentos
                'problem_count': 0,  # Removido an√°lise de sentimentos
                'sentiment_score': 0,  # Removido an√°lise de sentimentos
                # Arrays para compatibilidade com frontend existente
                'positive_comments': positive_comments,  # Array vazio
                'negative_comments': negative_comments,  # Array vazio
                'neutral_comments': neutral_comments,  # Array vazio
                # NOVO: Array √∫nico com TODOS os coment√°rios
                'all_comments': formatted_comments  # TODOS os coment√°rios do v√≠deo (sem limite)
            })

        # Agrupar problemas por tipo (usando coment√°rios com problema)
        problems_grouped = {
            'audio': [],
            'video': [],
            'content': [],
            'technical': []
        }

        # Contadores por tipo de problema
        actionable_breakdown = {
            'audio': 0,
            'video': 0,
            'content': 0,
            'technical': 0,
            'other': 0
        }

        # V√≠deos que precisam de a√ß√£o (com problemas)
        videos_needing_action = set()

        for comment in engagement_data.get('problem_comments', []):
            problem_type = comment.get('problem_type', 'other')
            video_title = comment.get('video_title', '')

            # Adicionar v√≠deo √† lista de a√ß√£o necess√°ria
            if video_title:
                videos_needing_action.add(video_title)

            # Contar por tipo
            if problem_type in actionable_breakdown:
                actionable_breakdown[problem_type] += 1
            else:
                actionable_breakdown['other'] += 1

            # Adicionar ao grupo apropriado
            if problem_type in problems_grouped:
                problems_grouped[problem_type].append({
                    'video_title': video_title,
                    'author': comment.get('author_name', ''),
                    'text_pt': comment.get('comment_text_pt', comment.get('comment_text', '')),
                    'specific_issue': comment.get('problem_description', ''),
                    'suggested_action': comment.get('suggested_action', '')
                })

        # Calcular total de p√°ginas
        total_videos = len(engagement_data.get('videos_summary', []))
        total_pages = (total_videos + limit - 1) // limit

        # Calcular totais de cobertura para o summary
        total_youtube_comments = sum(v['total_comments_youtube'] for v in videos_data)
        total_analyzed_comments = sum(v['total_comments_analyzed'] for v in videos_data)
        overall_coverage = (total_analyzed_comments / total_youtube_comments * 100) if total_youtube_comments > 0 else 0

        # Melhorar o summary com detalhes de actionable e cobertura
        enhanced_summary = engagement_data['summary'].copy()
        enhanced_summary['actionable_breakdown'] = actionable_breakdown
        enhanced_summary['videos_needing_action'] = list(videos_needing_action)
        enhanced_summary['videos_needing_action_count'] = len(videos_needing_action)
        # NOVO: Campos de cobertura unificados
        enhanced_summary['total_comments_youtube'] = total_youtube_comments
        enhanced_summary['total_comments_analyzed'] = total_analyzed_comments
        enhanced_summary['overall_coverage_pct'] = round(overall_coverage, 1)

        return {
            'summary': enhanced_summary,
            'videos': videos_data,  # V√≠deos paginados com coment√°rios
            'problems_grouped': problems_grouped,
            'pagination': {
                'page': page,
                'limit': limit,
                'total_videos': total_videos,
                'total_pages': total_pages
            }
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar engagement do canal {canal_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ======== ENDPOINTS DE COMENT√ÅRIOS PARA FRONTEND ========

@app.get("/api/comentarios/monetizados")
async def get_monetized_channels_comments():
    """
    Lista canais monetizados com estat√≠sticas de coment√°rios.
    OTIMIZADO: Cache de 5 minutos + queries agregadas

    Returns:
        Lista de canais com total de coment√°rios, v√≠deos e engagement
    """
    try:
        # Verificar cache
        cache_key = get_cache_key('comentarios_monetizados')
        if cache_key in comments_cache:
            cached_data, cached_time = comments_cache[cache_key]
            if datetime.now(timezone.utc) - cached_time < COMMENTS_CACHE_DURATION:
                logger.info("üì¶ Retornando canais monetizados do cache")
                return cached_data

        # Buscar dados (fun√ß√£o otimizada com apenas 3 queries)
        result = db.get_monetized_channels_with_comments()

        # Salvar no cache
        comments_cache[cache_key] = (result, datetime.now(timezone.utc))
        logger.info(f"üíæ Cache atualizado para canais monetizados: {len(result)} canais")

        return result
    except Exception as e:
        logger.error(f"Erro ao buscar canais monetizados: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/canais/{canal_id}/videos-com-comentarios")
async def get_canal_videos_with_comments(canal_id: int, limit: int = 50):
    """
    Lista v√≠deos de um canal com contagem de coment√°rios.
    OTIMIZADO: Cache de 5 minutos + query √∫nica

    Args:
        canal_id: ID do canal
        limit: N√∫mero m√°ximo de v√≠deos a retornar (padr√£o: 50)

    Returns:
        Lista de v√≠deos com estat√≠sticas de coment√°rios
    """
    try:
        # Verificar cache
        cache_key = get_cache_key('videos_comentarios', {'canal_id': canal_id, 'limit': limit})
        if cache_key in comments_cache:
            cached_data, cached_time = comments_cache[cache_key]
            if datetime.now(timezone.utc) - cached_time < COMMENTS_CACHE_DURATION:
                logger.info(f"üì¶ Retornando v√≠deos do canal {canal_id} do cache")
                return cached_data

        # Buscar dados (fun√ß√£o otimizada com apenas 2 queries)
        result = db.get_videos_with_comments_count(canal_id, limit)

        # Salvar no cache
        comments_cache[cache_key] = (result, datetime.now(timezone.utc))
        logger.info(f"üíæ Cache atualizado para v√≠deos do canal {canal_id}: {len(result)} v√≠deos")

        return result
    except Exception as e:
        logger.error(f"Erro ao buscar v√≠deos do canal {canal_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/videos/{video_id}/comentarios-paginados")
async def get_video_comments_paginated(video_id: str, page: int = 1, limit: int = 10):
    """
    Busca coment√°rios de um v√≠deo com pagina√ß√£o.

    Args:
        video_id: ID do v√≠deo no YouTube
        page: N√∫mero da p√°gina (padr√£o: 1)
        limit: Coment√°rios por p√°gina (padr√£o: 10)

    Returns:
        Coment√°rios paginados com sugest√µes de resposta
    """
    try:
        result = db.get_video_comments_paginated(video_id, page, limit)
        return result
    except Exception as e:
        logger.error(f"Erro ao buscar coment√°rios do v√≠deo {video_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.patch("/api/comentarios/{comment_id}/marcar-respondido")
async def mark_comment_responded(comment_id: int, body: dict = {}):
    """
    Marca um coment√°rio como respondido.

    Args:
        comment_id: ID do coment√°rio (database ID)
        body: JSON com 'actual_response' opcional

    Returns:
        Status da opera√ß√£o
    """
    try:
        actual_response = body.get('actual_response')
        success = db.mark_comment_as_responded(comment_id, actual_response)

        if success:
            return {"success": True, "message": "Coment√°rio marcado como respondido"}
        else:
            raise HTTPException(status_code=404, detail="Coment√°rio n√£o encontrado")
    except Exception as e:
        logger.error(f"Erro ao marcar coment√°rio {comment_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/comentarios/{comment_id}/gerar-resposta")
async def generate_comment_response(comment_id: int):
    """
    Gera resposta personalizada para um coment√°rio espec√≠fico.

    SISTEMA SIMPLIFICADO (03/02/2026):
    - Gera√ß√£o sob demanda
    - Resposta na mesma l√≠ngua do coment√°rio
    - Tom educado como dono do canal

    Args:
        comment_id: ID do coment√°rio (database ID)

    Returns:
        JSON com resposta sugerida
    """
    logger.info("="*80)
    logger.info("üö®üö®üö® ENDPOINT GERAR RESPOSTA CHAMADO! üö®üö®üö®")
    logger.info(f"Coment√°rio ID recebido: {comment_id}")
    logger.info(f"Tipo do ID: {type(comment_id)}")
    logger.info("="*80)

    try:
        # Buscar apenas o coment√°rio (sem joins desnecess√°rios)
        logger.info(f"Buscando coment√°rio {comment_id} no banco...")
        comment = db.supabase.table('video_comments').select(
            'id, author_name, comment_text_original, comment_text_pt'
        ).eq('id', comment_id).execute()

        if not comment.data:
            logger.error(f"‚ùå Coment√°rio {comment_id} n√£o encontrado no banco")
            raise HTTPException(status_code=404, detail=f"Coment√°rio {comment_id} n√£o encontrado")

        comment_data = comment.data[0]
        logger.info(f"‚úÖ Coment√°rio encontrado! Autor: {comment_data.get('author_name', 'N/A')}")

        # Pegar o texto do coment√°rio (preferir original para detectar idioma correto)
        comment_text = comment_data.get('comment_text_original') or comment_data.get('comment_text_pt')
        logger.info(f"üìù Texto do coment√°rio ({len(comment_text) if comment_text else 0} chars): {comment_text[:100] if comment_text else 'VAZIO'}...")

        if not comment_text or not comment_text.strip():
            logger.error("‚ùå Coment√°rio sem texto!")
            raise HTTPException(status_code=400, detail="Coment√°rio sem texto para an√°lise")

        # Verificar se OPENAI_API_KEY est√° configurada
        logger.info("üîë Verificando OPENAI_API_KEY...")
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("‚ùå OPENAI_API_KEY n√£o encontrada nas vari√°veis de ambiente")
            raise HTTPException(
                status_code=500,
                detail="OPENAI_API_KEY n√£o est√° configurada no Railway. Adicione em Settings > Variables"
            )

        # Sanitizar API key: remover espa√ßos, tabs, newlines, etc
        # Railway √†s vezes adiciona formata√ß√£o √†s vari√°veis de ambiente
        api_key = api_key.strip().replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
        logger.info(f"‚úÖ OPENAI_API_KEY encontrada e sanitizada: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else '***'}")

        # Prompt humanizado - resposta natural e genu√≠na com contexto
        prompt = f"""You're responding to a YouTube comment on your channel.

BE HUMAN AND NATURAL:
- Sometimes 1 sentence is perfect (for simple thanks or acknowledgments)
- Sometimes 2-3 sentences work better (for questions or discussions)
- NEVER force long responses - be genuine
- Match the commenter's energy and tone
- Use the EXACT SAME LANGUAGE as the comment

HOW TO RESPOND:
- If it's praise/compliment: thank them genuinely
- If it's criticism: acknowledge it and be understanding
- If it's a question: answer it directly
- If it's a suggestion: consider it and thank them
- If it's just "Thanks!": a simple "You're welcome!" is perfect

Comment from {comment_data.get('author_name', 'User')}:
"{comment_text}"

Your natural response:"""

        # Chamar OpenAI API diretamente via HTTP (igual aos agents que funcionam!)
        try:
            logger.info(f"üì§ Chamando OpenAI API diretamente via HTTP...")
            logger.info(f"   Modelo: gpt-4o-mini")
            logger.info(f"   Tamanho do coment√°rio: {len(comment_text)} caracteres")

            import requests

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "You are a YouTube channel owner responding to comments. IMPORTANT: Always respond in the EXACT SAME LANGUAGE as the comment. Be natural and human - sometimes 1 sentence is perfect, sometimes 2-3 sentences work better. Never force long responses. Be genuine, not robotic."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7
            }

            logger.info("üöÄ Fazendo requisi√ß√£o HTTP para api.openai.com...")
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30  # 30 segundos como voc√™ pediu
            )

            if response.status_code == 200:
                result = response.json()
                suggested_response = result["choices"][0]["message"]["content"].strip()
                logger.info(f"‚úÖ Resposta gerada com sucesso: {len(suggested_response)} caracteres")
            else:
                logger.error(f"‚ùå Erro da API OpenAI: Status {response.status_code}")
                logger.error(f"   Resposta: {response.text}")

                error_msg = f"Erro da OpenAI (Status {response.status_code})"
                if response.status_code == 401:
                    error_msg = "Chave da API inv√°lida ou expirada"
                elif response.status_code == 429:
                    error_msg = "Limite de requisi√ß√µes excedido. Tente em alguns segundos"
                elif response.status_code == 500:
                    error_msg = "Erro interno da OpenAI. Tente novamente"

                raise HTTPException(status_code=500, detail=error_msg)

        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout na chamada para OpenAI (30 segundos)")
            raise HTTPException(
                status_code=500,
                detail="Timeout na comunica√ß√£o com OpenAI. Tente novamente"
            )
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Erro de conex√£o com OpenAI: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Erro de conex√£o com OpenAI: {str(e)}"
            )
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado: {type(e).__name__}: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Erro ao gerar resposta: {str(e)}"
            )

        # Salvar resposta no banco
        db.supabase.table('video_comments').update({
            'suggested_response': suggested_response,
            'response_tone': 'friendly',
            'response_generated_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat()
        }).eq('id', comment_id).execute()

        return {
            "success": True,
            "suggested_response": suggested_response,
            "comment_text": comment_text,
            "response_generated_at": datetime.now(timezone.utc).isoformat()
        }

    except HTTPException as he:
        logger.error(f"‚ùå HTTPException capturada: {he.status_code} - {he.detail}")
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå‚ùå‚ùå ERRO INESPERADO NO ENDPOINT! ‚ùå‚ùå‚ùå")
        logger.error(f"Tipo: {type(e).__name__}")
        logger.error(f"Mensagem: {str(e)}")
        logger.error(f"Coment√°rio ID: {comment_id}")
        logger.error("="*80)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comentarios/resumo")
async def get_comments_summary():
    """
    Retorna resumo geral dos coment√°rios.

    Returns:
        Estat√≠sticas gerais dos coment√°rios
    """
    try:
        result = db.get_comments_summary()
        return result
    except Exception as e:
        logger.error(f"Erro ao buscar resumo de coment√°rios: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/test-openai")
async def test_openai_configuration():
    """
    Endpoint de diagn√≥stico para verificar configura√ß√£o da OpenAI.
    Use para debugar problemas com gera√ß√£o de respostas.
    """
    diagnostico = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "checks": {}
    }

    # 1. Verificar se OPENAI_API_KEY existe
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        diagnostico["checks"]["api_key_exists"] = False
        diagnostico["error"] = "OPENAI_API_KEY n√£o est√° configurada nas vari√°veis de ambiente"
        diagnostico["solution"] = "Adicione OPENAI_API_KEY no Railway em Settings > Variables"
        return diagnostico

    # Sanitizar API key: remover espa√ßos, tabs, newlines, etc
    api_key = api_key.strip().replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')

    diagnostico["checks"]["api_key_exists"] = True
    diagnostico["checks"]["api_key_format"] = f"{api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else '***'}"

    # 2. Verificar formato da chave
    if not api_key.startswith(("sk-", "sk-proj-")):
        diagnostico["checks"]["api_key_valid_format"] = False
        diagnostico["warning"] = "OPENAI_API_KEY n√£o come√ßa com 'sk-' ou 'sk-proj-'"
    else:
        diagnostico["checks"]["api_key_valid_format"] = True

    # 3. Tentar inicializar GPTAnalyzer
    try:
        from gpt_response_suggester import GPTAnalyzer
        analyzer = GPTAnalyzer()
        diagnostico["checks"]["gpt_analyzer_init"] = True
    except ImportError as e:
        diagnostico["checks"]["gpt_analyzer_init"] = False
        diagnostico["checks"]["import_error"] = str(e)
        diagnostico["error"] = "N√£o foi poss√≠vel importar GPTAnalyzer"
        return diagnostico
    except Exception as e:
        diagnostico["checks"]["gpt_analyzer_init"] = False
        diagnostico["checks"]["init_error"] = f"{type(e).__name__}: {str(e)}"
        diagnostico["error"] = f"Erro ao inicializar GPTAnalyzer: {str(e)}"
        return diagnostico

    # 4. Fazer uma chamada de teste simples
    try:
        response = analyzer.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "Responda apenas: OK"}
            ],
            max_tokens=10,
            temperature=0
        )
        test_response = response.choices[0].message.content.strip()
        diagnostico["checks"]["api_call_test"] = True
        diagnostico["checks"]["test_response"] = test_response
    except Exception as e:
        diagnostico["checks"]["api_call_test"] = False
        diagnostico["checks"]["api_error"] = f"{type(e).__name__}: {str(e)}"

        # Interpretar erro comum
        error_str = str(e).lower()
        if "api_key" in error_str or "authentication" in error_str:
            diagnostico["error"] = "Chave da API inv√°lida ou expirada"
            diagnostico["solution"] = "Verifique se a chave est√° correta e ativa em https://platform.openai.com/api-keys"
        elif "rate" in error_str:
            diagnostico["error"] = "Limite de rate da API excedido"
            diagnostico["solution"] = "Aguarde alguns segundos e tente novamente"
        elif "quota" in error_str or "insufficient" in error_str:
            diagnostico["error"] = "Quota da OpenAI excedida ou cr√©ditos insuficientes"
            diagnostico["solution"] = "Verifique seu saldo em https://platform.openai.com/usage"
        else:
            diagnostico["error"] = f"Erro na API: {str(e)}"

    # 5. Resumo final
    all_checks_passed = all(
        v for k, v in diagnostico["checks"].items()
        if isinstance(v, bool) and not k.startswith("api_key_format")
    )

    if all_checks_passed:
        diagnostico["status"] = "‚úÖ TUDO FUNCIONANDO"
        diagnostico["message"] = "OpenAI est√° configurada corretamente e funcionando"
    else:
        diagnostico["status"] = "‚ùå PROBLEMAS DETECTADOS"
        if "error" not in diagnostico:
            diagnostico["error"] = "Verifique os detalhes acima"

    return diagnostico


@app.post("/api/collect-comments/{canal_id}")
async def collect_canal_comments(canal_id: int, background_tasks: BackgroundTasks):
    """
    üí¨ Coleta coment√°rios de todos os v√≠deos de um canal.

    APENAS para canais tipo="nosso" (canais pr√≥prios).

    Processo:
    1. Valida se canal √© tipo="nosso"
    2. Busca √∫ltimos 20 v√≠deos do canal
    3. Coleta at√© 100 coment√°rios por v√≠deo
    4. Analisa sentimento e detecta problemas
    5. Salva no banco de dados

    Returns:
        JSON com resumo da coleta
    """
    try:
        # ========== VALIDA√á√ÉO: APENAS CANAIS "NOSSOS" ==========
        canal_response = db.supabase.table("canais_monitorados")\
            .select("id, tipo, nome_canal, url_canal")\
            .eq("id", canal_id)\
            .execute()

        if not canal_response.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        canal = canal_response.data[0]

        # Verificar se √© tipo="nosso"
        if canal.get('tipo') != 'nosso':
            logger.info(f"‚è≠Ô∏è Pulando coleta de coment√°rios - Canal minerado: {canal.get('nome_canal')} (ID: {canal_id})")
            raise HTTPException(
                status_code=403,
                detail="Coleta de coment√°rios dispon√≠vel apenas para canais pr√≥prios. Este √© um canal minerado de refer√™ncia."
            )

        logger.info(f"üéØ Iniciando coleta de coment√°rios do canal pr√≥prio: {canal.get('nome_canal')} (ID: {canal_id})")

        # Buscar v√≠deos do canal (TOP 20 mais recentes)
        videos_response = db.supabase.table("videos")\
            .select("video_id, titulo, views_atuais, data_publicacao")\
            .eq("canal_id", canal_id)\
            .order("data_publicacao", desc=True)\
            .limit(20)\
            .execute()

        logger.info(f"üìπ Encontrados {len(videos_response.data) if videos_response.data else 0} v√≠deos para coletar coment√°rios")

        if not videos_response.data:
            return {
                'success': True,
                'canal': canal.get('nome_canal'),
                'message': 'Canal n√£o possui v√≠deos para coletar coment√°rios',
                'total_videos': 0,
                'total_comments': 0
            }

        videos = videos_response.data
        logger.info(f"üìπ {len(videos)} v√≠deos encontrados para coleta de coment√°rios")

        # Extrair channel_id da URL
        url = canal.get('url_canal', '')
        channel_id = None
        if '@' in url:
            channel_id = url.split('@')[1].strip('/')
        elif '/channel/' in url:
            channel_id = url.split('/channel/')[1].split('/')[0]
        elif '/c/' in url:
            channel_id = url.split('/c/')[1].split('/')[0]

        if not channel_id:
            logger.error(f"‚ùå N√£o foi poss√≠vel extrair channel_id da URL: {url}")
            raise HTTPException(status_code=400, detail="URL do canal inv√°lida para coleta")

        # Coletar coment√°rios
        from scripts.comentarios.comment_analyzer import CommentAnalyzer
        analyzer = CommentAnalyzer()

        comments_data = await collector.get_all_channel_comments(
            channel_id=channel_id,
            canal_name=canal.get('nome_canal'),
            videos=videos
        )

        total_comments = comments_data.get('total_comments', 0)
        comments_by_video = comments_data.get('comments_by_video', {})

        # Obter l√≠ngua do canal
        canal_lingua = canal.get('lingua', '')

        # Analisar e salvar coment√°rios por v√≠deo
        saved_count = 0
        for video_id, comments in comments_by_video.items():
            if comments:
                # Analisar lote de coment√°rios
                analyzed_comments = await analyzer.analyze_comment_batch(comments)

                # Salvar no banco (passando a l√≠ngua do canal)
                success = await db.save_video_comments(video_id, canal_id, analyzed_comments, canal_lingua)
                if success:
                    saved_count += len(analyzed_comments)

        logger.info(f"‚úÖ Coleta conclu√≠da: {saved_count}/{total_comments} coment√°rios salvos")

        # üîÑ TRADU√á√ÉO AUTOM√ÅTICA EM BACKGROUND
        if saved_count > 0:
            try:
                logger.info(f"üåê Iniciando tradu√ß√£o autom√°tica de coment√°rios em background...")
                background_tasks.add_task(traduzir_comentarios_canal, canal_id)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel iniciar tradu√ß√£o autom√°tica: {e}")

        return {
            'success': True,
            'canal': canal.get('nome_canal'),
            'canal_id': canal_id,
            'total_videos': len(videos),
            'total_coletados': total_comments,
            'comments_saved': saved_count,
            'message': f'Coleta conclu√≠da com sucesso! {saved_count} coment√°rios analisados e salvos. Tradu√ß√£o iniciada em background.'
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao coletar coment√°rios do canal {canal_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/canais/{canal_id}/top-videos")
async def get_canal_top_videos(canal_id: int):
    """
    üì∫ Retorna os 5 v√≠deos mais vistos de um canal.

    Usado na aba "Top Videos" do modal de analytics.
    Ordena√ß√£o por views_atuais (maior ‚Üí menor).

    Args:
        canal_id: ID do canal

    Returns:
        {
            "canal_id": int,
            "canal_nome": str,
            "top_videos": [
                {
                    "video_id": str,
                    "titulo": str,
                    "url_video": str,
                    "url_thumbnail": str,  // calculado
                    "data_publicacao": str,
                    "views_atuais": int,
                    "likes": int,
                    "comentarios": int,
                    "duracao": int  // segundos
                }
            ]
        }
    """
    try:
        # Verificar se canal existe
        canal_response = db.supabase.table("canais_monitorados")\
            .select("id, nome_canal")\
            .eq("id", canal_id)\
            .execute()

        if not canal_response.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        canal = canal_response.data[0]

        logger.info(f"üì∫ Buscando top 5 v√≠deos do canal: {canal.get('nome_canal')} (ID: {canal_id})")

        # Buscar top 5 v√≠deos
        top_videos = await db.get_top_videos_by_canal(canal_id, limit=5)

        return {
            "canal_id": canal_id,
            "canal_nome": canal.get("nome_canal"),
            "top_videos": top_videos
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao buscar top v√≠deos do canal {canal_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.patch("/api/canais/{channel_id}/monetizacao")
async def toggle_monetizacao(channel_id: str, body: dict):
    """
    Ativa/desativa coleta de monetiza√ß√£o para um canal.

    Body: {"is_monetized": true/false}

    Se ativar (true):
      - Pr√≥xima coleta di√°ria (5 AM) j√° coleta revenue
      - Para hist√≥rico: rodar script coleta_historico_completo.py

    Se desativar (false):
      - Para de coletar revenue nas pr√≥ximas coletas
      - Dados hist√≥ricos permanecem no banco
    """
    try:
        is_monetized = body.get('is_monetized')

        if is_monetized is None:
            raise HTTPException(
                status_code=400,
                detail="Campo 'is_monetized' obrigat√≥rio (true/false)"
            )

        # Atualiza no Supabase
        result = supabase.table('yt_channels')\
            .update({'is_monetized': is_monetized})\
            .eq('channel_id', channel_id)\
            .execute()

        if not result.data:
            raise HTTPException(
                status_code=404,
                detail=f"Canal {channel_id} n√£o encontrado"
            )

        status = "ativada" if is_monetized else "desativada"
        logger.info(f"Monetiza√ß√£o {status} para canal {channel_id}")

        return {
            "success": True,
            "channel_id": channel_id,
            "is_monetized": is_monetized,
            "message": f"Monetiza√ß√£o {status} com sucesso"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao toggle monetiza√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comments/logs")
async def get_comments_logs(limit: int = 10):
    """
    üìä Retorna logs detalhados das coletas de coment√°rios

    Args:
        limit: N√∫mero m√°ximo de logs a retornar (default: 10)

    Returns:
        Lista de logs de coleta com detalhes de sucesso/erro
    """
    try:
        logs_manager = CommentsLogsManager()
        logs = logs_manager.get_latest_logs(limit=limit)

        return {
            "logs": logs,
            "total": len(logs)
        }
    except Exception as e:
        logger.error(f"Erro ao buscar logs de coment√°rios: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comments/logs/summary")
async def get_comments_logs_summary(days: int = 7):
    """
    üìä Retorna resumo estat√≠stico dos logs de coment√°rios

    Args:
        days: N√∫mero de dias para calcular o resumo (default: 7)

    Returns:
        Estat√≠sticas agregadas das coletas de coment√°rios
    """
    try:
        logs_manager = CommentsLogsManager()
        summary = logs_manager.get_logs_summary(days=days)

        # Adicionar canais problem√°ticos
        canais_problematicos = logs_manager.get_canais_com_mais_erros(limit=10)
        summary['canais_problematicos'] = canais_problematicos

        return summary
    except Exception as e:
        logger.error(f"Erro ao buscar resumo de logs: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comments/logs/{collection_id}")
async def get_comment_log_by_id(collection_id: str):
    """
    üìä Retorna detalhes de um log espec√≠fico de coleta

    Args:
        collection_id: ID da coleta

    Returns:
        Detalhes completos do log de coleta
    """
    try:
        logs_manager = CommentsLogsManager()
        log = logs_manager.get_log_by_id(collection_id)

        if not log:
            raise HTTPException(status_code=404, detail="Log n√£o encontrado")

        return log
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao buscar log por ID: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comments/stats")
async def get_comments_stats():
    """
    üìä Retorna estat√≠sticas completas dos coment√°rios analisados com GPT.

    Inclui:
    - Total de coment√°rios coletados e analisados
    - Distribui√ß√£o de sentimentos
    - Coment√°rios priorit√°rios
    - M√©tricas de uso do GPT (tokens, custo, tempo)
    - Canais com mais engajamento

    Returns:
        JSON com estat√≠sticas detalhadas
    """
    try:
        # Importar database_comments se ainda n√£o foi importado
        from database_comments import CommentsDB
        from gpt_response_suggester import GPTAnalyzer

        comments_db = CommentsDB()
        gpt_analyzer = GPTAnalyzer()

        # Obter estat√≠sticas gerais (√∫ltimos 7 dias)
        stats = await comments_db.get_comments_stats(days=7)

        # Obter m√©tricas GPT do dia
        gpt_metrics = gpt_analyzer.get_daily_metrics()

        # Buscar distribui√ß√£o de sentimentos (COM PAGINA√á√ÉO PARA >1000 COMENT√ÅRIOS)
        all_comments = await db.fetch_all_records(
            table='video_comments',
            select_fields='sentiment_category'
        )

        sentiment_counts = {}
        for row in all_comments:
            category = row.get('sentiment_category', 'unknown')
            sentiment_counts[category] = sentiment_counts.get(category, 0) + 1

        # Buscar coment√°rios de alta prioridade
        priority_query = db.supabase.table('video_comments')\
            .select('comment_id, comment_text_original, priority_score, sentiment_category, author_name, video_title')\
            .gte('priority_score', 70)\
            .order('priority_score', desc=True)\
            .limit(10)\
            .execute()

        high_priority = priority_query.data if priority_query.data else []

        # Buscar canais com mais coment√°rios
        top_canais_query = """
            SELECT
                cm.nome_canal,
                cm.tipo,
                COUNT(vc.id) as total_comments,
                AVG(vc.sentiment_score) as avg_sentiment,
                COUNT(CASE WHEN vc.priority_score >= 70 THEN 1 END) as high_priority_count
            FROM video_comments vc
            JOIN canais_monitorados cm ON vc.canal_id = cm.id
            WHERE vc.created_at >= NOW() - INTERVAL '7 days'
            GROUP BY cm.id, cm.nome_canal, cm.tipo
            ORDER BY total_comments DESC
            LIMIT 10
        """

        # Como n√£o temos RPC, vamos fazer uma query simplificada
        canais_stats = []
        nossos_canais = db.supabase.table('canais_monitorados')\
            .select('id, nome_canal, tipo')\
            .eq('tipo', 'nosso')\
            .execute()

        if nossos_canais.data:
            for canal in nossos_canais.data[:10]:
                # Usar pagina√ß√£o para garantir TODOS os coment√°rios do canal
                canal_comments = await db.fetch_all_records(
                    table='video_comments',
                    select_fields='id, sentiment_score, priority_score',
                    filters={'canal_id': canal['id']}
                )

                if canal_comments:
                    total = len(canal_comments)
                    avg_sentiment = sum(c.get('sentiment_score', 0) for c in canal_comments) / total if total > 0 else 0
                    high_priority = sum(1 for c in canal_comments if c.get('priority_score', 0) >= 70)

                    canais_stats.append({
                        'nome_canal': canal['nome_canal'],
                        'tipo': canal['tipo'],
                        'total_comments': total,
                        'avg_sentiment': round(avg_sentiment, 2),
                        'high_priority_count': high_priority
                    })

        # Montar resposta completa
        return {
            'summary': {
                'total_comments': stats.get('total_comments', 0),
                'analyzed_comments': stats.get('analyzed_comments', 0),
                'high_priority_count': stats.get('high_priority_count', 0),
                'pending_response_count': stats.get('pending_response_count', 0),
                'last_collection': stats.get('last_collection', None)
            },
            'sentiment_distribution': {
                'positive': sentiment_counts.get('positive', 0),
                'negative': sentiment_counts.get('negative', 0),
                'neutral': sentiment_counts.get('neutral', 0),
                'mixed': sentiment_counts.get('mixed', 0)
            },
            'gpt_metrics': {
                'total_analyzed': gpt_metrics.get('total_analyzed', 0),
                'total_tokens_input': gpt_metrics.get('total_tokens_input', 0),
                'total_tokens_output': gpt_metrics.get('total_tokens_output', 0),
                'estimated_cost_usd': gpt_metrics.get('estimated_cost_usd', 0),
                'avg_response_time_ms': gpt_metrics.get('avg_response_time_ms', 0),
                'success_rate': gpt_metrics.get('success_rate', 100)
            },
            'high_priority_comments': high_priority,
            'top_canais_engagement': canais_stats,
            'collection_info': {
                'comments_per_video_limit': 100,
                'batch_size': 15,  # Reduzido para evitar erros de JSON
                'model': 'gpt-4o-mini',
                'cost_per_1m_tokens': {
                    'input': 0.15,
                    'output': 0.60
                }
            }
        }

    except Exception as e:
        logger.error(f"Erro ao obter estat√≠sticas de coment√°rios: {e}")
        # Retornar estrutura vazia em caso de erro
        return {
            'summary': {
                'total_comments': 0,
                'analyzed_comments': 0,
                'high_priority_count': 0,
                'pending_response_count': 0,
                'last_collection': None
            },
            'sentiment_distribution': {
                'positive': 0,
                'negative': 0,
                'neutral': 0,
                'mixed': 0
            },
            'gpt_metrics': {
                'total_analyzed': 0,
                'total_tokens_input': 0,
                'total_tokens_output': 0,
                'estimated_cost_usd': 0,
                'avg_response_time_ms': 0,
                'success_rate': 0
            },
            'high_priority_comments': [],
            'top_canais_engagement': [],
            'error': str(e),
            'message': 'Sistema de coment√°rios ainda n√£o tem dados. Execute a coleta para ver estat√≠sticas.'
        }


@app.get("/api/comments/management")
async def get_comments_management():
    """
    üí¨ ENDPOINT REMOVIDO - Sistema antigo de respostas autom√°ticas

    Este endpoint foi removido em 03/02/2026.
    Use o novo endpoint: POST /api/comentarios/{comment_id}/gerar-resposta

    Returns:
        JSON com mensagem de deprecia√ß√£o
    """
    return {
        'status': 'deprecated',
        'message': 'Endpoint removido. Use POST /api/comentarios/{comment_id}/gerar-resposta para gerar respostas sob demanda.',
        'migration_date': '2026-02-03',
        'new_endpoint': '/api/comentarios/{comment_id}/gerar-resposta'
    }


@app.post("/api/comments/translate-pending")
async def translate_pending_comments(background_tasks: BackgroundTasks):
    """
    üåê Traduz todos os comentarios pendentes de traducao.

    Executa em background para nao bloquear a API.
    Traduz apenas comentarios de canais que NAO sao em portugues.

    Returns:
        JSON com status e quantidade de comentarios pendentes
    """
    try:
        # Buscar canais nossos que NAO sao em portugues
        canais_response = db.supabase.table('canais_monitorados')\
            .select('id, nome_canal, lingua')\
            .eq('tipo', 'nosso')\
            .execute()

        if not canais_response.data:
            return {'status': 'error', 'message': 'Nenhum canal encontrado'}

        # Filtrar canais que precisam traducao (nao sao PT)
        canais_para_traduzir = []
        for canal in canais_response.data:
            lingua = (canal.get('lingua') or '').lower()
            if 'portug' not in lingua and lingua not in ['portuguese', 'portugu√™s', 'pt', 'pt-br']:
                canais_para_traduzir.append(canal)

        # Contar comentarios pendentes
        total_pendentes = 0
        for canal in canais_para_traduzir:
            count_response = db.supabase.table('video_comments')\
                .select('id', count='exact')\
                .eq('canal_id', canal['id'])\
                .eq('is_translated', False)\
                .execute()
            total_pendentes += count_response.count if count_response.count else 0

        if total_pendentes == 0:
            return {
                'status': 'success',
                'message': 'Todos os comentarios ja estao traduzidos!',
                'canais_verificados': len(canais_para_traduzir),
                'pendentes': 0,
                'canais_em_traducao': list(canais_em_traducao)
            }

        # Disparar traducao em background para cada canal
        for canal in canais_para_traduzir:
            background_tasks.add_task(traduzir_comentarios_canal, canal['id'])

        return {
            'status': 'processing',
            'message': f'Traducao iniciada para {len(canais_para_traduzir)} canais',
            'canais': [c['nome_canal'] for c in canais_para_traduzir],
            'pendentes_estimados': total_pendentes
        }

    except Exception as e:
        logger.error(f"Erro ao iniciar traducao: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/comments/translation-status")
async def get_translation_status():
    """
    üìä Retorna status atual da traducao de comentarios.

    Mostra quantos comentarios estao traduzidos vs pendentes,
    e quais canais estao em processo de traducao.
    """
    try:
        # Buscar canais nossos que NAO sao em portugues
        canais_response = db.supabase.table('canais_monitorados')\
            .select('id, nome_canal, lingua')\
            .eq('tipo', 'nosso')\
            .execute()

        if not canais_response.data:
            return {'status': 'error', 'message': 'Nenhum canal encontrado'}

        # Filtrar canais que precisam traducao (nao sao PT)
        canais_nao_pt = []
        canais_pt = []
        for canal in canais_response.data:
            lingua = (canal.get('lingua') or '').lower()
            if 'portug' not in lingua and lingua not in ['portuguese', 'portugu√™s', 'pt', 'pt-br']:
                canais_nao_pt.append(canal)
            else:
                canais_pt.append(canal)

        # Contar comentarios por status
        stats_por_canal = []
        total_traduzidos = 0
        total_pendentes = 0

        for canal in canais_nao_pt:
            # Traduzidos
            trad_response = db.supabase.table('video_comments')\
                .select('id', count='exact')\
                .eq('canal_id', canal['id'])\
                .eq('is_translated', True)\
                .execute()
            traduzidos = trad_response.count if trad_response.count else 0

            # Pendentes
            pend_response = db.supabase.table('video_comments')\
                .select('id', count='exact')\
                .eq('canal_id', canal['id'])\
                .eq('is_translated', False)\
                .execute()
            pendentes = pend_response.count if pend_response.count else 0

            if traduzidos > 0 or pendentes > 0:
                stats_por_canal.append({
                    'canal': canal['nome_canal'],
                    'lingua': canal.get('lingua', 'desconhecida'),
                    'traduzidos': traduzidos,
                    'pendentes': pendentes
                })

            total_traduzidos += traduzidos
            total_pendentes += pendentes

        return {
            'status': 'success',
            'resumo': {
                'total_traduzidos': total_traduzidos,
                'total_pendentes': total_pendentes,
                'percentual_completo': round(total_traduzidos / (total_traduzidos + total_pendentes) * 100, 1) if (total_traduzidos + total_pendentes) > 0 else 100
            },
            'canais_em_traducao': list(canais_em_traducao),
            'canais_nao_portugues': len(canais_nao_pt),
            'canais_portugues': len(canais_pt),
            'detalhes': stats_por_canal
        }

    except Exception as e:
        logger.error(f"Erro ao buscar status de traducao: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/comments/test-translation")
async def test_translation():
    """
    üß™ Testa se a traducao esta funcionando.

    Tenta traduzir um texto de teste e retorna o resultado ou erro.
    Util para diagnosticar problemas com a API do OpenAI.
    """
    import os
    import traceback

    # Verificar se a chave existe
    api_key = os.getenv("OPENAI_API_KEY")
    key_exists = api_key is not None
    key_prefix = api_key[:10] + "..." if api_key else None

    try:
        from translate_comments_optimized import OptimizedTranslator

        translator = OptimizedTranslator()
        test_text = "This is a test message to verify translation is working correctly."

        result = await translator.translate_batch([test_text])

        return {
            'status': 'success',
            'original': test_text,
            'translated': result[0] if result else None,
            'message': 'Traducao funcionando corretamente!',
            'key_exists': key_exists,
            'key_prefix': key_prefix
        }

    except Exception as e:
        logger.error(f"Erro no teste de traducao: {e}")
        logger.error(traceback.format_exc())
        return {
            'status': 'error',
            'error': str(e),
            'error_type': type(e).__name__,
            'key_exists': key_exists,
            'key_prefix': key_prefix,
            'message': 'Traducao NAO esta funcionando.'
        }


@app.get("/api/videos")
async def get_videos(
    nicho: Optional[str] = None,
    subnicho: Optional[str] = None,
    lingua: Optional[str] = None,
    canal: Optional[str] = None,
    periodo_publicacao: Optional[str] = "60d",
    views_min: Optional[int] = None,
    growth_min: Optional[float] = None,
    order_by: Optional[str] = "views_atuais",
    limit: Optional[int] = 100,
    offset: Optional[int] = None
):
    try:
        videos = await db.get_videos_with_filters(
            nicho=nicho,
            subnicho=subnicho,
            lingua=lingua,
            canal=canal,
            periodo_publicacao=periodo_publicacao,
            views_min=views_min,
            growth_min=growth_min,
            order_by=order_by,
            limit=limit,
            offset=offset
        )
        return {"videos": videos, "total": len(videos)}
    except Exception as e:
        logger.error(f"Error fetching videos: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/filtros")
async def get_filtros():
    try:
        filtros = await db.get_filter_options()
        return filtros
    except Exception as e:
        logger.error(f"Error fetching filtros: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/add-canal")
async def add_canal_manual(
    nome_canal: str,
    url_canal: str,
    nicho: str = "",
    subnicho: str = "",
    lingua: str = "English",
    tipo: str = "minerado",
    status: str = "ativo"
):
    try:
        canal_data = {
            'nome_canal': nome_canal,
            'url_canal': url_canal,
            'nicho': nicho,
            'subnicho': subnicho,
            'lingua': lingua,
            'tipo': tipo,
            'status': status
        }
        
        result = await db.upsert_canal(canal_data)
        return {"message": "Canal added successfully", "canal": result}
    except Exception as e:
        logger.error(f"Error adding canal: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/canais/{canal_id}")
async def update_canal(
    canal_id: int,
    nome_canal: str,
    url_canal: str,
    nicho: str = "",
    subnicho: str = "",
    lingua: str = "English",
    tipo: str = "minerado",
    status: str = "ativo"
):
    try:
        canal_exists = db.supabase.table("canais_monitorados").select("id").eq("id", canal_id).execute()
        if not canal_exists.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")
        
        response = db.supabase.table("canais_monitorados").update({
            "nome_canal": nome_canal,
            "url_canal": url_canal,
            "nicho": nicho,
            "subnicho": subnicho,
            "lingua": lingua,
            "tipo": tipo,
            "status": status
        }).eq("id", canal_id).execute()
        
        logger.info(f"Canal updated: {nome_canal} (ID: {canal_id})")
        return {"message": "Canal atualizado com sucesso", "canal": response.data[0] if response.data else None}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating canal: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def can_start_collection() -> tuple[bool, str]:
    global collection_in_progress, last_collection_time

    # LOCK NO BANCO: Verificar se j√° h√° coleta ativa (fonte da verdade)
    try:
        active_collections = db.supabase.table("coletas_historico")\
            .select("id, data_inicio")\
            .eq("status", "em_progresso")\
            .execute()

        if active_collections.data:
            collection_id = active_collections.data[0]["id"]
            started_at = active_collections.data[0]["data_inicio"]
            return False, f"Collection #{collection_id} already in progress (started: {started_at})"
    except Exception as e:
        logger.error(f"Error checking active collections in DB: {e}")

    # Verificar vari√°vel em mem√≥ria (prote√ß√£o secund√°ria)
    if collection_in_progress:
        return False, "Collection already in progress (in-memory lock)"

    # Verificar cooldown de 1 minuto
    if last_collection_time:
        time_since_last = datetime.now(timezone.utc) - last_collection_time
        cooldown = timedelta(minutes=1)

        if time_since_last < cooldown:
            remaining = cooldown - time_since_last
            seconds = int(remaining.total_seconds())
            return False, f"Cooldown: aguarde {seconds}s"

    # Limpar coletas travadas (> 2h)
    try:
        await db.cleanup_stuck_collections()
    except Exception as e:
        logger.error(f"Error cleaning stuck collections: {e}")

    return True, "OK"

@app.post("/api/collect-data")
async def collect_data(background_tasks: BackgroundTasks):
    try:
        can_collect, message = await can_start_collection()
        
        if not can_collect:
            return {"message": message, "status": "blocked"}
        
        background_tasks.add_task(run_collection_job)
        return {"message": "Collection started", "status": "processing"}
    except Exception as e:
        logger.error(f"Error starting collection: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è ADICIONE ESTE BLOCO AQUI ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è
@app.post("/api/force-notifier")
async def force_notifier():
    """
    For√ßa execu√ß√£o do notifier manualmente.
    √ötil para: testes, debug, ou recuperar notifica√ß√µes perdidas.
    """
    try:
        logger.info("üîî FOR√áANDO EXECU√á√ÉO DO NOTIFIER (manual)")
        
        # Importar e executar o notifier
        from notifier import NotificationChecker
        
        checker = NotificationChecker(db.supabase)
        await checker.check_and_create_notifications()
        
        logger.info("‚úÖ Notifier executado com sucesso!")
        
        return {
            "status": "success",
            "message": "Notificador executado com sucesso! Verifique as notifica√ß√µes."
        }
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao executar notifier: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/force-cache-rebuild")
async def force_cache_rebuild(canal_id: Optional[int] = None):
    """
    üîÑ For√ßa reconstru√ß√£o do cache de engajamento.

    √ötil para testes ou ap√≥s corre√ß√µes manuais.

    Args:
        canal_id: ID espec√≠fico do canal ou None para todos

    Returns:
        Estat√≠sticas do processamento
    """
    try:
        logger.info(f"üîÑ FOR√áANDO REBUILD DO CACHE (canal_id: {canal_id or 'TODOS'})")

        from engagement_preprocessor import EngagementPreprocessor
        preprocessor = EngagementPreprocessor(db)

        result = await preprocessor.force_rebuild_cache(canal_id)

        logger.info(f"‚úÖ Cache rebuild conclu√≠do: {result}")

        return {
            "status": "success",
            "result": result,
            "message": f"Cache reconstru√≠do: {result['processed']}/{result['total']} canais processados"
        }

    except Exception as e:
        logger.error(f"‚ùå Erro ao reconstruir cache: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/cache-stats")
async def get_cache_stats():
    """
    üìä Retorna estat√≠sticas do cache de engajamento.
    """
    try:
        from engagement_preprocessor import EngagementPreprocessor
        preprocessor = EngagementPreprocessor(db)

        stats = await preprocessor.get_cache_stats()

        return stats

    except Exception as e:
        logger.error(f"‚ùå Erro ao obter estat√≠sticas do cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))
# ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è AT√â AQUI ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è

@app.get("/api/stats")
async def get_stats():
    try:
        stats = await db.get_system_stats()
        return stats
    except Exception as e:
        logger.error(f"Error fetching stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/cleanup")
async def cleanup_data():
    try:
        await db.cleanup_old_data()
        return {"message": "Cleanup conclu√≠do com sucesso", "status": "success"}
    except Exception as e:
        logger.error(f"Error in cleanup: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/reset-suspended-keys")
async def reset_suspended_keys():
    """üÜï Endpoint para resetar chaves suspensas (testar novamente ap√≥s contesta√ß√£o)"""
    try:
        count = collector.reset_suspended_keys()
        return {
            "message": f"{count} chave(s) suspensa(s) resetada(s) com sucesso",
            "keys_reset": count,
            "status": "success"
        }
    except Exception as e:
        logger.error(f"Error resetting suspended keys: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/refresh-mv")
async def refresh_materialized_view():
    """
    üîÑ For√ßa refresh da Materialized View mv_dashboard_completo.

    Use este endpoint quando:
    - inscritos_diff estiver mostrando 0 para muitos canais
    - Ap√≥s coleta manual
    - Para garantir dados atualizados
    """
    try:
        logger.info("=" * 60)
        logger.info("üîÑ REFRESH MANUAL DA MATERIALIZED VIEW")
        logger.info(f"Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)

        # Tentar m√©todo 1: Via RPC (se existir)
        try:
            response = db.supabase.rpc("refresh_all_dashboard_mvs").execute()
            if response.data:
                logger.info("‚úÖ Refresh via RPC executado com sucesso")
                # Limpar cache ap√≥s refresh
                cache_cleared = clear_all_cache()
                return {
                    "success": True,
                    "method": "rpc",
                    "message": "Materialized View atualizada via RPC",
                    "data": response.data,
                    "cache_cleared": cache_cleared['entries_cleared']
                }
        except Exception as rpc_error:
            logger.warning(f"RPC n√£o dispon√≠vel: {rpc_error}")

        # M√©todo 2: Verificar dados dispon√≠veis e limpar cache
        try:
            # Verificar dados de hoje e ontem
            hoje = datetime.now(timezone.utc).date()
            ontem = hoje - timedelta(days=1)

            # Contar registros dispon√≠veis
            hoje_count = db.supabase.table('dados_canais_historico') \
                .select('id', count='exact') \
                .gte('data_coleta', hoje.isoformat() + 'T00:00:00') \
                .lte('data_coleta', hoje.isoformat() + 'T23:59:59') \
                .execute()

            ontem_count = db.supabase.table('dados_canais_historico') \
                .select('id', count='exact') \
                .gte('data_coleta', ontem.isoformat() + 'T00:00:00') \
                .lte('data_coleta', ontem.isoformat() + 'T23:59:59') \
                .execute()

            logger.info(f"üìä Dados dispon√≠veis - Hoje: {hoje_count.count}, Ontem: {ontem_count.count}")

            # Se n√£o h√° dados suficientes
            if (hoje_count.count or 0) == 0 or (ontem_count.count or 0) == 0:
                return {
                    "success": False,
                    "method": "data_check",
                    "message": "Dados insuficientes para calcular inscritos_diff. Execute uma coleta primeiro.",
                    "stats": {
                        "dados_hoje": hoje_count.count or 0,
                        "dados_ontem": ontem_count.count or 0
                    }
                }

            # Limpar cache para for√ßar rec√°lculo
            cache_cleared = clear_all_cache()
            logger.info(f"üßπ Cache limpo: {cache_cleared['entries_cleared']} entradas")

            # For√ßar atualiza√ß√£o via database.py
            mv_result = await db.refresh_all_dashboard_mvs()

            return {
                "success": True,
                "method": "force_refresh",
                "message": "Cache limpo e MV atualizada. Dados ser√£o recalculados.",
                "stats": {
                    "dados_hoje": hoje_count.count or 0,
                    "dados_ontem": ontem_count.count or 0,
                    "cache_limpo": cache_cleared['entries_cleared']
                },
                "mv_result": mv_result
            }

        except Exception as fallback_error:
            logger.error(f"Erro no refresh: {fallback_error}")
            return {
                "success": False,
                "error": str(fallback_error),
                "message": "Execute o SQL manualmente no Supabase: REFRESH MATERIALIZED VIEW CONCURRENTLY mv_dashboard_completo;"
            }

    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico no refresh da MV: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "Erro ao tentar atualizar Materialized View"
        }

@app.get("/api/coletas/historico")
async def get_coletas_historico(limit: Optional[int] = 20):
    try:
        historico = await db.get_coletas_historico(limit=limit)

        # Buscar canais problem√°ticos para o modal de logs
        canais_problematicos = await db.get_canais_problematicos()

        quota_usada = await db.get_quota_diaria_usada()

        quota_total = len(collector.api_keys) * 10000
        quota_disponivel = quota_total - quota_usada
        porcentagem_usada = (quota_usada / quota_total) * 100 if quota_total > 0 else 0

        now_utc = datetime.now(timezone.utc)
        next_reset = now_utc.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)

        brasilia_offset = timedelta(hours=-3)
        next_reset_brasilia = next_reset + brasilia_offset

        # Usar informa√ß√µes reais do collector - calcular baseado na quota gasta
        chaves_esgotadas_real = min(int(quota_usada // 10000), len(collector.api_keys))
        chaves_suspensas_real = len(collector.suspended_keys)
        chaves_ativas_real = len(collector.api_keys) - chaves_esgotadas_real - chaves_suspensas_real

        # Pegar dados da √∫ltima coleta
        ultima_coleta = historico[0] if historico else {}
        videos_coletados = ultima_coleta.get("videos_coletados", 0)

        return {
            "historico": historico,
            "total": len(historico),
            "canais_com_erro": {
                "total": len(canais_problematicos),
                "lista": [
                    {
                        "nome": c.get("nome_canal"),
                        "subnicho": c.get("subnicho"),
                        "tipo": c.get("tipo"),
                        "erro": c.get("coleta_ultimo_erro"),
                        "falhas_consecutivas": c.get("coleta_falhas_consecutivas"),
                        "lingua": c.get("lingua"),
                        "url_canal": c.get("url_canal")
                    }
                    for c in canais_problematicos
                ]
            },
            "quota_info": {
                "total_diario": quota_total,
                "usado_hoje": quota_usada,
                "disponivel": quota_disponivel,
                "porcentagem_usada": round(porcentagem_usada, 1),
                "total_chaves": len(collector.api_keys),
                "chaves_ativas": chaves_ativas_real,
                "chaves_esgotadas": chaves_esgotadas_real,
                "chaves_esgotadas_ids": list(collector.exhausted_keys_date.keys()),
                "chaves_suspensas": len(collector.suspended_keys),
                "chaves_suspensas_ids": list(collector.suspended_keys),
                "videos_coletados": videos_coletados,
                "proximo_reset_utc": next_reset.isoformat(),
                "proximo_reset_local": next_reset_brasilia.strftime("%d/%m/%Y %H:%M (Hor√°rio de Bras√≠lia)")
            }
        }
    except Exception as e:
        logger.error(f"Error fetching coletas historico: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/coletas/cleanup")
async def cleanup_stuck_collections():
    try:
        count = await db.cleanup_stuck_collections()
        return {"message": f"{count} coletas travadas marcadas como erro", "count": count}
    except Exception as e:
        logger.error(f"Error in cleanup: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/coletas/{coleta_id}")
async def delete_coleta(coleta_id: int):
    try:
        await db.delete_coleta(coleta_id)
        return {"message": "Coleta deletada com sucesso"}
    except Exception as e:
        logger.error(f"Error deleting coleta: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/favoritos/adicionar")
async def add_favorito(tipo: str, item_id: int):
    try:
        if tipo not in ["canal", "video"]:
            raise HTTPException(status_code=400, detail="Tipo deve ser 'canal' ou 'video'")
        
        if tipo == "canal":
            canal_exists = db.supabase.table("canais_monitorados").select("id").eq("id", item_id).execute()
            if not canal_exists.data:
                raise HTTPException(status_code=404, detail="Canal n√£o encontrado")
        elif tipo == "video":
            video_exists = db.supabase.table("videos_historico").select("id").eq("id", item_id).execute()
            if not video_exists.data:
                raise HTTPException(status_code=404, detail="V√≠deo n√£o encontrado")
        
        result = await db.add_favorito(tipo, item_id)
        return {"message": "Favorito adicionado com sucesso", "favorito": result}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error adding favorito: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/favoritos/remover")
async def remove_favorito(tipo: str, item_id: int):
    try:
        if tipo not in ["canal", "video"]:
            raise HTTPException(status_code=400, detail="Tipo deve ser 'canal' ou 'video'")

        await db.remove_favorito(tipo, item_id)
        return {"message": "Favorito removido com sucesso"}
    except Exception as e:
        logger.error(f"Error removing favorito: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/favoritos/canais")
async def get_favoritos_canais():
    try:
        canais = await db.get_favoritos_canais()
        return {"canais": canais, "total": len(canais)}
    except Exception as e:
        logger.error(f"Error fetching favoritos canais: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/favoritos/videos")
async def get_favoritos_videos():
    try:
        videos = await db.get_favoritos_videos()
        return {"videos": videos, "total": len(videos)}
    except Exception as e:
        logger.error(f"Error fetching favoritos videos: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/canais/{canal_id}")
async def delete_canal(canal_id: int, permanent: bool = False):
    try:
        if permanent:
            try:
                notif_response = db.supabase.table("notificacoes").delete().eq("canal_id", canal_id).execute()
                deleted_count = len(notif_response.data) if notif_response.data else 0
                logger.info(f"Deleted {deleted_count} notifications for canal {canal_id}")
            except Exception as e:
                logger.warning(f"Error deleting notifications for canal {canal_id}: {e}")

            await db.delete_canal_permanently(canal_id)
            return {"message": "Canal deletado permanentemente"}
        else:
            response = db.supabase.table("canais_monitorados").update({
                "status": "inativo"
            }).eq("id", canal_id).execute()
            return {"message": "Canal desativado", "canal": response.data}
    except Exception as e:
        logger.error(f"Error deleting canal: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/notificacoes")
async def get_notificacoes_nao_vistas():
    try:
        notificacoes = await db.get_notificacoes_nao_vistas()
        return {
            "notificacoes": notificacoes,
            "total": len(notificacoes)
        }
    except Exception as e:
        logger.error(f"Error fetching notificacoes: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/notificacoes/todas")
async def get_notificacoes_todas(
    limit: Optional[int] = 500,
    offset: Optional[int] = 0,
    vista: Optional[bool] = None,
    dias: Optional[int] = 30,
    lingua: Optional[str] = None,
    tipo_canal: Optional[str] = None
):
    """
    Lista todas as notifica√ß√µes com filtros.

    Query params:
    - tipo_canal: Filtrar por tipo (nosso/minerado)
    - lingua: Filtrar por l√≠ngua
    - vista: Filtrar por vistas (true/false)
    - dias: Per√≠odo em dias (padr√£o: 30)
    """
    try:
        notificacoes = await db.get_notificacoes_all(
            limit=limit,
            offset=offset,
            vista_filter=vista,
            dias=dias,
            lingua=lingua,
            tipo_canal=tipo_canal
        )
        return {
            "notificacoes": notificacoes,
            "total": len(notificacoes)
        }
    except Exception as e:
        logger.error(f"Error fetching all notificacoes: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/notificacoes/historico")
async def get_notificacoes_historico(limit: Optional[int] = 100):
    try:
        notificacoes = await db.get_notificacoes_all(limit=limit, offset=0)
        return {
            "historico": notificacoes,
            "total": len(notificacoes)
        }
    except Exception as e:
        logger.error(f"Error fetching historico: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/notificacoes/{notif_id}/marcar-vista")
async def marcar_notificacao_vista(notif_id: int):
    """
    Marca uma notifica√ß√£o como vista.
    """
    try:
        success = await db.marcar_notificacao_vista(notif_id)
        
        if success:
            return {
                "message": "Notifica√ß√£o marcada como vista",
                "notif_id": notif_id
            }
        else:
            raise HTTPException(status_code=404, detail="Notifica√ß√£o n√£o encontrada")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error marking notificacao as vista: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/notificacoes/{notif_id}/desmarcar-vista")
async def desmarcar_notificacao_vista(notif_id: int):
    """
    Desmarca uma notifica√ß√£o como vista (volta para n√£o vista).
    √ötil quando usu√°rio marca por engano.
    """
    try:
        logger.info(f"üîÑ Desmarcando notifica√ß√£o {notif_id} como n√£o vista")
        
        success = await db.desmarcar_notificacao_vista(notif_id)
        
        if success:
            logger.info(f"‚úÖ Notifica√ß√£o {notif_id} desmarcada com sucesso")
            return {
                "message": "Notifica√ß√£o desmarcada como vista",
                "notif_id": notif_id
            }
        else:
            raise HTTPException(status_code=404, detail="Notifica√ß√£o n√£o encontrada")
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao desmarcar notifica√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/notificacoes/marcar-todas")
async def marcar_todas_notificacoes_vistas(
    lingua: Optional[str] = None,
    subnicho: Optional[str] = None,
    tipo_canal: Optional[str] = None,
    periodo_dias: Optional[int] = None
):
    """
    Marca notifica√ß√µes n√£o vistas como vistas (com filtros opcionais).

    Query params:
    - lingua: Filtrar por l√≠ngua (ex: portugu√™s, franc√™s)
    - subnicho: Filtrar por subnicho
    - tipo_canal: Filtrar por tipo (nosso/minerado)
    - periodo_dias: Filtrar por per√≠odo da regra (7, 15, 30)
    """
    try:
        count = await db.marcar_todas_notificacoes_vistas(
            lingua=lingua,
            subnicho=subnicho,
            tipo_canal=tipo_canal,
            periodo_dias=periodo_dias
        )
        return {
            "message": f"{count} notifica√ß√µes marcadas como vistas",
            "count": count
        }
    except Exception as e:
        logger.error(f"Error marking all notificacoes as vistas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/cache/clear")
async def clear_all_cache():
    """
    Limpa todo o cache do dashboard e for√ßa atualiza√ß√£o das Materialized Views.
    Use este endpoint ap√≥s deletar canais para for√ßar atualiza√ß√£o imediata.
    """
    try:
        # Limpar cache global
        global dashboard_cache, tabela_cache, cache_timestamp_dashboard, cache_timestamp_tabela, comments_cache
        dashboard_cache = {}
        tabela_cache = {}
        comments_cache = {}  # Limpar cache de coment√°rios tamb√©m
        cache_timestamp_dashboard = None
        cache_timestamp_tabela = None

        # For√ßar refresh das MVs
        try:
            await db.refresh_all_dashboard_mvs()
            mv_refreshed = True
        except Exception as e:
            logger.warning(f"Could not refresh MVs: {e}")
            mv_refreshed = False

        logger.info("üßπ Cache limpo: Dashboard, Tabela e Coment√°rios")

        return {
            "message": "Cache limpo com sucesso",
            "cache_cleared": True,
            "comments_cache_cleared": True,
            "mv_refreshed": mv_refreshed,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/notificacoes/stats")
async def get_notificacoes_stats():
    try:
        stats = await db.get_notificacao_stats()
        return stats
    except Exception as e:
        logger.error(f"Error fetching notificacao stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/regras-notificacoes")
async def get_regras_notificacoes():
    try:
        regras = await db.get_regras_notificacoes()
        return {
            "regras": regras,
            "total": len(regras)
        }
    except Exception as e:
        logger.error(f"Error fetching regras: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/regras-notificacoes")
async def create_regra_notificacao(regra: RegraNotificacaoCreate):
    """üÜï Aceita JSON body via Pydantic model"""
    try:
        regra_data = {
            "nome_regra": regra.nome_regra,
            "views_minimas": regra.views_minimas,
            "periodo_dias": regra.periodo_dias,
            "tipo_canal": regra.tipo_canal,
            "subnichos": regra.subnichos,
            "ativa": regra.ativa
        }
        
        result = await db.create_regra_notificacao(regra_data)
        
        if result:
            return {
                "message": "Regra criada com sucesso",
                "regra": result
            }
        else:
            raise HTTPException(status_code=500, detail="Erro ao criar regra")
    except Exception as e:
        logger.error(f"Error creating regra: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/regras-notificacoes/{regra_id}")
async def update_regra_notificacao(regra_id: int, regra: RegraNotificacaoCreate):
    """üÜï Aceita JSON body via Pydantic model"""
    try:
        regra_data = {
            "nome_regra": regra.nome_regra,
            "views_minimas": regra.views_minimas,
            "periodo_dias": regra.periodo_dias,
            "tipo_canal": regra.tipo_canal,
            "subnichos": regra.subnichos,
            "ativa": regra.ativa
        }
        
        result = await db.update_regra_notificacao(regra_id, regra_data)
        
        if result:
            return {
                "message": "Regra atualizada com sucesso",
                "regra": result
            }
        else:
            raise HTTPException(status_code=404, detail="Regra n√£o encontrada")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating regra: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/regras-notificacoes/{regra_id}")
async def delete_regra_notificacao(regra_id: int):
    try:
        success = await db.delete_regra_notificacao(regra_id)
        
        if success:
            return {"message": "Regra deletada com sucesso"}
        else:
            raise HTTPException(status_code=404, detail="Regra n√£o encontrada")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting regra: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/regras-notificacoes/{regra_id}/toggle")
async def toggle_regra_notificacao(regra_id: int):
    try:
        result = await db.toggle_regra_notificacao(regra_id)
        
        if result:
            status = "ativada" if result["ativa"] else "desativada"
            return {
                "message": f"Regra {status} com sucesso",
                "regra": result
            }
        else:
            raise HTTPException(status_code=404, detail="Regra n√£o encontrada")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error toggling regra: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =========================================================================
# ANALYSIS TAB - New Endpoints
# Added by Claude Code - 2024-11-05
# =========================================================================

@app.get("/api/analysis/keywords")
async def get_keywords_analysis(subniche: str = None, days: int = 30):
    """
    Retorna top 10 keywords

    üöÄ OTIMIZADO: Usa tabela pr√©-calculada (atualizada diariamente)
    Fallback para tempo real se tabela vazia
    """
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )
    try:
        if days not in [7, 15, 30]:
            raise HTTPException(status_code=400, detail="days deve ser 7, 15 ou 30")

        # 1. Tentar buscar da tabela pr√©-calculada (R√ÅPIDO - ~50ms)
        keywords = await db.get_keyword_analysis(period_days=days)

        # 2. Filtrar por subniche se especificado
        if subniche and keywords:
            keywords = [k for k in keywords if k.get('subniche') == subniche]

        # 3. Fallback: Se tabela vazia, calcular em tempo real
        if not keywords:
            logger.warning(f"Tabela keyword_analysis vazia - calculando em tempo real")
            from analyzer import Analyzer
            analyzer = Analyzer(db.supabase)
            keywords = analyzer.analyze_keywords(subniche=subniche, period_days=days)

        return {
            "subniche": subniche or "todos",
            "period_days": days,
            "total": len(keywords),
            "keywords": keywords
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting keywords analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/title-patterns")
async def get_title_patterns_analysis(subniche: str, days: int = 30):
    """
    Retorna top 5 padr√µes de t√≠tulo

    üöÄ OTIMIZADO: Usa tabela pr√©-calculada (atualizada diariamente)
    Fallback para tempo real se tabela vazia
    """
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )
    try:
        if days not in [7, 15, 30]:
            raise HTTPException(status_code=400, detail="days deve ser 7, 15 ou 30")

        # 1. Tentar buscar da tabela pr√©-calculada (R√ÅPIDO - ~50ms)
        patterns = await db.get_title_patterns(subniche=subniche, period_days=days)

        # 2. Fallback: Se tabela vazia, calcular em tempo real
        if not patterns:
            logger.warning(f"Tabela title_patterns vazia para {subniche} - calculando em tempo real")
            from analyzer import Analyzer
            analyzer = Analyzer(db.supabase)
            patterns = analyzer.analyze_title_patterns(subniche=subniche, period_days=days)

        return {
            "subniche": subniche,
            "period_days": days,
            "total": len(patterns),
            "patterns": patterns
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting title patterns: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/top-channels")
async def get_top_channels_analysis(subniche: str, days: int = 30):
    """
    Retorna top 5 canais por subniche

    üöÄ OTIMIZADO: Usa tabela pr√©-calculada quando dispon√≠vel
    Fallback para tempo real com filtro de per√≠odo
    """
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )
    try:
        if days not in [7, 15, 30]:
            raise HTTPException(status_code=400, detail="days deve ser 7, 15 ou 30")

        channels = []

        # 1. Para per√≠odo de 30 dias, tentar usar snapshot (R√ÅPIDO - ~50ms)
        if days == 30:
            channels = await db.get_top_channels_snapshot(subniche=subniche)

        # 2. Fallback: Se snapshot vazio OU per√≠odo diferente de 30 dias, calcular em tempo real
        if not channels:
            if days == 30:
                logger.warning(f"Snapshot vazio para {subniche} - calculando em tempo real")
            from analyzer import Analyzer
            analyzer = Analyzer(db.supabase)
            channels = analyzer.analyze_top_channels(subniche=subniche, period_days=days)

        return {
            "subniche": subniche,
            "period_days": days,
            "total": len(channels),
            "channels": channels
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting top channels: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/subniches")
async def get_all_subniches():
    """Retorna lista de todos os subniches ativos"""
    try:
        subniches = await db.get_all_subniches()
        return {"total": len(subniches), "subniches": subniches}
    except Exception as e:
        logger.error(f"Error getting subniches: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/subniche-trends")
async def get_subniche_trends():
    """
    Retorna tend√™ncias pr√©-calculadas de todos os subnichos (7d, 15d, 30d)

    üöÄ OTIMIZADO: Retorna os 3 per√≠odos em uma √∫nica chamada
    Dados atualizados diariamente durante coleta
    """
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )
    try:
        # Buscar os 3 per√≠odos de uma vez (otimiza√ß√£o frontend)
        trends = await db.get_all_subniche_trends()

        return {
            "success": True,
            "data": trends,
            "total_7d": len(trends.get("7d", [])),
            "total_15d": len(trends.get("15d", [])),
            "total_30d": len(trends.get("30d", []))
        }
    except Exception as e:
        logger.error(f"Error getting subniche trends: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/reports/weekly/latest")
async def get_latest_weekly_report():
    """Retorna o relat√≥rio semanal mais recente"""
    try:
        report = await db.get_weekly_report_latest()
        if report:
            return report
        else:
            raise HTTPException(status_code=404, detail="Nenhum relat√≥rio encontrado")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting weekly report: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/reports/weekly/generate")
async def generate_weekly_report_endpoint():
    """For√ßa a gera√ß√£o de um novo relat√≥rio semanal"""
    try:
        # PROTE√á√ÉO: report_generator.py pode n√£o existir se analyzer.py estiver faltando
        try:
            from report_generator import ReportGenerator
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Report generator n√£o dispon√≠vel: {e}")
            raise HTTPException(
                status_code=503,
                detail="Sistema de relat√≥rios indispon√≠vel (m√≥dulo analyzer.py n√£o encontrado)"
            )

        logger.info("üîÑ Starting weekly report generation...")
        generator = ReportGenerator(db.supabase)
        report = generator.generate_weekly_report()
        logger.info("‚úÖ Weekly report generated successfully")
        return {"message": "Relat√≥rio gerado com sucesso", "report": report}
    except HTTPException:
        raise  # Re-levanta a HTTPException
    except Exception as e:
        logger.error(f"Error generating weekly report: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/analysis/run-daily")
async def run_daily_analysis():
    """Executa an√°lises di√°rias manualmente"""
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )

@app.post("/api/analysis/run-gaps")
async def run_gap_analysis():
    """Executa an√°lise de gaps manualmente"""
    # DESATIVADO - Sistema de an√°lise removido (aba exclu√≠da do dashboard)
    raise HTTPException(
        status_code=503,
        detail="Sistema de an√°lise foi desativado. Aba removida do dashboard."
    )


async def run_collection_job():
    global collection_in_progress, last_collection_time

    coleta_id = None
    canais_sucesso = 0
    canais_erro = 0
    videos_total = 0
    comentarios_total = 0  # Contador de coment√°rios coletados
    comentarios_analisados_total = 0  # Contador de coment√°rios analisados com GPT
    comentarios_com_erro_total = 0  # Contador de coment√°rios que falharam

    # Sistema de logs para coment√°rios
    comments_logger = CommentsLogsManager()
    collection_id = str(uuid.uuid4())
    collection_timestamp = datetime.now(timezone.utc)

    # Listas para rastrear detalhes da coleta
    detalhes_sucesso = []
    detalhes_erros = []

    # Timeout de seguran√ßa - 2 horas m√°ximo
    collection_start_time = time.time()
    MAX_COLLECTION_TIME = 7200  # 2 horas em segundos

    try:
        collection_in_progress = True
        logger.info("=" * 80)
        logger.info("üöÄ STARTING COLLECTION JOB")
        logger.info("=" * 80)

        collector.reset_for_new_collection()

        canais_to_collect = await db.get_canais_for_collection()
        total_canais = len(canais_to_collect)
        logger.info(f"üìä Found {total_canais} canais to collect")

        coleta_id = await db.create_coleta_log(total_canais)
        logger.info(f"üìù Created coleta log ID: {coleta_id}")

        # Criar CommentsDB uma vez s√≥ quando necess√°rio
        comments_db = None

        for index, canal in enumerate(canais_to_collect, 1):
            # SEM TIMEOUT - processar todos os canais

            if collector.all_keys_exhausted():
                logger.error("=" * 80)
                logger.error("‚ùå ALL API KEYS EXHAUSTED - STOPPING COLLECTION")
                logger.error(f"‚úÖ Collected {canais_sucesso}/{total_canais} canais")
                logger.error(f"üìä Total requests used: {collector.total_quota_units}")
                logger.error("=" * 80)
                break
            
            try:
                logger.info(f"[{index}/{total_canais}] üîÑ Processing: {canal['nome_canal']}")

                # üöÄ OTIMIZA√á√ÉO: get_canal_data agora retorna (stats, videos) juntos
                # Isso economiza ~50% da quota de API eliminando busca duplicada!
                canal_data, videos_data = await collector.get_canal_data(canal['url_canal'], canal['nome_canal'])

                if canal_data:
                    saved = await db.save_canal_data(canal['id'], canal_data)
                    if saved:
                        canais_sucesso += 1
                        await db.marcar_coleta_sucesso(canal['id'])  # üÜï Tracking de sucesso
                        logger.info(f"‚úÖ [{index}/{total_canais}] Success: {canal['nome_canal']}")
                    else:
                        canais_erro += 1
                        await db.marcar_coleta_falha(canal['id'], "Dados n√£o salvos (all zeros)")
                        logger.warning(f"‚ö†Ô∏è [{index}/{total_canais}] Data not saved (all zeros): {canal['nome_canal']}")
                else:
                    canais_erro += 1
                    await db.marcar_coleta_falha(canal['id'], "Falha ao obter dados do canal")
                    logger.warning(f"‚ùå [{index}/{total_canais}] Failed: {canal['nome_canal']}")

                # üöÄ Usar v√≠deos j√° buscados (n√£o buscar novamente!)
                if videos_data:
                    await db.save_videos_data(canal['id'], videos_data)
                    videos_total += len(videos_data)

                await db.update_last_collection(canal['id'])

                # üí¨ COLETA DE COMENT√ÅRIOS (APENAS CANAIS NOSSOS)
                if canal.get('tipo') == 'nosso' and videos_data:
                    try:
                        logger.info(f"üí¨ [{index}/{total_canais}] Collecting comments: {canal['nome_canal']}")

                        # Buscar channel_id necess√°rio para coleta
                        channel_id = await collector.get_channel_id(canal['url_canal'], canal['nome_canal'])

                        if channel_id:
                            # Adaptar estrutura dos v√≠deos para a fun√ß√£o de coleta
                            videos_adapted = []
                            for video in videos_data:  # Processar TODOS os v√≠deos dos √∫ltimos 30 dias
                                videos_adapted.append({
                                    'videoId': video.get('video_id'),
                                    'title': video.get('titulo'),
                                    'viewCount': video.get('views_atuais'),
                                    'publishedAt': video.get('data_publicacao')
                                })

                            # Buscar timestamp do √∫ltimo coment√°rio coletado (para coleta incremental)
                            last_comment_timestamp = canal.get('ultimo_comentario_coletado')

                            # Coletar coment√°rios de todos os v√≠deos recentes
                            comments_data = await collector.get_all_channel_comments(
                                channel_id=channel_id,
                                canal_name=canal['nome_canal'],
                                videos=videos_adapted,
                                last_collected_timestamp=last_comment_timestamp
                            )

                            if comments_data and comments_data.get('total_comments', 0) > 0:
                                # Inicializar CommentsDB uma vez s√≥ (na primeira vez que precisar)
                                if comments_db is None:
                                    logger.info("üíæ Inicializando CommentsDB...")
                                    from database_comments import CommentsDB
                                    comments_db = CommentsDB()
                                    logger.info("‚úÖ CommentsDB inicializado")

                                # Processar coment√°rios por v√≠deo - APENAS SALVAR (an√°lise GPT vem depois)
                                for video_id, video_comments in comments_data.get('comments_by_video', {}).items():
                                    if video_comments and video_comments.get('comments'):
                                        # Preparar coment√°rios SEM an√°lise (ser√£o analisados no reprocessamento)
                                        comments_to_save = []

                                        for comment in video_comments['comments']:
                                            comment_data = {
                                                'comment_id': comment.get('comment_id'),
                                                'video_id': video_id,
                                                'canal_id': canal['id'],
                                                'author_name': comment.get('author_name', ''),
                                                'comment_text_original': comment.get('comment_text_original', ''),
                                                'published_at': comment.get('published_at'),
                                                'like_count': comment.get('like_count', 0),
                                                'reply_count': comment.get('reply_count', 0),
                                                # Campos de an√°lise vazios (para reprocessar depois)
                                                'sentiment_category': None,
                                                'sentiment_score': None,
                                                'priority_score': None,
                                                'emotional_tone': None,
                                                'requires_response': False,
                                                'suggested_response': None,
                                                'analyzed_at': None  # NULL = precisa an√°lise GPT
                                            }
                                            comments_to_save.append(comment_data)

                                        # Salvar coment√°rios no banco (SEM an√°lise GPT)
                                        if comments_to_save:
                                            try:
                                                await comments_db.save_video_comments(
                                                    video_id=video_id,
                                                    canal_id=canal['id'],
                                                    comments=comments_to_save
                                                )
                                                logger.info(f"üíæ {len(comments_to_save)} coment√°rios salvos (sem an√°lise) para {canal['nome_canal']}")

                                                # Canais em portugu√™s: marcar como traduzido (n√£o precisa GPT)
                                                canal_lingua = (canal.get('lingua') or '').lower()
                                                if 'portug' in canal_lingua or canal_lingua in ('pt', 'pt-br'):
                                                    try:
                                                        for c in comments_to_save:
                                                            if c.get('comment_id'):
                                                                supabase.table('video_comments').update({
                                                                    'is_translated': True,
                                                                    'comment_text_pt': c.get('comment_text_original', '')
                                                                }).eq('comment_id', c['comment_id']).execute()
                                                        logger.info(f"üáßüá∑ {len(comments_to_save)} coment√°rios PT marcados como traduzidos para {canal['nome_canal']}")
                                                    except Exception as pt_err:
                                                        logger.error(f"Erro ao marcar coment√°rios PT: {pt_err}")

                                            except Exception as save_error:
                                                logger.error(f"‚ùå Erro ao salvar coment√°rios no banco: {save_error}")
                                                # Registrar falha de salvamento
                                                await db.marcar_coleta_falha(
                                                    canal['id'],
                                                    f"Database save failed: {str(save_error)}"
                                                )

                                comentarios_total += comments_data['total_comments']

                                # Adicionar ao log de sucesso
                                detalhes_sucesso.append({
                                    'canal_nome': canal['nome_canal'],
                                    'canal_id': canal['id'],
                                    'videos_processados': len(videos_adapted),
                                    'comentarios_coletados': comments_data['total_comments'],
                                    'comentarios_analisados_gpt': 0  # An√°lise ser√° feita no reprocessamento
                                })

                                # Atualizar timestamp do √∫ltimo coment√°rio coletado (para coleta incremental)
                                if comments_data.get('latest_comment_timestamp'):
                                    try:
                                        await db.update_canal_ultimo_comentario(
                                            canal['id'],
                                            comments_data['latest_comment_timestamp']
                                        )
                                        logger.debug(f"üìÖ Timestamp atualizado para {canal['nome_canal']}: {comments_data['latest_comment_timestamp']}")
                                    except Exception as e:
                                        logger.warning(f"‚ö†Ô∏è Erro ao atualizar timestamp: {e}")

                                logger.info(f"‚úÖ [{index}/{total_canais}] {comments_data['total_comments']} comments saved: {canal['nome_canal']}")
                            else:
                                logger.info(f"‚ÑπÔ∏è [{index}/{total_canais}] No new comments: {canal['nome_canal']}")
                        else:
                            logger.warning(f"‚ö†Ô∏è [{index}/{total_canais}] Channel ID not found for comments: {canal['nome_canal']}")

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è [{index}/{total_canais}] Error collecting comments from {canal['nome_canal']}: {e}")
                        # Adicionar ao log de erros
                        detalhes_erros.append({
                            'canal_nome': canal['nome_canal'],
                            'canal_id': canal['id'],
                            'tipo_erro': 'api_error' if 'quota' in str(e).lower() else 'sem_comentarios',
                            'mensagem': str(e)[:200]
                        })
                        # N√£o interrompe o fluxo - apenas registra o erro

                # üöÄ OTIMIZA√á√ÉO: Removido sleep entre canais - RateLimiter j√° controla
                # await asyncio.sleep(1)

                # Atualizar progresso no banco a cada 10 canais
                if index % 10 == 0 and coleta_id:
                    try:
                        await db.update_coleta_log(
                            coleta_id=coleta_id,
                            status="em_progresso",
                            canais_sucesso=canais_sucesso,
                            canais_erro=canais_erro,
                            videos_coletados=videos_total,
                            requisicoes_usadas=collector.total_quota_units
                        )
                        logger.info(f"üìä Progress update: {canais_sucesso} success, {canais_erro} errors, {videos_total} videos")
                    except Exception as update_error:
                        logger.warning(f"‚ö†Ô∏è Failed to update progress: {update_error}")

                # Log de progresso a cada 25 canais
                if index % 25 == 0:
                    logger.info("=" * 80)
                    logger.info(f"üîÑ PROGRESS CHECKPOINT [{index}/{total_canais}]")
                    logger.info(f"‚úÖ Success: {canais_sucesso} | ‚ùå Errors: {canais_erro} | üé¨ Videos: {videos_total}")
                    logger.info(f"üí¨ Comments: {comentarios_total} | üì° API: {collector.total_quota_units} | ‚è±Ô∏è  Time: ongoing")
                    logger.info("=" * 80)

            except Exception as e:
                logger.error(f"‚ùå Error processing {canal['nome_canal']}: {e}")
                await db.marcar_coleta_falha(canal['id'], str(e))  # üÜï Tracking de falha
                canais_erro += 1
                continue
        
        stats = collector.get_request_stats()
        total_requests = stats['total_quota_units']
        
        logger.info("=" * 80)
        logger.info(f"üìä COLLECTION STATISTICS")
        logger.info(f"‚úÖ Success: {canais_sucesso}/{total_canais}")
        logger.info(f"‚ùå Errors: {canais_erro}/{total_canais}")
        logger.info(f"üé¨ Videos: {videos_total}")
        logger.info(f"üí¨ Comments: {comentarios_total}")
        logger.info(f"üì° Total API Requests: {total_requests}")
        logger.info(f"üîë Active keys: {stats['active_keys']}/{len(collector.api_keys)}")
        logger.info("=" * 80)

        # Salvar log de coment√°rios se houve coleta
        if comentarios_total > 0 or len(detalhes_erros) > 0:
            try:
                # Calcular tokens usados pelo GPT
                # NOTA: Durante coleta, sempre ser√° 0 (an√°lise s√≥ no reprocessamento)
                tokens_usados = 0
                percentual_limite = 0.0
                if comentarios_analisados_total > 0:
                    # C√°lculo de tokens: ~37.5 tokens input + ~20 tokens output por coment√°rio
                    tokens_input = int((comentarios_analisados_total * 150) / 4)  # ~150 chars por coment√°rio, 4 chars por token
                    tokens_output = comentarios_analisados_total * 20  # ~20 tokens por an√°lise
                    tokens_usados = tokens_input + tokens_output
                    percentual_limite = (tokens_usados / 1_000_000) * 100  # % do limite de 1M tokens/dia

                log_data = {
                    'collection_id': collection_id,
                    'timestamp': collection_timestamp,
                    'tipo': 'automatic',  # ou 'manual' se foi disparado manualmente
                    'canais_processados': len([d for d in detalhes_sucesso]) + len(detalhes_erros),
                    'canais_com_sucesso': len(detalhes_sucesso),
                    'canais_com_erro': len(detalhes_erros),
                    'total_comentarios': comentarios_total,
                    'comentarios_analisados': 0,  # Durante coleta = 0 (an√°lise no reprocessamento)
                    'comentarios_nao_analisados': comentarios_total,  # Todos pendentes de an√°lise
                    'detalhes_erros': detalhes_erros,
                    'detalhes_sucesso': detalhes_sucesso,
                    'tempo_execucao': time.time() - collection_start_time,
                    'tokens_usados': tokens_usados,
                    'percentual_limite_diario': percentual_limite
                }

                saved = comments_logger.save_collection_log(log_data)
                if saved:
                    logger.info(f"üíæ Log de coment√°rios salvo: {collection_id}")
                else:
                    logger.warning("‚ö†Ô∏è Falha ao salvar log de coment√°rios")

            except Exception as e:
                logger.error(f"‚ùå Erro ao salvar log de coment√°rios: {e}")

        # Registrar m√©tricas GPT se coment√°rios foram analisados
        if comentarios_total > 0:
            try:
                from database_comments import CommentsDB
                comments_db = CommentsDB()

                # Obter m√©tricas do GPT analyzer (se foi usado)
                try:
                    from gpt_response_suggester import GPTAnalyzer
                    gpt_analyzer = GPTAnalyzer()
                    gpt_metrics = gpt_analyzer.get_daily_metrics()

                    if gpt_metrics['total_analyzed'] > 0:
                        await comments_db.record_gpt_metrics(gpt_metrics)
                        logger.info(f"ü§ñ GPT Metrics: {gpt_metrics['total_analyzed']} analyzed, ${gpt_metrics['estimated_cost_usd']} cost")
                except:
                    pass  # GPT n√£o foi usado ou erro ao obter m√©tricas
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao registrar m√©tricas GPT: {e}")
        
        if canais_sucesso > 0:
            try:
                logger.info("=" * 80)
                logger.info("üîî CHECKING NOTIFICATIONS")
                logger.info("=" * 80)
                await notifier.check_and_create_notifications()
                logger.info("‚úÖ Notification check completed")

                # Cleanup old notifications (>30 days)
                logger.info("=" * 80)
                logger.info("üßπ CLEANING OLD NOTIFICATIONS")
                logger.info("=" * 80)
                deleted_count = await db.cleanup_old_notifications(days=30)
                logger.info(f"‚úÖ Cleaned up {deleted_count} old notifications (>30 days)")
            except Exception as e:
                logger.error(f"‚ùå Error checking notifications: {e}")

            # ü§ñ AUTOMA√á√ÉO P√ìS-COLETA: Tradu√ß√£o e gera√ß√£o de respostas
            if comentarios_total > 0:
                try:
                    logger.info("=" * 80)
                    logger.info("ü§ñ INICIANDO AUTOMA√á√ÉO P√ìS-COLETA")
                    logger.info(f"üìù Processando {comentarios_total} novos coment√°rios...")
                    logger.info("=" * 80)

                    # Importar e executar automa√ß√£o (aguardar conclus√£o)
                    from post_collection_automation import PostCollectionAutomation
                    automation = PostCollectionAutomation()

                    # AGUARDAR automa√ß√£o completar antes de continuar
                    logger.info("üîÑ Aguardando automa√ß√£o p√≥s-coleta...")
                    await automation.run(only_recent=True)

                    logger.info("‚úÖ Automa√ß√£o p√≥s-coleta CONCLU√çDA")
                    logger.info("   ‚Üí Coment√°rios traduzidos")
                    logger.info("   ‚Üí Respostas geradas")

                except Exception as e:
                    logger.error(f"‚ùå Erro ao iniciar automa√ß√£o p√≥s-coleta: {e}")
                    logger.error(f"   Detalhes: {str(e)}")
                    # N√£o interrompe o fluxo - apenas registra o erro

            # üîÑ REPROCESSAMENTO AUTOM√ÅTICO DE COMENT√ÅRIOS SEM AN√ÅLISE (LEGADO - DESATIVADO)
            # NOTA: Substitu√≠do pela automa√ß√£o p√≥s-coleta acima
            if False:  # Desativado - mantido apenas para refer√™ncia
                try:
                    logger.info("=" * 80)
                    logger.info("üîÑ REPROCESSAMENTO AUTOM√ÅTICO DE COMENT√ÅRIOS")
                    logger.info("=" * 80)

                    # Buscar coment√°rios sem an√°lise
                    sem_analise = db.supabase.table('video_comments')\
                        .select('id')\
                        .is_('analyzed_at', 'null')\
                        .execute()

                    total_sem_analise = len(sem_analise.data) if sem_analise.data else 0

                    if total_sem_analise > 0:
                        logger.info(f"üìä {total_sem_analise} coment√°rios sem an√°lise encontrados")
                        logger.info("üîÑ Iniciando reprocessamento autom√°tico...")

                        # Importar fun√ß√£o de reprocessamento
                        from reprocess_comments import reprocess_unanalyzed_comments

                        # Executar reprocessamento
                        await reprocess_unanalyzed_comments()

                        # Verificar resultado
                        ainda_sem = db.supabase.table('video_comments')\
                            .select('id')\
                            .is_('analyzed_at', 'null')\
                            .execute()

                        ainda_sem_count = len(ainda_sem.data) if ainda_sem.data else 0
                        processados = total_sem_analise - ainda_sem_count

                        logger.info(f"‚úÖ {processados} coment√°rios reprocessados com sucesso")
                        if ainda_sem_count > 0:
                            logger.warning(f"‚ö†Ô∏è {ainda_sem_count} coment√°rios ainda sem an√°lise (pr√≥xima tentativa amanh√£)")
                    else:
                        logger.info("‚úÖ Todos os coment√°rios j√° est√£o analisados!")

                except Exception as e:
                    logger.error(f"‚ùå Erro no reprocessamento autom√°tico: {str(e)}")
                    # N√£o falhar a coleta por erro no reprocessamento

            # üÜï P√ìS-PROCESSAMENTO: Tradu√ß√£o + Gera√ß√£o de Respostas
            # Executado AP√ìS an√°lise GPT para n√£o quebrar o processo atual
            if comentarios_total > 0:
                try:
                    logger.info("=" * 80)
                    logger.info("üîÑ P√ìS-PROCESSAMENTO: TRADU√á√ÉO E RESPOSTAS")
                    logger.info("=" * 80)

                    # Importar o workflow corrigido
                    from workflow_comments_fixed import WorkflowCommentsFixed

                    # Processar TODOS os coment√°rios usando workflow corrigido
                    processor = WorkflowCommentsFixed()
                    post_stats = await processor.run_complete_workflow()

                    logger.info(f"‚úÖ P√≥s-processamento completo:")
                    logger.info(f"  - Total coment√°rios: {post_stats.get('total_comments', 0)}")
                    logger.info(f"  - Com texto: {post_stats.get('comments_with_text', 0)}")
                    logger.info(f"  - Traduzidos: {post_stats.get('translated', 0)}")
                    logger.info(f"  - Respostas geradas: {post_stats.get('responses_generated', 0)}")
                    logger.info(f"  - Erros: {post_stats.get('errors', 0)}")

                except Exception as e:
                    # N√£o quebrar a coleta se o p√≥s-processamento falhar
                    logger.error(f"‚ö†Ô∏è Erro no p√≥s-processamento (n√£o cr√≠tico): {e}")

            # üí∞ COLETA DE MONETIZA√á√ÉO (ESTIMATIVAS)
            try:
                logger.info("=" * 80)
                logger.info("üí∞ STARTING MONETIZATION COLLECTION (ESTIMATES)")
                logger.info("=" * 80)
                from monetization_collector import collect_monetization
                await collect_monetization()
                logger.info("‚úÖ Monetization estimates collection completed")
            except Exception as e:
                logger.error(f"‚ùå Error in monetization collection: {e}")

            # üîê COLETA OAUTH (REVENUE REAL)
            try:
                logger.info("=" * 80)
                logger.info("üîê STARTING OAUTH COLLECTION (REAL REVENUE)")
                logger.info("=" * 80)
                from monetization_oauth_collector import collect_oauth_metrics
                result = await collect_oauth_metrics()
                logger.info(f"‚úÖ OAuth collection completed - Success: {result['success']}, Errors: {result['errors']}")
            except Exception as e:
                logger.error(f"‚ùå Error in OAuth collection: {e}")

        if canais_sucesso >= (total_canais * 0.5):
            logger.info("üßπ Cleanup threshold met (>50% success)")
            await db.cleanup_old_data()
        else:
            logger.warning(f"‚è≠Ô∏è Skipping cleanup - only {canais_sucesso}/{total_canais} succeeded")
        
        if canais_erro == 0:
            status = "sucesso"
        elif canais_sucesso > 0:
            status = "parcial"
        else:
            status = "erro"
        
        if coleta_id:
            await db.update_coleta_log(
                coleta_id=coleta_id,
                status=status,
                canais_sucesso=canais_sucesso,
                canais_erro=canais_erro,
                videos_coletados=videos_total,
                requisicoes_usadas=total_requests
            )
        
        logger.info("=" * 80)
        logger.info(f"‚úÖ COLLECTION COMPLETED")
        logger.info("=" * 80)

        last_collection_time = datetime.now(timezone.utc)

        # üöÄ REFRESH AUTOM√ÅTICO DAS MATERIALIZED VIEWS + CACHE
        # Atualiza TODAS as MVs e limpa cache ap√≥s cada coleta
        try:
            logger.info("")  # Linha em branco para melhor visualiza√ß√£o
            logger.info("=" * 60)
            logger.info("üîÑ ATUALIZANDO MATERIALIZED VIEWS E CACHE")
            logger.info("=" * 60)

            # 1. Atualizar TODAS as Materialized Views
            mv_results = await db.refresh_all_dashboard_mvs()

            # 2. Limpar todo o cache do dashboard (ser√° renovado no pr√≥ximo acesso)
            cache_stats = clear_all_cache()
            logger.info(f"üßπ Cache limpo: {cache_stats['entries_cleared']} entradas removidas")
            logger.info(f"üíæ Mem√≥ria liberada: ~{cache_stats['approx_size_kb']}KB")

            logger.info("‚úÖ Dashboard pronto com dados frescos e cache renovado!")
            logger.info("‚ö° Pr√≥ximo acesso ser√° instant√¢neo (< 1ms)")
            logger.info("=" * 60)
            logger.info("")  # Linha em branco para melhor visualiza√ß√£o

        except Exception as mv_error:
            # N√£o √© cr√≠tico - apenas log de warning
            logger.warning(f"‚ö†Ô∏è Falha ao atualizar MVs/Cache: {mv_error}")
            logger.warning("Dashboard continuar√° funcionando com dados anteriores")

        # =====================================================================
        # AN√ÅLISE DI√ÅRIA DESATIVADA (aba removida do dashboard)
        # C√≥digo preservado para refer√™ncia futura
        # =====================================================================
        # await run_daily_analysis_job()  # DESATIVADO - analyzer.py n√£o existe

        # =====================================================================
        # BUILD ENGAGEMENT CACHE - Movido para c√° (roda SEMPRE ap√≥s coleta)
        # N√£o depende mais da an√°lise di√°ria
        # =====================================================================
        try:
            logger.info("üîÑ INICIANDO BUILD DO CACHE DE ENGAJAMENTO")
            from engagement_preprocessor import build_engagement_cache
            cache_result = await build_engagement_cache()
            logger.info(f"‚úÖ ENGAGEMENT CACHE ATUALIZADO: {cache_result.get('processed', 0)}/{cache_result.get('total', 0)} canais processados")
        except Exception as cache_error:
            logger.error(f"‚ùå Erro ao construir cache de engajamento: {cache_error}")
            # N√£o falha o job principal se o cache falhar

    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"‚ùå COLLECTION JOB FAILED: {e}")
        logger.error("=" * 80)
        
        if coleta_id:
            await db.update_coleta_log(
                coleta_id=coleta_id,
                status="erro",
                canais_sucesso=canais_sucesso,
                canais_erro=canais_erro,
                videos_coletados=videos_total,
                requisicoes_usadas=collector.total_quota_units if hasattr(collector, 'total_quota_units') else 0,
                mensagem_erro=str(e)
            )
        
        raise
    finally:
        collection_in_progress = False


# =========================================================================
# CRON JOBS - Daily Analysis + Weekly Report
# =========================================================================

"""
=====================================================================
FUN√á√ïES DE AN√ÅLISE DESATIVADAS
Aba "An√°lise" removida do dashboard - c√≥digo preservado para refer√™ncia
=====================================================================
"""

# DESATIVADO - analyzer.py n√£o existe
'''
async def run_daily_analysis_job():
    """Executa an√°lises di√°rias ap√≥s a coleta de dados"""
    try:
        from analyzer import Analyzer, save_analysis_to_db
        logger.info("=" * 80)
        logger.info("STARTING DAILY ANALYSIS JOB")
        logger.info("=" * 80)
        analyzer = Analyzer(db.supabase)
        subniches = await db.get_all_subniches()

        # Keywords, Patterns e Channels POR SUBNICHE
        for subniche in subniches:
            # Keywords por subniche
            for days in [30, 15, 7]:
                keywords = analyzer.analyze_keywords(subniche=subniche, period_days=days)
                save_analysis_to_db(db.supabase, 'keywords', keywords, period_days=days, subniche=subniche)

            # Patterns por subniche
            for days in [30, 15, 7]:
                patterns = analyzer.analyze_title_patterns(subniche, period_days=days)
                save_analysis_to_db(db.supabase, 'patterns', patterns, period_days=days, subniche=subniche)

            # Channels por subniche
            channels = analyzer.analyze_top_channels(subniche)
            save_analysis_to_db(db.supabase, 'channels', channels, subniche=subniche)

        # =====================================================================
        # SUBNICHE TRENDS (novo - 2025-01-07)
        # =====================================================================
        logger.info("Analisando tendencias por subniche...")
        for days in [7, 15, 30]:
            trends = analyzer.analyze_subniche_trends(period_days=days)
            save_analysis_to_db(db.supabase, 'subniche_trends', trends, period_days=days)
        logger.info(f"OK - Tendencias de {len(subniches)} subnichos calculadas (7d, 15d, 30d)")

        logger.info("OK - DAILY ANALYSIS COMPLETED")

        # =====================================================================
        # √öLTIMO STEP: BUILD ENGAGEMENT CACHE
        # Executa AP√ìS todas as an√°lises e processamentos
        # =====================================================================
        try:
            logger.info("üîÑ INICIANDO BUILD DO CACHE DE ENGAJAMENTO (√öLTIMO STEP)")
            from engagement_preprocessor import build_engagement_cache
            cache_result = await build_engagement_cache()
            logger.info(f"‚úÖ ENGAGEMENT CACHE ATUALIZADO: {cache_result.get('processed', 0)}/{cache_result.get('total', 0)} canais processados")
        except Exception as cache_error:
            logger.error(f"‚ùå Erro ao construir cache de engajamento: {cache_error}")
            # N√£o falha o job principal se o cache falhar

    except Exception as e:
        logger.error(f"ERRO - DAILY ANALYSIS FAILED: {e}")

async def run_weekly_report_job():
    """Gera relat√≥rio semanal completo (segundas 5h AM)"""
    try:
        from report_generator import ReportGenerator
        from analyzer import Analyzer, save_analysis_to_db
        logger.info("üìä STARTING WEEKLY REPORT JOB")

        # Gap analysis
        analyzer = Analyzer(db.supabase)
        subniches = await db.get_all_subniches()
        for subniche in subniches:
            gaps = analyzer.analyze_gaps(subniche)
            save_analysis_to_db(db.supabase, 'gaps', gaps, subniche=subniche)

        # Gerar relat√≥rio
        generator = ReportGenerator(db.supabase)
        report = generator.generate_weekly_report()
        logger.info(f"‚úÖ WEEKLY REPORT COMPLETED: {report['week_start']} to {report['week_end']}")
    except Exception as e:
        logger.error(f"‚ùå WEEKLY REPORT FAILED: {e}")

async def weekly_report_scheduler():
    """Background task para relat√≥rio semanal (segundas 5h AM)"""
    while True:
        try:
            now = datetime.now(timezone.utc)
            sao_paulo_tz = timezone(timedelta(hours=-3))
            now_sp = now.astimezone(sao_paulo_tz)

            if now_sp.weekday() == 0 and now_sp.hour >= 5:
                await run_weekly_report_job()
                await asyncio.sleep(86400)
            else:
                await asyncio.sleep(3600)
        except Exception as e:
            logger.error(f"‚ùå Weekly scheduler error: {e}")
            await asyncio.sleep(3600)
'''


async def schedule_spreadsheet_scanner():
    """
    Background task para varredura de planilhas Google Sheets.

    Roda a cada X minutos (configur√°vel via SCANNER_INTERVAL_MINUTES).
    Detecta v√≠deos prontos para upload e adiciona na fila automaticamente.
    """
    from yt_uploader.spreadsheet_scanner import SpreadsheetScanner

    # Configura√ß√µes
    interval_minutes = int(os.getenv("SCANNER_INTERVAL_MINUTES", "20"))  # 20 min - suporte garantido para 70+ canais
    enabled = os.getenv("SCANNER_ENABLED", "true").lower() == "true"

    if not enabled:
        logger.info("üìä Scanner de planilhas DESABILITADO (SCANNER_ENABLED=false)")
        return

    logger.info(f"üìä Scanner de planilhas AGENDADO (a cada {interval_minutes} min)")

    scanner = SpreadsheetScanner()

    while True:
        try:
            await scanner.scan_all_spreadsheets()
        except Exception as e:
            logger.error(f"‚ùå Scanner error: {e}", exc_info=True)

        # Aguarda pr√≥xima execu√ß√£o
        await asyncio.sleep(interval_minutes * 60)


@app.on_event("startup")
async def startup_event():
    logger.info("=" * 80)
    logger.info("üöÄ YOUTUBE DASHBOARD API STARTING")
    logger.info("=" * 80)

    try:
        await db.test_connection()
        logger.info("‚úÖ Database connected")
    except Exception as e:
        logger.error(f"‚ùå Database failed: {e}")

    try:
        await db.cleanup_stuck_collections()
    except Exception as e:
        logger.error(f"Error cleaning stuck collections: {e}")

    # PROTE√á√ÉO: S√≥ iniciar schedulers no Railway
    is_railway = os.environ.get("RAILWAY_ENVIRONMENT") is not None

    if is_railway:
        logger.info("üìÖ Scheduling daily collection (NO startup collection)")
        asyncio.create_task(schedule_daily_collection())
        # asyncio.create_task(weekly_report_scheduler())  # DESATIVADO - Sistema de an√°lise removido
        asyncio.create_task(schedule_spreadsheet_scanner())

        # Upload Queue Worker (isolado - falha n√£o afeta main app)
        try:
            from yt_uploader.queue_worker import start_queue_worker
            asyncio.create_task(start_queue_worker())
            logger.info("‚úÖ Upload queue worker scheduled")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Upload worker disabled: {e}")

        # Daily YouTube Upload System (isolado - falha n√£o afeta main app)
        if os.environ.get("DAILY_UPLOAD_ENABLED", "").lower() == "true":
            try:
                asyncio.create_task(schedule_daily_uploader())
                logger.info("‚úÖ Daily YouTube upload scheduler started")
                logger.info("üìÖ Upload scheduled for 5:30 AM daily")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Daily upload scheduler disabled: {e}")
        else:
            logger.info("üìå Daily upload system disabled (set DAILY_UPLOAD_ENABLED=true to enable)")

        logger.info("‚úÖ Schedulers started (Railway environment + Scanner + Upload Worker)")
    else:
        logger.warning("‚ö†Ô∏è LOCAL ENVIRONMENT - Schedulers DISABLED")
        logger.warning("‚ö†Ô∏è Use /api/collect-data endpoint for manual collection")

    logger.info("=" * 80)

async def schedule_daily_collection():
    logger.info("=" * 80)
    logger.info("‚è∞ PROTE√á√ÉO DE STARTUP ATIVADA")
    logger.info("‚è≥ Aguardando 5 minutos para evitar coletas durante deploy...")
    logger.info("=" * 80)
    await asyncio.sleep(300)
    logger.info("‚úÖ Prote√ß√£o de startup completa - scheduler ativo")
    
    while True:
        try:
            now = datetime.now(timezone.utc)
            
            next_run = now.replace(hour=8, minute=0, second=0, microsecond=0)
            
            if next_run <= now:
                next_run += timedelta(days=1)
            
            sleep_seconds = (next_run - now).total_seconds()
            
            logger.info("=" * 80)
            logger.info(f"‚è∞ Next collection: {next_run.isoformat()} (05:00 AM S√£o Paulo)")
            logger.info(f"‚è≥ Sleeping for {sleep_seconds/3600:.1f} hours")
            logger.info("=" * 80)
            
            await asyncio.sleep(sleep_seconds)
            
            can_collect, message = await can_start_collection()
            
            if can_collect:
                logger.info("üöÄ Starting scheduled collection...")
                await run_collection_job()
            else:
                logger.warning(f"‚ö†Ô∏è Scheduled collection blocked: {message}")
            
        except Exception as e:
            logger.error(f"‚ùå Scheduled collection failed: {e}")
            await asyncio.sleep(3600)


# ========================================
# üåê TRADU√á√ÉO AUTOM√ÅTICA DE COMENT√ÅRIOS
# ========================================

# Dicion√°rio para controle de lock (evita tradu√ß√£o duplicada)
canais_em_traducao = set()

async def traduzir_comentarios_canal(canal_id: int):
    """
    Traduz TODOS os coment√°rios n√£o traduzidos de um canal em background.
    Processa em loop at√© n√£o haver mais pendentes.
    N√ÉO traduz coment√°rios de canais em portugu√™s.
    Sistema de lock previne duplica√ß√£o.
    """
    # Verificar se j√° est√° em tradu√ß√£o
    if canal_id in canais_em_traducao:
        logger.info(f"‚ö†Ô∏è Canal {canal_id} j√° est√° sendo traduzido, pulando...")
        return

    # Marcar canal como em tradu√ß√£o
    canais_em_traducao.add(canal_id)

    try:
        logger.info(f"üåê Iniciando verifica√ß√£o de tradu√ß√£o para canal {canal_id}")

        # Verificar l√≠ngua do canal
        canal_response = db.supabase.table('canais_monitorados')\
            .select('nome_canal, lingua')\
            .eq('id', canal_id)\
            .execute()

        if not canal_response.data:
            logger.error(f"‚ùå Canal {canal_id} n√£o encontrado")
            return

        canal = canal_response.data[0]
        lingua = canal.get('lingua', '').lower()

        # Se canal √© portugu√™s, pular tradu√ß√£o
        if 'portug' in lingua or lingua in ['portuguese', 'portugu√™s', 'pt', 'pt-br']:
            logger.info(f"üáßüá∑ Canal {canal['nome_canal']} √© em portugu√™s - tradu√ß√£o n√£o necess√°ria")
            return

        logger.info(f"üåç Canal {canal['nome_canal']} ({lingua}) - iniciando tradu√ß√£o completa")

        # Importar tradutor
        from translate_comments_optimized import OptimizedTranslator
        translator = OptimizedTranslator()

        total_traduzidos = 0
        rodadas = 0

        # Loop at√© traduzir TODOS os coment√°rios
        while True:
            rodadas += 1

            # Buscar pr√≥ximo lote de coment√°rios n√£o traduzidos
            response = db.supabase.table('video_comments')\
                .select('id, comment_text_original')\
                .eq('canal_id', canal_id)\
                .eq('is_translated', False)\
                .limit(50)\
                .execute()

            if not response.data:
                logger.info(f"‚úÖ Todos os coment√°rios do canal {canal_id} foram traduzidos!")
                break

            comentarios = response.data
            logger.info(f"üìù Rodada {rodadas}: {len(comentarios)} coment√°rios para traduzir")

            # Processar em lotes de 20
            batch_size = 20
            traduzidos_rodada = 0

            for i in range(0, len(comentarios), batch_size):
                batch = comentarios[i:i+batch_size]
                textos_originais = [c['comment_text_original'] for c in batch]

                # Tentar traduzir com retry (at√© 3 tentativas)
                for tentativa in range(3):
                    try:
                        # Traduzir batch
                        textos_traduzidos = await translator.translate_batch(textos_originais)

                        # Atualizar no banco
                        for j, comentario in enumerate(batch):
                            if j < len(textos_traduzidos):
                                texto_traduzido = textos_traduzidos[j]

                                # S√≥ atualizar se recebeu tradu√ß√£o
                                if texto_traduzido:
                                    update_result = db.supabase.table('video_comments')\
                                        .update({
                                            'comment_text_pt': texto_traduzido,
                                            'is_translated': True
                                        })\
                                        .eq('id', comentario['id'])\
                                        .execute()

                                    if update_result.data:
                                        traduzidos_rodada += 1

                        logger.info(f"‚úÖ Lote {i//batch_size + 1} traduzido: {traduzidos_rodada} coment√°rios")
                        break  # Sucesso, sai do loop de retry

                    except Exception as e:
                        if tentativa < 2:
                            logger.warning(f"‚ö†Ô∏è Erro ao traduzir lote (tentativa {tentativa + 1}/3): {e}")
                            await asyncio.sleep(5 * (tentativa + 1))  # 5s, 10s
                        else:
                            logger.error(f"‚ùå Erro ap√≥s 3 tentativas no lote: {e}")
                            break  # Pula este lote ap√≥s 3 falhas

                # Rate limiting entre batches
                await asyncio.sleep(2)

            total_traduzidos += traduzidos_rodada
            logger.info(f"üìä Rodada {rodadas} conclu√≠da: {traduzidos_rodada} traduzidos (Total: {total_traduzidos})")

            # Se n√£o traduziu nenhum nesta rodada (todos falharam), parar para evitar loop infinito
            if traduzidos_rodada == 0:
                logger.warning(f"‚ö†Ô∏è Nenhum coment√°rio traduzido nesta rodada, parando...")
                break

        logger.info(f"üéâ Tradu√ß√£o COMPLETA do canal {canal_id}: {total_traduzidos} coment√°rios traduzidos em {rodadas} rodadas")

    except Exception as e:
        logger.error(f"‚ùå Erro na tradu√ß√£o autom√°tica do canal {canal_id}: {e}")
    finally:
        # Remover lock do canal
        canais_em_traducao.discard(canal_id)


# ========================================
# üí∞ ENDPOINTS FINANCEIRO
# ========================================

# ========== CATEGORIAS ==========

@app.get("/api/financeiro/categorias")
async def listar_categorias_financeiras(ativo: bool = None):
    """Lista todas as categorias financeiras"""
    try:
        categorias = await financeiro.listar_categorias(ativo)
        return {"categorias": categorias}
    except Exception as e:
        logger.error(f"Erro ao listar categorias: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/financeiro/categorias")
async def criar_categoria_financeira(
    nome: str,
    tipo: str,
    cor: str = None,
    icon: str = None
):
    """Cria nova categoria financeira"""
    try:
        categoria = await financeiro.criar_categoria(nome, tipo, cor, icon)
        return categoria
    except Exception as e:
        logger.error(f"Erro ao criar categoria: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/financeiro/categorias/{categoria_id}")
async def editar_categoria_financeira(categoria_id: int, dados: Dict[str, Any]):
    """Edita categoria existente"""
    try:
        categoria = await financeiro.editar_categoria(categoria_id, dados)
        return categoria
    except Exception as e:
        logger.error(f"Erro ao editar categoria: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/financeiro/categorias/{categoria_id}")
async def deletar_categoria_financeira(categoria_id: int):
    """Deleta categoria (soft delete)"""
    try:
        await financeiro.deletar_categoria(categoria_id)
        return {"success": True, "message": "Categoria deletada"}
    except Exception as e:
        logger.error(f"Erro ao deletar categoria: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== LAN√áAMENTOS ==========

@app.get("/api/financeiro/lancamentos")
async def listar_lancamentos_financeiros(
    periodo: str = "30d",
    tipo: str = None,
    recorrencia: str = None
):
    """Lista lan√ßamentos com filtros"""
    try:
        lancamentos = await financeiro.listar_lancamentos(periodo, tipo, recorrencia)
        return {"lancamentos": lancamentos, "total": len(lancamentos)}
    except Exception as e:
        logger.error(f"Erro ao listar lan√ßamentos: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/financeiro/lancamentos")
async def criar_lancamento_financeiro(request: Request):
    """Cria novo lan√ßamento"""
    try:
        data = await request.json()

        categoria_id = data.get('categoria_id')
        valor = data.get('valor')
        data_lancamento = data.get('data')
        descricao = data.get('descricao', '')
        tipo = data.get('tipo')
        recorrencia = data.get('recorrencia')
        usuario = data.get('usuario', 'Usuario')

        # Valida√ß√µes
        if not categoria_id:
            raise HTTPException(status_code=422, detail="categoria_id √© obrigat√≥rio")
        if not valor:
            raise HTTPException(status_code=422, detail="valor √© obrigat√≥rio")
        if not data_lancamento:
            raise HTTPException(status_code=422, detail="data √© obrigat√≥ria")
        if not tipo:
            raise HTTPException(status_code=422, detail="tipo √© obrigat√≥rio")
        if tipo not in ['receita', 'despesa']:
            raise HTTPException(status_code=422, detail="tipo deve ser 'receita' ou 'despesa'")

        lancamento = await financeiro.criar_lancamento(
            categoria_id, valor, data_lancamento, descricao, tipo, recorrencia, usuario
        )
        return lancamento
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao criar lan√ßamento: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/financeiro/lancamentos/{lancamento_id}")
async def editar_lancamento_financeiro(lancamento_id: int, dados: Dict[str, Any]):
    """Edita lan√ßamento existente"""
    try:
        lancamento = await financeiro.editar_lancamento(lancamento_id, dados)
        return lancamento
    except Exception as e:
        logger.error(f"Erro ao editar lan√ßamento: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/financeiro/lancamentos/{lancamento_id}")
async def deletar_lancamento_financeiro(lancamento_id: int):
    """Deleta lan√ßamento"""
    try:
        await financeiro.deletar_lancamento(lancamento_id)
        return {"success": True, "message": "Lan√ßamento deletado"}
    except Exception as e:
        logger.error(f"Erro ao deletar lan√ßamento: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/lancamentos/export-csv")
async def exportar_lancamentos_csv(periodo: str = "30d"):
    """Exporta lan√ßamentos em formato CSV"""
    try:
        lancamentos = await financeiro.listar_lancamentos(periodo)

        # Gerar CSV
        from fastapi.responses import Response
        import io

        output = io.StringIO()
        output.write("Data,Tipo,Recorr√™ncia,Categoria,Descri√ß√£o,Valor\n")

        for lanc in lancamentos:
            categoria_nome = lanc.get('financeiro_categorias', {}).get('nome', 'N/A') if lanc.get('financeiro_categorias') else 'N/A'
            recorrencia = lanc.get('recorrencia', 'N/A') or 'N/A'

            output.write(f"{lanc['data']},{lanc['tipo']},{recorrencia},{categoria_nome},{lanc.get('descricao', '')},{lanc['valor']}\n")

        csv_content = output.getvalue()
        output.close()

        return Response(
            content=csv_content.encode('utf-8-sig'),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=financeiro_{periodo}.csv"}
        )
    except Exception as e:
        logger.error(f"Erro ao exportar CSV: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== TAXAS ==========

@app.get("/api/financeiro/taxas")
async def listar_taxas_financeiras(ativo: bool = None):
    """Lista todas as taxas"""
    try:
        taxas = await financeiro.listar_taxas(ativo)
        return {"taxas": taxas}
    except Exception as e:
        logger.error(f"Erro ao listar taxas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/financeiro/taxas")
async def criar_taxa_financeira(request: Request):
    """Cria nova taxa"""
    try:
        data = await request.json()

        nome = data.get('nome')
        percentual = data.get('percentual')
        aplica_sobre = data.get('aplica_sobre', 'receita_bruta')

        # Valida√ß√µes
        if not nome:
            raise HTTPException(status_code=422, detail="nome √© obrigat√≥rio")
        if not percentual or percentual <= 0:
            raise HTTPException(status_code=422, detail="percentual deve ser maior que 0")
        if aplica_sobre not in ['receita_bruta', 'receita_liquida']:
            raise HTTPException(status_code=422, detail="aplica_sobre deve ser 'receita_bruta' ou 'receita_liquida'")

        taxa = await financeiro.criar_taxa(nome, percentual, aplica_sobre)
        return taxa
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao criar taxa: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/financeiro/taxas/{taxa_id}")
async def editar_taxa_financeira(taxa_id: int, dados: Dict[str, Any]):
    """Edita taxa existente"""
    try:
        taxa = await financeiro.editar_taxa(taxa_id, dados)
        return taxa
    except Exception as e:
        logger.error(f"Erro ao editar taxa: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/financeiro/taxas/{taxa_id}")
async def deletar_taxa_financeira(taxa_id: int):
    """Deleta taxa (soft delete)"""
    try:
        await financeiro.deletar_taxa(taxa_id)
        return {"success": True, "message": "Taxa deletada"}
    except Exception as e:
        logger.error(f"Erro ao deletar taxa: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== METAS ==========

@app.get("/api/financeiro/metas")
async def listar_metas_financeiras(ativo: bool = None):
    """Lista todas as metas"""
    try:
        metas = await financeiro.listar_metas(ativo)
        return {"metas": metas}
    except Exception as e:
        logger.error(f"Erro ao listar metas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/metas/progresso")
async def progresso_metas_financeiras():
    """Calcula progresso de todas as metas ativas"""
    try:
        progresso = await financeiro.calcular_progresso_metas()
        return {"metas": progresso}
    except Exception as e:
        logger.error(f"Erro ao calcular progresso metas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/financeiro/metas")
async def criar_meta_financeira(request: Request):
    """Cria nova meta"""
    try:
        data = await request.json()

        nome = data.get('nome')
        tipo = data.get('tipo')
        valor_objetivo = data.get('valor_objetivo')
        periodo_inicio = data.get('periodo_inicio')
        periodo_fim = data.get('periodo_fim')

        # Valida√ß√µes
        if not nome:
            raise HTTPException(status_code=422, detail="nome √© obrigat√≥rio")
        if not tipo:
            raise HTTPException(status_code=422, detail="tipo √© obrigat√≥rio")
        if tipo not in ['receita', 'lucro_liquido']:
            raise HTTPException(status_code=422, detail="tipo deve ser 'receita' ou 'lucro_liquido'")
        if not valor_objetivo or valor_objetivo <= 0:
            raise HTTPException(status_code=422, detail="valor_objetivo deve ser maior que 0")
        if not periodo_inicio:
            raise HTTPException(status_code=422, detail="periodo_inicio √© obrigat√≥rio")
        if not periodo_fim:
            raise HTTPException(status_code=422, detail="periodo_fim √© obrigat√≥rio")

        meta = await financeiro.criar_meta(nome, tipo, valor_objetivo, periodo_inicio, periodo_fim)
        return meta
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao criar meta: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.patch("/api/financeiro/metas/{meta_id}")
async def editar_meta_financeira(meta_id: int, dados: Dict[str, Any]):
    """Edita meta existente"""
    try:
        meta = await financeiro.editar_meta(meta_id, dados)
        return meta
    except Exception as e:
        logger.error(f"Erro ao editar meta: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/financeiro/metas/{meta_id}")
async def deletar_meta_financeira(meta_id: int):
    """Deleta meta (soft delete)"""
    try:
        await financeiro.deletar_meta(meta_id)
        return {"success": True, "message": "Meta deletada"}
    except Exception as e:
        logger.error(f"Erro ao deletar meta: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== OVERVIEW / DASHBOARD ==========

@app.get("/api/financeiro/overview")
async def overview_financeiro(periodo: str = "30d"):
    """
    Retorna overview completo:
    - Receita bruta
    - Despesas (total + breakdown fixas/√∫nicas)
    - Taxas totais
    - Lucro l√≠quido
    - Compara√ß√£o com per√≠odo anterior
    """
    try:
        overview = await financeiro.get_overview(periodo)
        return overview
    except Exception as e:
        logger.error(f"Erro ao gerar overview: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/taxa-cambio")
async def taxa_cambio_atual():
    """
    Retorna taxa de c√¢mbio USD-BRL atual
    Exemplo: {"taxa": 5.52, "atualizado_em": "2025-12-17 15:35:03"}
    """
    try:
        from financeiro import get_usd_brl_rate
        taxa = await get_usd_brl_rate()
        return taxa
    except Exception as e:
        logger.error(f"Erro ao buscar taxa de c√¢mbio: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/graficos/receita-despesas")
async def grafico_receita_despesas(periodo: str = "30d"):
    """
    Dados para gr√°fico de linha: receita vs despesas vs lucro
    """
    try:
        dados = await financeiro.get_grafico_receita_despesas(periodo)
        return {"dados": dados}
    except Exception as e:
        logger.error(f"Erro ao gerar gr√°fico receita/despesas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/graficos/despesas-breakdown")
async def grafico_despesas_breakdown(periodo: str = "30d"):
    """
    Dados para gr√°fico pizza/barras: breakdown de despesas
    - Por categoria
    - Fixas vs √önicas
    """
    try:
        dados = await financeiro.get_grafico_despesas_breakdown(periodo)
        return dados
    except Exception as e:
        logger.error(f"Erro ao gerar gr√°fico breakdown: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========== INTEGRA√á√ÉO YOUTUBE ==========

@app.get("/api/financeiro/youtube-revenue")
async def youtube_revenue_financeiro(periodo: str = "30d"):
    """Consulta receita YouTube do per√≠odo"""
    try:
        revenue = await financeiro.get_youtube_revenue(periodo)
        return {"receita_youtube": revenue, "periodo": periodo}
    except Exception as e:
        logger.error(f"Erro ao consultar receita YouTube: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/projecao-mes")
async def projecao_mes_financeiro():
    """Retorna proje√ß√£o de receita para o m√™s atual"""
    try:
        projecao = await financeiro.get_projecao_mes()
        return projecao
    except Exception as e:
        logger.error(f"Erro ao calcular proje√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/financeiro/comparacao-mensal")
async def comparacao_mensal_financeiro(meses: int = 6):
    """Retorna compara√ß√£o m√™s a m√™s dos √∫ltimos N meses"""
    try:
        comparacao = await financeiro.get_comparacao_mensal(meses)
        return {"meses": comparacao}
    except Exception as e:
        logger.error(f"Erro ao consultar receita YouTube: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/financeiro/sync-youtube")
async def sync_youtube_financeiro(periodo: str = "90d"):
    """
    Sincroniza receita YouTube:
    - Consulta yt_daily_metrics
    - Agrupa por m√™s
    - Cria lan√ßamentos autom√°ticos
    """
    try:
        resultado = await financeiro.sync_youtube_revenue(periodo)
        return resultado
    except Exception as e:
        logger.error(f"Erro ao sincronizar YouTube: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ========================================
# üì§ YOUTUBE UPLOAD AUTOMATION
# ========================================

class WebhookUploadRequest(BaseModel):
    """Request do webhook da planilha Google Sheets"""
    video_url: str
    titulo: str
    descricao: str  # COM hashtags
    channel_id: str
    subnicho: str
    lingua: Optional[str] = None
    nome_canal: Optional[str] = None
    sheets_row: int
    spreadsheet_id: str

async def process_upload_task(upload_id: int, max_retries=3):
    """
    Processa upload com retry autom√°tico e controle de concorr√™ncia.

    - M√°ximo 3 uploads simult√¢neos (semaphore)
    - At√© 3 tentativas em caso de falha
    - Aguarda: 15s entre tentativa 1‚Üí2, 30s entre tentativa 2‚Üí3
    """

    # Aguarda se j√° tiver 3 uploads rodando (controle de concorr√™ncia)
    async with upload_semaphore:

        for attempt in range(1, max_retries + 1):
            try:
                # Busca dados do upload
                upload = get_upload_by_id(upload_id)

                if not upload:
                    logger.error(f"Upload {upload_id} n√£o encontrado no banco")
                    return

                channel_id = upload['channel_id']
                logger.info(f"[{channel_id}] üì§ Tentativa {attempt}/{max_retries} (upload_id: {upload_id})")

                # Atualiza retry_count se retry
                if attempt > 1:
                    supabase.table('yt_upload_queue')\
                        .update({
                            'retry_count': attempt,
                            'last_retry_at': datetime.now(timezone.utc).isoformat()
                        })\
                        .eq('id', upload_id)\
                        .execute()
                    logger.info(f"[{channel_id}] üîÑ Retry #{attempt} ap√≥s falha anterior")

                # FASE 1: Download
                update_upload_status(upload_id, 'downloading')
                logger.info(f"[{channel_id}] üì• Baixando v√≠deo do Drive...")
                video_path = uploader.download_video(upload['video_url'], channel_id=channel_id)

                # FASE 2: Upload
                update_upload_status(upload_id, 'uploading')
                logger.info(f"[{channel_id}] ‚¨ÜÔ∏è  Fazendo upload para YouTube...")

                result = uploader.upload_to_youtube(
                    channel_id=upload['channel_id'],
                    video_path=video_path,
                    metadata={
                        'titulo': upload['titulo'],
                        'descricao': upload['descricao']  # COM #hashtags
                    }
                )

                # FASE 3: Sucesso
                update_upload_status(
                    upload_id,
                    'completed',
                    youtube_video_id=result['video_id']
                )

                # FASE 4: Atualiza planilha Google Sheets
                logger.info(f"[{channel_id}] üìä Atualizando planilha (row {upload['sheets_row_number']})")
                update_upload_status_in_sheet(
                    spreadsheet_id=upload['spreadsheet_id'],
                    row=upload['sheets_row_number'],
                    status='‚úÖ done'
                )
                logger.info(f"[{channel_id}] ‚úÖ Planilha atualizada: ‚úÖ done")

                # FASE 4.5: Registrar no hist√≥rico (NOVO)
                try:
                    from daily_uploader import DailyUploader
                    from datetime import date

                    uploader_instance = DailyUploader()
                    uploader_instance._registrar_canal_diario(
                        channel_id=channel_id,
                        channel_name=upload.get('channel_name', ''),
                        data=date.today(),
                        status='sucesso',
                        erro_mensagem=None,
                        tentativa_numero=99,  # 99 = upload manual
                        upload_id=upload_id,
                        video_titulo=upload.get('titulo', ''),
                        video_url=f"https://youtube.com/watch?v={result['video_id']}"
                    )
                    logger.info(f"[{channel_id}] üìù Hist√≥rico registrado (upload manual)")
                except Exception as hist_error:
                    logger.warning(f"[{channel_id}] ‚ö†Ô∏è N√£o foi poss√≠vel registrar hist√≥rico: {hist_error}")

                # FASE 5: Cleanup
                uploader.cleanup(video_path)

                logger.info(f"[{channel_id}] ‚úÖ Upload completo na tentativa {attempt} (video_id: {result['video_id']})")
                return  # Sucesso - sai do loop

            except Exception as e:
                logger.error(f"[{channel_id}] ‚ùå Erro na tentativa {attempt}/{max_retries}: {e}")

                # Se √∫ltima tentativa, marca como failed
                if attempt == max_retries:
                    error_msg = f"Falhou ap√≥s {max_retries} tentativas: {str(e)}"
                    logger.error(f"[{channel_id}] üíî {error_msg}")

                    # Busca retry_count atual do banco (TOTAL de tentativas incluindo scanner retries)
                    current_upload = get_upload_by_id(upload_id)
                    total_retry_count = current_upload.get('retry_count', 0) if current_upload else 0

                    update_upload_status(
                        upload_id,
                        'failed',
                        error_message=error_msg,
                        retry_count=total_retry_count + 1  # Incrementa contador total
                    )

                    # Registrar erro no hist√≥rico (NOVO)
                    try:
                        from daily_uploader import DailyUploader
                        from datetime import date

                        uploader_instance = DailyUploader()
                        uploader_instance._registrar_canal_diario(
                            channel_id=channel_id,
                            channel_name=upload.get('channel_name', ''),
                            data=date.today(),
                            status='erro',
                            erro_mensagem=error_msg,
                            tentativa_numero=99,  # 99 = upload manual
                            upload_id=upload_id,
                            video_titulo=upload.get('titulo', ''),
                            video_url=upload.get('video_url', '')
                        )
                        logger.info(f"[{channel_id}] üìù Hist√≥rico de erro registrado (upload manual)")
                    except Exception as hist_error:
                        logger.warning(f"[{channel_id}] ‚ö†Ô∏è N√£o foi poss√≠vel registrar erro no hist√≥rico: {hist_error}")

                    # Atualiza planilha com erro
                    # Se j√° tentou 3 vezes TOTAL (incluindo retries do scanner), marca "‚ùå Erro Final"
                    try:
                        if upload and upload.get('spreadsheet_id') and upload.get('sheets_row_number'):
                            # 0 = primeira falha (permite 2 retries), 1 = segunda falha (permite 1 retry), 2 = terceira falha (FINAL)
                            if total_retry_count >= 2:
                                logger.info(f"[{channel_id}] üìä Planilha atualizada: ‚ùå Erro Final (3 tentativas esgotadas)")
                                update_upload_status_in_sheet(
                                    spreadsheet_id=upload['spreadsheet_id'],
                                    row=upload['sheets_row_number'],
                                    status='‚ùå Erro Final'
                                )
                            else:
                                logger.info(f"[{channel_id}] üìä Planilha atualizada: ‚ùå Erro (retry {total_retry_count + 1}/3)")
                                update_upload_status_in_sheet(
                                    spreadsheet_id=upload['spreadsheet_id'],
                                    row=upload['sheets_row_number'],
                                    status='‚ùå Erro'
                                )
                    except Exception as sheet_error:
                        logger.error(f"[{channel_id}] ‚ö†Ô∏è Erro ao atualizar planilha: {sheet_error}")

                    # Cleanup se arquivo existe
                    try:
                        if 'video_path' in locals():
                            uploader.cleanup(video_path)
                    except:
                        pass

                else:
                    # N√£o √© √∫ltima tentativa - aguarda antes do retry
                    wait_time = 15 if attempt == 1 else 30  # 15s entre 1‚Üí2, 30s entre 2‚Üí3
                    logger.info(f"[{channel_id}] ‚è≥ Aguardando {wait_time}s antes do retry #{attempt+1}...")
                    await asyncio.sleep(wait_time)

@app.post("/api/yt-upload/webhook")
async def webhook_new_video(
    request: WebhookUploadRequest,
    background_tasks: BackgroundTasks
):
    """
    Recebe webhook da planilha Google Sheets quando adiciona novo v√≠deo.
    Adiciona na fila e inicia processamento em background.
    """
    try:
        logger.info(f"üì© Webhook recebido: {request.titulo[:50]}...")

        # Cria upload na fila
        upload = create_upload(
            channel_id=request.channel_id,
            video_url=request.video_url,
            titulo=request.titulo,  # EXATO da planilha
            descricao=request.descricao,  # EXATO da planilha (COM #hashtags)
            subnicho=request.subnicho,
            sheets_row=request.sheets_row,
            spreadsheet_id=request.spreadsheet_id
        )

        # Agenda processamento em background
        background_tasks.add_task(process_upload_task, upload['id'])

        return {
            'success': True,
            'upload_id': upload['id'],
            'message': 'Upload adicionado na fila'
        }

    except Exception as e:
        logger.error(f"Erro webhook: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/yt-upload/status/{upload_id}")
async def get_upload_status(upload_id: int):
    """Consulta status de um upload espec√≠fico"""
    upload = get_upload_by_id(upload_id)

    if not upload:
        raise HTTPException(status_code=404, detail="Upload n√£o encontrado")

    return upload

@app.get("/api/yt-upload/recent")
async def get_recent_uploads(limit: int = 50):
    """Lista uploads recentes (para dashboard)"""
    result = supabase.table('yt_upload_queue')\
        .select('*')\
        .order('created_at', desc=True)\
        .limit(limit)\
        .execute()

    return result.data

@app.get("/api/yt-upload/queue")
async def get_queue_status():
    """Status geral da fila de uploads"""

    # Conta por status
    pending = supabase.table('yt_upload_queue')\
        .select('*', count='exact')\
        .eq('status', 'pending')\
        .execute()

    processing = supabase.table('yt_upload_queue')\
        .select('*', count='exact')\
        .in_('status', ['downloading', 'uploading'])\
        .execute()


@app.get("/api/yt-upload/channels")
async def get_upload_channels():
    """
    Lista canais com upload autom√°tico ativado.
    √ötil para ver qual √© o √∫ltimo canal adicionado.
    """
    result = supabase.table('yt_channels')\
        .select('channel_id, channel_name, spreadsheet_id, upload_automatico, is_monetized, created_at')\
        .eq('is_active', True)\
        .eq('upload_automatico', True)\
        .order('created_at', desc=True)\
        .execute()

    return {
        'total': len(result.data),
        'channels': result.data
    }

    completed = supabase.table('yt_upload_queue')\
        .select('*', count='exact')\
        .eq('status', 'completed')\
        .execute()

    failed = supabase.table('yt_upload_queue')\
        .select('*', count='exact')\
        .eq('status', 'failed')\
        .execute()

    return {
        'pending': pending.count or 0,
        'processing': processing.count or 0,
        'completed': completed.count or 0,
        'failed': failed.count or 0
    }


@app.post("/api/yt-upload/force/{channel_id}")
async def force_upload_for_channel(channel_id: str, background_tasks: BackgroundTasks):
    """
    Forca o upload para um canal especifico.
    Usa o mesmo fluxo do upload automatico das 05:00 AM.

    Args:
        channel_id: ID do canal do YouTube (UCxxxxxxxxx)
    """
    try:
        from daily_uploader import DailyUploader

        # Verificar se canal existe e tem upload automatico ativo
        canal = supabase.table('yt_channels')\
            .select('*')\
            .eq('channel_id', channel_id)\
            .execute()

        if not canal.data:
            raise HTTPException(status_code=404, detail=f"Canal {channel_id} nao encontrado")

        canal_data = canal.data[0]

        if not canal_data.get('upload_automatico'):
            raise HTTPException(status_code=400, detail=f"Canal {canal_data['channel_name']} nao tem upload automatico ativado")

        if not canal_data.get('spreadsheet_id'):
            raise HTTPException(status_code=400, detail=f"Canal {canal_data['channel_name']} nao tem planilha configurada")

        # NOVA VERIFICA√á√ÉO: Verificar se tem v√≠deo dispon√≠vel ANTES de criar background task
        logger.info(f"Verificando se h√° v√≠deo dispon√≠vel para {canal_data['channel_name']}...")
        uploader = DailyUploader()

        # Limpar cache da planilha para for√ßar busca atualizada
        if canal_data['spreadsheet_id'] in SPREADSHEET_CACHE:
            del SPREADSHEET_CACHE[canal_data['spreadsheet_id']]
            logger.info(f"Cache da planilha limpo para {canal_data['channel_name']}")

        # Verificar se tem v√≠deo pronto na planilha
        # NOTA: _find_ready_video √© async, deve ser chamado com await direto (n√£o run_in_threadpool)
        try:
            video_pronto = await uploader._find_ready_video(
                canal_data['spreadsheet_id'],
                canal_data['channel_name']
            )

            # Se n√£o tiver v√≠deo, retornar imediatamente com status 'sem_video'
            if not video_pronto:
                logger.warning(f"Nenhum v√≠deo dispon√≠vel na planilha de {canal_data['channel_name']}")
                return {
                    'status': 'sem_video',
                    'message': f'Nenhum v√≠deo dispon√≠vel na planilha de {canal_data["channel_name"]}',
                    'channel_id': channel_id
                }

            logger.info(f"V√≠deo encontrado: {video_pronto.get('titulo', 'Sem t√≠tulo')}")

        except Exception as e:
            logger.error(f"Erro ao verificar v√≠deo dispon√≠vel: {e}")
            return {
                'status': 'sem_video',
                'message': f'Erro ao verificar planilha de {canal_data["channel_name"]}: {str(e)[:100]}',
                'channel_id': channel_id
            }

        # Se chegou aqui, tem v√≠deo dispon√≠vel - executar upload em background
        async def run_upload():
            try:
                uploader = DailyUploader()
                from datetime import date

                # Usar o mesmo fluxo do daily_uploader que ja funciona
                logger.info(f"Buscando video pronto para {canal_data['channel_name']}...")

                # _process_canal_upload ja faz:
                # 1. Verifica se ja fez upload hoje (opcional para force)
                # 2. Busca video pronto na planilha via _find_ready_video()
                # 3. Adiciona na fila e processa

                # Usa data de hoje para o registro
                hoje = date.today()
                resultado = await uploader._process_canal_upload(canal_data, hoje, retry_attempt=1)

                logger.info(f"Resultado upload {canal_data['channel_name']}: {resultado}")

                if resultado.get('status') == 'sem_video':
                    logger.warning(f"Nenhum video pronto na planilha de {canal_data['channel_name']}")
                elif resultado.get('status') == 'erro':
                    logger.error(f"Erro no upload: {resultado.get('error')}")
                elif resultado.get('status') == 'sucesso':
                    logger.info(f"Upload realizado com sucesso: {resultado.get('video_title')}")

            except Exception as e:
                logger.error(f"Erro no upload forcado: {e}")

        background_tasks.add_task(run_upload)

        return {
            'status': 'processing',
            'message': f'Upload iniciado para {canal_data["channel_name"]}',
            'channel_id': channel_id,
            'spreadsheet_id': canal_data['spreadsheet_id']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao forcar upload: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/yt-upload/historico-completo")
async def get_historico_completo(
    days: int = 30,
    status_filter: Optional[str] = None  # sucesso, erro, sem_video
):
    """
    Retorna hist√≥rico completo de uploads de TODOS os canais

    Query params:
        - days: √öltimos X dias (padr√£o: 30)
        - status_filter: Filtrar por status (opcional)

    Returns:
        {
            "total_dias": 30,
            "total_registros": 150,
            "historico_por_data": [
                {
                    "data": "2026-02-11",
                    "total_uploads": 3,
                    "sucesso": 3,
                    "erro": 0,
                    "sem_video": 17,
                    "canais": [
                        {
                            "channel_name": "Canal X",
                            "video_titulo": "Video Y",
                            "status": "sucesso",
                            "hora": "05:30:15",
                            "tentativa_numero": 1
                        }
                    ]
                }
            ]
        }
    """
    try:
        from datetime import date, timedelta

        hoje = date.today()
        data_inicio = hoje - timedelta(days=days)

        # Query base
        query = supabase.table('yt_canal_upload_historico')\
            .select('*')\
            .gte('data', data_inicio.isoformat())\
            .order('data', desc=True)\
            .order('hora_processamento', desc=True)

        # Filtro opcional de status
        if status_filter:
            query = query.eq('status', status_filter)

        result = query.execute()

        # Agrupar por data
        historico_por_data = {}
        for record in result.data:
            data_str = record['data']

            if data_str not in historico_por_data:
                historico_por_data[data_str] = {
                    'data': data_str,
                    'total_uploads': 0,
                    'sucesso': 0,
                    'erro': 0,
                    'sem_video': 0,
                    'canais': []
                }

            # Contadores
            historico_por_data[data_str]['total_uploads'] += 1
            status = record.get('status', 'pendente')
            if status in ['sucesso', 'erro', 'sem_video']:
                historico_por_data[data_str][status] += 1

            # Dados do canal
            hora = record.get('hora_processamento', '')
            if hora:
                # Converter para hor√°rio de Bras√≠lia (UTC-3)
                from datetime import datetime
                import pytz

                try:
                    # Parse do timestamp UTC
                    dt_utc = datetime.fromisoformat(hora.replace('Z', '+00:00'))
                    # Converter para Bras√≠lia
                    brasil_tz = pytz.timezone('America/Sao_Paulo')
                    dt_brasil = dt_utc.astimezone(brasil_tz)
                    hora_formatada = dt_brasil.strftime('%H:%M:%S')
                except:
                    hora_formatada = hora.split('T')[1][:8] if 'T' in hora else hora[:8] if hora else '-'
            else:
                hora_formatada = '-'

            historico_por_data[data_str]['canais'].append({
                'channel_name': record.get('channel_name', '-'),
                'video_titulo': record.get('video_titulo', '-'),
                'status': status,
                'hora': hora_formatada,
                'tentativa_numero': record.get('tentativa_numero', 1),
                'youtube_video_id': record.get('youtube_video_id'),
                'erro_mensagem': record.get('erro_mensagem')
            })

        # Converter dict para lista ordenada
        historico_lista = sorted(
            historico_por_data.values(),
            key=lambda x: x['data'],
            reverse=True
        )

        return {
            'total_dias': len(historico_lista),
            'total_registros': len(result.data),
            'data_inicio': data_inicio.isoformat(),
            'data_fim': hoje.isoformat(),
            'historico_por_data': historico_lista
        }

    except Exception as e:
        logger.error(f"Erro ao buscar hist√≥rico completo: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/yt-upload/failed")
async def get_failed_uploads(limit: int = 50):
    """
    Lista uploads que falharam (status='failed').
    Mostra detalhes do erro e quantas tentativas foram feitas.
    """
    result = supabase.table('yt_upload_queue')\
        .select('id, channel_id, titulo, status, error_message, retry_count, created_at, completed_at, spreadsheet_id, sheets_row_number')\
        .eq('status', 'failed')\
        .order('completed_at', desc=True)\
        .limit(limit)\
        .execute()

    # Enriquece com nome do canal
    uploads = []
    for upload in result.data:
        channel = supabase.table('yt_channels')\
            .select('channel_name')\
            .eq('channel_id', upload['channel_id'])\
            .limit(1)\
            .execute()

        upload['channel_name'] = channel.data[0]['channel_name'] if channel.data else 'Desconhecido'
        upload['can_retry'] = (upload.get('retry_count', 0) or 0) < 3
        uploads.append(upload)

    return {
        'total': len(uploads),
        'uploads': uploads
    }


@app.post("/api/yt-upload/retry/{upload_id}")
async def retry_failed_upload(upload_id: int, background_tasks: BackgroundTasks):
    """
    Reprocessa um upload espec√≠fico que falhou.

    - Verifica se upload existe e est√° com status='failed'
    - Verifica se retry_count < 3
    - Reseta status para 'pending' e processa novamente
    """
    try:
        # Busca upload
        upload = supabase.table('yt_upload_queue')\
            .select('*')\
            .eq('id', upload_id)\
            .limit(1)\
            .execute()

        if not upload.data:
            raise HTTPException(status_code=404, detail=f"Upload {upload_id} n√£o encontrado")

        upload_data = upload.data[0]

        # Verifica status
        if upload_data['status'] != 'failed':
            raise HTTPException(
                status_code=400,
                detail=f"Upload {upload_id} n√£o est√° com status 'failed' (atual: {upload_data['status']})"
            )

        # Verifica retry_count
        retry_count = upload_data.get('retry_count', 0) or 0
        if retry_count >= 3:
            raise HTTPException(
                status_code=400,
                detail=f"Upload {upload_id} j√° atingiu limite de 3 tentativas"
            )

        # Reseta status para pending
        supabase.table('yt_upload_queue').update({
            'status': 'pending',
            'error_message': None,
            'started_at': None,
            'completed_at': None
        }).eq('id', upload_id).execute()

        logger.info(f"üîÅ Retry manual para upload {upload_id} (tentativa {retry_count + 1}/3)")

        # Processa em background
        background_tasks.add_task(process_upload_task, upload_id)

        return {
            'status': 'processing',
            'message': f'Retry iniciado para upload {upload_id}',
            'upload_id': upload_id,
            'retry_count': retry_count + 1,
            'titulo': upload_data['titulo'],
            'channel_id': upload_data['channel_id']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao reprocessar upload {upload_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/yt-upload/retry-all-failed")
async def retry_all_failed_uploads(background_tasks: BackgroundTasks):
    """
    Reprocessa TODOS os uploads que falharam e ainda podem ser retentados.

    - Apenas uploads com status='failed' e retry_count < 3
    - Processa um por vez em sequ√™ncia
    """
    try:
        # Busca todos os uploads failed que podem ser retentados
        result = supabase.table('yt_upload_queue')\
            .select('id, channel_id, titulo, retry_count')\
            .eq('status', 'failed')\
            .lt('retry_count', 3)\
            .execute()

        if not result.data:
            return {
                'status': 'no_uploads',
                'message': 'Nenhum upload com erro eleg√≠vel para retry',
                'total': 0
            }

        # Reseta status de todos para pending
        upload_ids = [u['id'] for u in result.data]

        for upload_id in upload_ids:
            supabase.table('yt_upload_queue').update({
                'status': 'pending',
                'error_message': None,
                'started_at': None,
                'completed_at': None
            }).eq('id', upload_id).execute()

        logger.info(f"üîÅ Retry em massa: {len(upload_ids)} uploads reativados")

        # Processa cada um em background (com delay entre eles)
        async def process_all():
            for upload_id in upload_ids:
                await process_upload_task(upload_id)
                await asyncio.sleep(5)  # 5s entre uploads

        background_tasks.add_task(process_all)

        return {
            'status': 'processing',
            'message': f'Retry iniciado para {len(upload_ids)} uploads',
            'total': len(upload_ids),
            'upload_ids': upload_ids
        }

    except Exception as e:
        logger.error(f"Erro ao reprocessar uploads em massa: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/yt-upload/clear-old-failed")
async def clear_old_failed_uploads():
    """
    Remove uploads antigos que falharam e j√° atingiram o limite de retry.

    - Deleta uploads com status='failed' e retry_count >= 3
    - Usado para limpar a fila de uploads antigos que n√£o ser√£o mais retentados
    """
    try:
        # Busca uploads failed com retry_count >= 3
        result = supabase.table('yt_upload_queue')\
            .select('id, titulo, channel_id, created_at')\
            .eq('status', 'failed')\
            .gte('retry_count', 3)\
            .execute()

        if not result.data:
            return {
                'status': 'no_uploads',
                'message': 'Nenhum upload antigo com erro para limpar',
                'deleted': 0
            }

        # Deleta todos
        upload_ids = [u['id'] for u in result.data]

        for upload_id in upload_ids:
            supabase.table('yt_upload_queue')\
                .delete()\
                .eq('id', upload_id)\
                .execute()

        logger.info(f"üóëÔ∏è Limpeza: {len(upload_ids)} uploads antigos com erro removidos")

        return {
            'status': 'success',
            'message': f'{len(upload_ids)} uploads antigos com erro removidos',
            'deleted': len(upload_ids)
        }

    except Exception as e:
        logger.error(f"Erro ao limpar uploads antigos: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================
# SISTEMA KANBAN - FUN√á√ïES
# =====================================================

async def get_kanban_structure():
    """
    Retorna a estrutura completa do Kanban com cards, subnichos e canais.
    Apenas canais tipo='nosso'.
    """
    try:
        # Buscar todos os canais nossos
        canais = db.supabase.table("canais_monitorados")\
            .select("*")\
            .eq("tipo", "nosso")\
            .order("subnicho", desc=False)\
            .order("nome_canal", desc=False)\
            .execute()

        # Separar monetizados e n√£o monetizados
        monetizados = []
        nao_monetizados = []

        for canal in canais.data:
            # Calcular dias no status
            if canal.get("kanban_status_since"):
                try:
                    # Tentar v√°rios formatos de data
                    date_str = canal["kanban_status_since"]
                    if "Z" in date_str:
                        date_str = date_str.replace("Z", "+00:00")

                    # Adicionar zeros se necess√°rio para microsegundos
                    if "." in date_str:
                        parts = date_str.split(".")
                        microsec_part = parts[1].split("+")[0]
                        # Garantir 6 d√≠gitos nos microsegundos
                        if len(microsec_part) < 6:
                            microsec_part = microsec_part.ljust(6, '0')
                        date_str = f"{parts[0]}.{microsec_part}+00:00"

                    status_date = datetime.fromisoformat(date_str)
                    dias_no_status = (datetime.now(timezone.utc) - status_date).days
                except Exception as e:
                    logger.warning(f"Erro ao processar data: {e}")
                    dias_no_status = 0
            else:
                dias_no_status = 0

            # Mapear status para label e cor
            status_map = {
                # N√£o monetizados
                "em_teste_inicial": ("Em Teste Inicial", "yellow"),
                "demonstrando_tracao": ("Demonstrando Tra√ß√£o", "green"),
                "em_andamento": ("Em Andamento p/ Monetizar", "orange"),
                "monetizado": ("Monetizado", "blue"),
                # Monetizados
                "em_crescimento": ("Em Crescimento", "green"),
                "em_testes_novos": ("Em Testes Novos", "yellow"),
                "canal_constante": ("Canal Constante", "blue"),
            }

            status_info = status_map.get(canal.get("kanban_status"), ("Sem Status", "gray"))

            # Buscar total de notas do canal
            notas = db.supabase.table("kanban_notes")\
                .select("id")\
                .eq("canal_id", canal["id"])\
                .execute()

            canal_info = {
                "id": canal["id"],
                "nome": canal["nome_canal"],
                "subnicho": canal["subnicho"],
                "lingua": canal.get("lingua", ""),
                "url_canal": canal.get("url_canal", ""),
                "kanban_status": canal.get("kanban_status"),
                "status_label": status_info[0],
                "status_color": status_info[1],
                "status_since": canal.get("kanban_status_since"),
                "dias_no_status": dias_no_status,
                "total_notas": len(notas.data) if notas.data else 0
            }

            if canal.get("monetizado"):
                monetizados.append(canal_info)
            else:
                nao_monetizados.append(canal_info)

        # Agrupar por subnicho
        def agrupar_por_subnicho(canais_list):
            subnichos = {}
            for canal in canais_list:
                subnicho = canal["subnicho"] or "Sem Subnicho"
                if subnicho not in subnichos:
                    subnichos[subnicho] = {
                        "nome": subnicho,
                        "total": 0,
                        "canais": []
                    }
                subnichos[subnicho]["canais"].append(canal)
                subnichos[subnicho]["total"] += 1
            return subnichos

        return {
            "monetizados": {
                "total": len(monetizados),
                "subnichos": agrupar_por_subnicho(monetizados)
            },
            "nao_monetizados": {
                "total": len(nao_monetizados),
                "subnichos": agrupar_por_subnicho(nao_monetizados)
            }
        }

    except Exception as e:
        logger.error(f"Erro ao buscar estrutura Kanban: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def get_kanban_board(canal_id: int):
    """
    Retorna o quadro Kanban individual de um canal.
    """
    try:
        # Buscar dados do canal
        canal = db.supabase.table("canais_monitorados")\
            .select("*")\
            .eq("id", canal_id)\
            .eq("tipo", "nosso")\
            .single()\
            .execute()

        if not canal.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        # Calcular dias no status
        dias_no_status = 0
        if canal.data.get("kanban_status_since"):
            try:
                # Tentar v√°rios formatos de data
                date_str = canal.data["kanban_status_since"]
                if "Z" in date_str:
                    date_str = date_str.replace("Z", "+00:00")

                # Adicionar zeros se necess√°rio para microsegundos
                if "." in date_str:
                    parts = date_str.split(".")
                    microsec_part = parts[1].split("+")[0]
                    # Garantir 6 d√≠gitos nos microsegundos
                    if len(microsec_part) < 6:
                        microsec_part = microsec_part.ljust(6, '0')
                    date_str = f"{parts[0]}.{microsec_part}+00:00"

                status_date = datetime.fromisoformat(date_str)
                dias_no_status = (datetime.now(timezone.utc) - status_date).days
            except Exception as e:
                logger.warning(f"Erro ao processar data: {e}")
                dias_no_status = 0

        # Definir colunas baseado se √© monetizado ou n√£o
        if canal.data.get("monetizado"):
            colunas = [
                {
                    "id": "em_crescimento",
                    "label": "Em Crescimento",
                    "emoji": "üü¢",
                    "descricao": "Canal saud√°vel e escalando",
                    "is_current": canal.data.get("kanban_status") == "em_crescimento"
                },
                {
                    "id": "em_testes_novos",
                    "label": "Em Testes Novos",
                    "emoji": "üü°",
                    "descricao": "Perdeu tra√ß√£o, testando novas estrat√©gias",
                    "is_current": canal.data.get("kanban_status") == "em_testes_novos"
                },
                {
                    "id": "canal_constante",
                    "label": "Canal Constante",
                    "emoji": "üîµ",
                    "descricao": "Est√°vel, performance previs√≠vel",
                    "is_current": canal.data.get("kanban_status") == "canal_constante"
                }
            ]
        else:
            colunas = [
                {
                    "id": "em_teste_inicial",
                    "label": "Em Teste Inicial",
                    "emoji": "üü°",
                    "descricao": "Canal testando micro-nichos pela primeira vez",
                    "is_current": canal.data.get("kanban_status") == "em_teste_inicial"
                },
                {
                    "id": "demonstrando_tracao",
                    "label": "Demonstrando Tra√ß√£o",
                    "emoji": "üü¢",
                    "descricao": "Sinais positivos, v√≠deos viralizando",
                    "is_current": canal.data.get("kanban_status") == "demonstrando_tracao"
                },
                {
                    "id": "em_andamento",
                    "label": "Em Andamento p/ Monetizar",
                    "emoji": "üü†",
                    "descricao": "Caminhando para 1K subs e 4K horas",
                    "is_current": canal.data.get("kanban_status") == "em_andamento"
                },
                {
                    "id": "monetizado",
                    "label": "Monetizado",
                    "emoji": "üîµ",
                    "descricao": "Atingiu requisitos de monetiza√ß√£o",
                    "is_current": canal.data.get("kanban_status") == "monetizado"
                }
            ]

        # Buscar notas do canal
        notas = db.supabase.table("kanban_notes")\
            .select("*")\
            .eq("canal_id", canal_id)\
            .order("position", desc=False)\
            .execute()

        # Buscar hist√≥rico (√∫ltimos 20 registros n√£o deletados)
        historico = db.supabase.table("kanban_history")\
            .select("*")\
            .eq("canal_id", canal_id)\
            .eq("is_deleted", False)\
            .order("performed_at", desc=True)\
            .limit(20)\
            .execute()

        return {
            "canal": {
                "id": canal.data["id"],
                "nome": canal.data["nome_canal"],
                "subnicho": canal.data["subnicho"],
                "monetizado": canal.data.get("monetizado", False),
                "status_atual": canal.data.get("kanban_status"),
                "status_since": canal.data.get("kanban_status_since"),
                "dias_no_status": dias_no_status
            },
            "colunas": colunas,
            "notas": notas.data if notas.data else [],
            "historico": historico.data if historico.data else []
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao buscar kanban board: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def move_kanban_status(canal_id: int, new_status: str):
    """
    Move um canal para um novo status no Kanban.
    """
    try:
        # Verificar se o canal existe e √© nosso
        canal = db.supabase.table("canais_monitorados")\
            .select("id, nome_canal, kanban_status")\
            .eq("id", canal_id)\
            .eq("tipo", "nosso")\
            .single()\
            .execute()

        if not canal.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        old_status = canal.data.get("kanban_status")

        # Atualizar status
        result = db.supabase.table("canais_monitorados")\
            .update({
                "kanban_status": new_status,
                "kanban_status_since": datetime.now(timezone.utc).isoformat()
            })\
            .eq("id", canal_id)\
            .execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": canal_id,
            "action_type": "status_change",
            "description": f"Status mudou de {old_status or 'indefinido'} para {new_status}",
            "details": {
                "from_status": old_status,
                "to_status": new_status
            }
        }).execute()

        return {"success": True, "message": "Status atualizado com sucesso"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao mover status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def create_kanban_note(canal_id: int, note_text: str, note_color: str = "yellow", coluna_id: Optional[str] = None):
    """
    Cria uma nova nota para um canal.
    Agora suporta coluna_id para permitir notas em qualquer coluna.
    """
    try:
        # Verificar se o canal existe e √© nosso
        canal = db.supabase.table("canais_monitorados")\
            .select("id, nome_canal")\
            .eq("id", canal_id)\
            .eq("tipo", "nosso")\
            .single()\
            .execute()

        if not canal.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        # Buscar √∫ltima posi√ß√£o
        last_note = db.supabase.table("kanban_notes")\
            .select("position")\
            .eq("canal_id", canal_id)\
            .order("position", desc=True)\
            .limit(1)\
            .execute()

        next_position = 1
        if last_note.data:
            next_position = last_note.data[0]["position"] + 1

        # Criar nota com suporte a coluna_id
        nota_data = {
            "canal_id": canal_id,
            "note_text": note_text,
            "note_color": note_color,
            "position": next_position
        }

        # Adicionar coluna_id se fornecido
        if coluna_id:
            nota_data["coluna_id"] = coluna_id

        nota = db.supabase.table("kanban_notes").insert(nota_data).execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": canal_id,
            "action_type": "note_added",
            "description": f"Nota {note_color} adicionada",
            "details": {
                "note_id": nota.data[0]["id"],
                "color": note_color
            }
        }).execute()

        return nota.data[0]

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao criar nota: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def update_kanban_note(note_id: int, note_text: Optional[str] = None, note_color: Optional[str] = None, coluna_id: Optional[str] = None):
    """
    Atualiza uma nota existente.
    Agora suporta mover nota entre colunas com coluna_id.
    """
    try:
        # Buscar nota atual
        nota_atual = db.supabase.table("kanban_notes")\
            .select("*")\
            .eq("id", note_id)\
            .single()\
            .execute()

        if not nota_atual.data:
            raise HTTPException(status_code=404, detail="Nota n√£o encontrada")

        # Preparar campos para atualizar
        update_fields = {"updated_at": datetime.now(timezone.utc).isoformat()}
        if note_text is not None:
            update_fields["note_text"] = note_text
        if note_color is not None:
            update_fields["note_color"] = note_color
        if coluna_id is not None:
            update_fields["coluna_id"] = coluna_id

        # Atualizar nota
        result = db.supabase.table("kanban_notes")\
            .update(update_fields)\
            .eq("id", note_id)\
            .execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": nota_atual.data["canal_id"],
            "action_type": "note_edited",
            "description": f"Nota editada",
            "details": {
                "note_id": note_id,
                "old_color": nota_atual.data["note_color"],
                "new_color": note_color or nota_atual.data["note_color"]
            }
        }).execute()

        return result.data[0]

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao atualizar nota: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def delete_kanban_note(note_id: int):
    """
    Deleta uma nota.
    """
    try:
        # Buscar nota antes de deletar
        nota = db.supabase.table("kanban_notes")\
            .select("canal_id, note_color")\
            .eq("id", note_id)\
            .single()\
            .execute()

        if not nota.data:
            raise HTTPException(status_code=404, detail="Nota n√£o encontrada")

        # Deletar nota
        db.supabase.table("kanban_notes")\
            .delete()\
            .eq("id", note_id)\
            .execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": nota.data["canal_id"],
            "action_type": "note_deleted",
            "description": f"Nota {nota.data['note_color']} removida",
            "details": {
                "note_id": note_id,
                "color": nota.data["note_color"]
            }
        }).execute()

        return {"success": True, "message": "Nota deletada com sucesso"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao deletar nota: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def move_note_to_column(note_id: int, target_column: str):
    """
    Move uma nota para outra coluna.
    """
    try:
        # Buscar nota atual
        nota_atual = db.supabase.table("kanban_notes")\
            .select("*")\
            .eq("id", note_id)\
            .single()\
            .execute()

        if not nota_atual.data:
            raise HTTPException(status_code=404, detail="Nota n√£o encontrada")

        old_column = nota_atual.data.get("coluna_id")

        # Atualizar coluna da nota
        result = db.supabase.table("kanban_notes")\
            .update({"coluna_id": target_column})\
            .eq("id", note_id)\
            .execute()

        if not result.data:
            raise HTTPException(status_code=400, detail="Erro ao mover nota")

        # Buscar informa√ß√µes do canal para o hist√≥rico
        canal = db.supabase.table("canais_monitorados")\
            .select("nome_canal")\
            .eq("id", nota_atual.data["canal_id"])\
            .single()\
            .execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": nota_atual.data["canal_id"],
            "action_type": "note_moved",
            "description": f"Nota movida de {old_column or 'sem coluna'} para {target_column}",
            "details": {
                "note_id": note_id,
                "from_column": old_column,
                "to_column": target_column,
                "note_color": nota_atual.data["note_color"]
            }
        }).execute()

        return {
            "success": True,
            "message": "Nota movida com sucesso",
            "data": result.data[0]
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao mover nota: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def reorder_kanban_notes(canal_id: int, note_positions: List[Dict[str, int]]):
    """
    Reordena as notas de um canal.
    """
    try:
        # Verificar se o canal existe
        canal = db.supabase.table("canais_monitorados")\
            .select("id")\
            .eq("id", canal_id)\
            .eq("tipo", "nosso")\
            .single()\
            .execute()

        if not canal.data:
            raise HTTPException(status_code=404, detail="Canal n√£o encontrado")

        # Atualizar posi√ß√µes
        for item in note_positions:
            db.supabase.table("kanban_notes")\
                .update({"position": item["position"]})\
                .eq("id", item["note_id"])\
                .eq("canal_id", canal_id)\
                .execute()

        # Registrar no hist√≥rico
        db.supabase.table("kanban_history").insert({
            "canal_id": canal_id,
            "action_type": "note_reordered",
            "description": "Notas reordenadas",
            "details": {"positions": note_positions}
        }).execute()

        return {"success": True, "message": "Notas reordenadas com sucesso"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao reordenar notas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def get_kanban_history(canal_id: int, limit: int = 50):
    """
    Retorna o hist√≥rico de a√ß√µes de um canal.
    """
    try:
        historico = db.supabase.table("kanban_history")\
            .select("*")\
            .eq("canal_id", canal_id)\
            .eq("is_deleted", False)\
            .order("performed_at", desc=True)\
            .limit(limit)\
            .execute()

        return historico.data if historico.data else []

    except Exception as e:
        logger.error(f"Erro ao buscar hist√≥rico: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def delete_history_item(history_id: int):
    """
    Remove um item do hist√≥rico (soft delete).
    """
    try:
        result = db.supabase.table("kanban_history")\
            .update({"is_deleted": True})\
            .eq("id", history_id)\
            .execute()

        if not result.data:
            raise HTTPException(status_code=404, detail="Item de hist√≥rico n√£o encontrado")

        return {"success": True, "message": "Item removido do hist√≥rico"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro ao deletar item do hist√≥rico: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =====================================================
# SISTEMA KANBAN - ENDPOINTS
# =====================================================

@app.get("/api/kanban/structure")
async def kanban_structure_endpoint():
    """Retorna a estrutura completa do Kanban"""
    return await get_kanban_structure()

@app.get("/api/kanban/canal/{canal_id}/board")
async def kanban_board_endpoint(canal_id: int):
    """Retorna o quadro Kanban de um canal espec√≠fico"""
    return await get_kanban_board(canal_id)

@app.patch("/api/kanban/canal/{canal_id}/move-status")
async def kanban_move_status_endpoint(canal_id: int, request: KanbanMoveStatusRequest):
    """Move um canal para outro status"""
    return await move_kanban_status(canal_id, request.new_status)

@app.post("/api/kanban/canal/{canal_id}/note")
async def kanban_create_note_endpoint(canal_id: int, request: KanbanNoteRequest):
    """Cria uma nova nota para o canal"""
    return await create_kanban_note(canal_id, request.note_text, request.note_color, request.coluna_id)

@app.patch("/api/kanban/note/{note_id}")
async def kanban_update_note_endpoint(note_id: int, request: KanbanNoteUpdateRequest):
    """Atualiza uma nota existente"""
    return await update_kanban_note(note_id, request.note_text, request.note_color, request.coluna_id)

@app.delete("/api/kanban/note/{note_id}")
async def kanban_delete_note_endpoint(note_id: int):
    """Deleta uma nota"""
    return await delete_kanban_note(note_id)

@app.patch("/api/kanban/note/{note_id}/move")
async def kanban_move_note_endpoint(note_id: int, request: KanbanMoveNoteRequest):
    """Move uma nota para outra coluna (compat√≠vel com stage_id e coluna_id)"""
    target_column = request.target_column
    if not target_column:
        raise HTTPException(status_code=400, detail="stage_id ou coluna_id √© obrigat√≥rio")
    return await move_note_to_column(note_id, target_column)

@app.patch("/api/kanban/canal/{canal_id}/reorder-notes")
async def kanban_reorder_notes_endpoint(canal_id: int, request: KanbanReorderNotesRequest):
    """Reordena as notas de um canal"""
    return await reorder_kanban_notes(canal_id, request.note_positions)

@app.get("/api/kanban/canal/{canal_id}/history")
async def kanban_history_endpoint(canal_id: int, limit: int = 50):
    """Retorna o hist√≥rico de a√ß√µes do canal"""
    return await get_kanban_history(canal_id, limit)

@app.delete("/api/kanban/history/{history_id}")
async def kanban_delete_history_endpoint(history_id: int):
    """Remove um item do hist√≥rico (soft delete)"""
    return await delete_history_item(history_id)

# ============================================================
# DASHBOARD DE UPLOAD v2 - Integrado no main.py
# Acesso: /dash-upload
# ============================================================

def _extrair_hora(timestamp_str):
    """Extrai HH:MM do timestamp (sem conversao de timezone)"""
    if not timestamp_str:
        return None
    try:
        ts = str(timestamp_str)
        if 'T' in ts and len(ts) >= 16:
            return ts[11:16]
        return None
    except:
        return None

_dash_cache = {'data': None, 'timestamp': 0}
_DASH_CACHE_TTL = 3

DASH_UPLOAD_HTML = '''
<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <title>Upload Dashboard</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        :root {
            --bg-primary: #0a0a0b;
            --bg-secondary: #141415;
            --bg-tertiary: #1c1c1e;
            --bg-elevated: #1f1f22;
            --border-primary: #27272a;
            --border-secondary: #3f3f46;
            --text-primary: #fafafa;
            --text-secondary: #a1a1aa;
            --text-tertiary: #71717a;
            --success: #22c55e;
            --success-muted: rgba(34, 197, 94, 0.12);
            --warning: #eab308;
            --warning-muted: rgba(234, 179, 8, 0.12);
            --error: #ef4444;
            --error-muted: rgba(239, 68, 68, 0.12);
            --info: #3b82f6;
            --info-muted: rgba(59, 130, 246, 0.12);
            --pending: #a855f7;
            --pending-muted: rgba(168, 85, 247, 0.12);
            --accent: #f97316;
            --radius-sm: 6px;
            --radius-md: 8px;
            --radius-lg: 12px;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            background: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, system-ui, sans-serif;
            padding-bottom: 48px;
            min-height: 100vh;
        }
        ::-webkit-scrollbar { width: 8px; }
        ::-webkit-scrollbar-track { background: var(--bg-primary); }
        ::-webkit-scrollbar-thumb { background: var(--border-secondary); border-radius: 4px; }
        ::-webkit-scrollbar-thumb:hover { background: var(--text-tertiary); }
        .page-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 32px 16px;
            border-bottom: 1px solid var(--border-primary);
            margin-bottom: 24px;
        }
        .header-title { font-size: 22px; font-weight: 600; letter-spacing: -0.025em; color: var(--text-primary); }
        .header-subtitle { font-size: 13px; color: var(--text-tertiary); margin-top: 2px; }
        .live-indicator { display: flex; align-items: center; gap: 8px; font-size: 13px; color: var(--text-secondary); }
        .live-dot { width: 8px; height: 8px; border-radius: 50%; background: var(--success); animation: pulse-live 2s ease-in-out infinite; }
        @keyframes pulse-live {
            0%, 100% { box-shadow: 0 0 0 0 rgba(34, 197, 94, 0.4); }
            50% { box-shadow: 0 0 0 6px rgba(34, 197, 94, 0); }
        }
        .stats-grid { display: grid; grid-template-columns: repeat(5, 1fr); gap: 12px; padding: 0 32px; margin-bottom: 28px; }
        .stat-card { background: var(--bg-secondary); border: 1px solid var(--border-primary); border-radius: var(--radius-md); padding: 20px; cursor: pointer; transition: all 0.15s ease; position: relative; display: flex; flex-direction: column; justify-content: space-between; min-height: 90px; }
        .stat-card:hover { border-color: var(--border-secondary); background: var(--bg-tertiary); }
        .stat-card.active { border-left: 3px solid var(--card-accent, var(--info)); background: var(--bg-tertiary); }
        .stat-card--total { --card-accent: var(--info); }
        .stat-card--sucesso { --card-accent: var(--success); }
        .stat-card--sem_video { --card-accent: var(--warning); }
        .stat-card--erro { --card-accent: var(--error); }
        .stat-card--historico { --card-accent: var(--accent); border-style: dashed; }
        .stat-card--total:hover { border-color: var(--info); }
        .stat-card--sucesso:hover { border-color: var(--success); }
        .stat-card--sem_video:hover { border-color: var(--warning); }
        .stat-card--erro:hover { border-color: var(--error); }
        .stat-card--historico:hover { border-color: var(--accent); }
        .stat-value { font-size: 32px; font-weight: 700; line-height: 1; margin-bottom: 6px; }
        .stat-value--total { color: var(--info); }
        .stat-value--sucesso { color: var(--success); }
        .stat-value--sem_video { color: var(--warning); }
        .stat-value--erro { color: var(--error); }
        .stat-value--historico { color: var(--accent); font-size: 32px; }
        .stat-label { font-size: 11px; font-weight: 500; text-transform: uppercase; letter-spacing: 0.05em; color: var(--text-tertiary); }
        .content { padding: 0 32px; }
        .section { background: var(--bg-secondary); border: 1px solid var(--border-primary); border-radius: var(--radius-lg); margin-bottom: 16px; overflow: hidden; border-left: 3px solid var(--section-accent, var(--border-primary)); }
        .section-header { padding: 14px 20px; display: flex; justify-content: space-between; align-items: center; border-bottom: 1px solid var(--border-primary); }
        .section-title { display: flex; align-items: center; gap: 10px; font-size: 14px; font-weight: 600; }
        .section-icon { width: 28px; height: 28px; border-radius: var(--radius-sm); display: flex; align-items: center; justify-content: center; font-size: 14px; font-weight: 700; background: var(--section-accent-muted, var(--bg-tertiary)); color: var(--section-accent, var(--text-secondary)); }
        .section-count { font-size: 12px; font-weight: 400; color: var(--text-tertiary); }
        .section-pills { display: flex; gap: 8px; }
        .stat-pill { font-size: 11px; font-weight: 500; padding: 3px 10px; border-radius: 9999px; }
        .stat-pill--success { background: var(--success-muted); color: var(--success); }
        .stat-pill--warning { background: var(--warning-muted); color: var(--warning); }
        .stat-pill--error { background: var(--error-muted); color: var(--error); }
        .stat-pill--pending { background: var(--pending-muted); color: var(--pending); }
        .channel-table { width: 100%; border-collapse: collapse; }
        .channel-table th { padding: 10px 16px; text-align: left; font-size: 11px; font-weight: 500; text-transform: uppercase; letter-spacing: 0.05em; color: var(--text-tertiary); border-bottom: 1px solid var(--border-primary); background: transparent; }
        .channel-table td { padding: 10px 16px; font-size: 13px; border-bottom: 1px solid var(--border-primary); vertical-align: middle; }
        .channel-table tr:last-child td { border-bottom: none; }
        .channel-table tbody tr { transition: background 0.1s ease; }
        .channel-table tbody tr:hover { background: rgba(255, 255, 255, 0.02); }
        .cell-channel { display: flex; align-items: center; gap: 6px; }
        .channel-name { font-weight: 500; color: var(--text-primary); white-space: nowrap; }
        .lang-tag { font-size: 10px; color: var(--text-tertiary); background: var(--bg-tertiary); padding: 1px 6px; border-radius: 3px; font-weight: 500; letter-spacing: 0.02em; }
        .monetized-dot { width: 7px; height: 7px; border-radius: 50%; background: var(--success); display: inline-block; flex-shrink: 0; }
        .status-badge { display: inline-flex; align-items: center; padding: 3px 10px; border-radius: 9999px; font-size: 11px; font-weight: 600; }
        .status-badge--success { background: var(--success-muted); color: var(--success); }
        .status-badge--sem_video { background: var(--warning-muted); color: var(--warning); }
        .status-badge--error { background: var(--error-muted); color: var(--error); }
        .status-badge--pending { background: var(--pending-muted); color: var(--pending); }
        .video-title { max-width: 350px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; color: var(--text-secondary); font-size: 13px; }
        .cell-time { color: var(--text-tertiary); font-size: 13px; font-variant-numeric: tabular-nums; }
        .cell-actions { display: flex; gap: 4px; }
        .btn-icon { width: 38px; height: 38px; display: inline-flex; align-items: center; justify-content: center; border: 1px solid transparent; border-radius: var(--radius-sm); background: transparent; cursor: pointer; font-size: 20px; color: var(--text-secondary); transition: all 0.15s ease; text-decoration: none; }
        .btn-icon:hover { background: var(--bg-tertiary); border-color: var(--border-secondary); color: var(--text-primary); }
        .btn-icon:active { transform: scale(0.9); }
        .btn-icon:disabled { opacity: 0.3; cursor: not-allowed; }
        .btn-icon:disabled:hover { background: transparent; border-color: transparent; }
        @keyframes upload-spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        @keyframes upload-pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
        @keyframes success-pop { 0% { transform: scale(0.5); opacity: 0; } 50% { transform: scale(1.2); } 100% { transform: scale(1); opacity: 1; } }
        .btn-icon--uploading { animation: upload-spin 2s linear infinite, upload-pulse 1.2s ease-in-out infinite; background: rgba(168, 85, 247, 0.15); border-color: rgba(168, 85, 247, 0.3); pointer-events: none; color: var(--text-primary); }
        .btn-icon--upload-success { animation: success-pop 0.4s ease-out; background: rgba(34, 197, 94, 0.15); border-color: rgba(34, 197, 94, 0.3); pointer-events: none; }
        .btn-icon--upload-error { background: rgba(239, 68, 68, 0.15); border-color: rgba(239, 68, 68, 0.3); pointer-events: none; }
        .status-bar { position: fixed; bottom: 0; left: 0; right: 0; background: rgba(20, 20, 21, 0.85); backdrop-filter: blur(12px); -webkit-backdrop-filter: blur(12px); border-top: 1px solid var(--border-primary); padding: 10px 32px; display: flex; justify-content: space-between; align-items: center; font-size: 12px; color: var(--text-tertiary); z-index: 50; }
        .status-bar-left, .status-bar-right { display: flex; align-items: center; gap: 16px; }
        .status-bar-sep { width: 1px; height: 12px; background: var(--border-primary); }
        .modal-overlay { position: fixed; inset: 0; background: rgba(0, 0, 0, 0.6); backdrop-filter: blur(4px); -webkit-backdrop-filter: blur(4px); z-index: 1000; display: flex; align-items: flex-start; justify-content: center; padding-top: 5vh; opacity: 0; visibility: hidden; pointer-events: none; transition: opacity 0.2s ease, visibility 0s 0.2s; }
        .modal-overlay.show { opacity: 1; visibility: visible; pointer-events: auto; transition: opacity 0.2s ease, visibility 0s 0s; }
        .modal-panel { background: var(--bg-elevated); border: 1px solid var(--border-primary); border-radius: var(--radius-lg); width: 90%; max-width: 960px; max-height: 80vh; overflow: hidden; display: flex; flex-direction: column; transform: translateY(-12px) scale(0.98); transition: transform 0.2s ease; }
        .modal-overlay.show .modal-panel { transform: translateY(0) scale(1); }
        .modal-header { padding: 20px 24px; border-bottom: 1px solid var(--border-primary); display: flex; justify-content: space-between; align-items: center; flex-shrink: 0; }
        .modal-title { font-size: 16px; font-weight: 600; color: var(--text-primary); }
        .btn-close { width: 32px; height: 32px; display: flex; align-items: center; justify-content: center; border-radius: var(--radius-sm); border: none; background: transparent; color: var(--text-secondary); font-size: 18px; cursor: pointer; transition: all 0.15s ease; }
        .btn-close:hover { background: var(--bg-tertiary); color: var(--text-primary); }
        .modal-body { padding: 20px 24px; overflow-y: auto; flex: 1; }
        .modal-table { width: 100%; border-collapse: collapse; }
        .modal-table th { text-align: left; padding: 8px 12px; font-size: 11px; font-weight: 500; text-transform: uppercase; letter-spacing: 0.05em; color: var(--text-tertiary); border-bottom: 1px solid var(--border-primary); }
        .modal-table td { padding: 8px 12px; font-size: 13px; border-bottom: 1px solid var(--border-primary); color: var(--text-secondary); }
        .modal-table tr:last-child td { border-bottom: none; }
        .modal-table tbody tr:hover { background: rgba(255, 255, 255, 0.02); }
        .modal-summary { display: flex; gap: 16px; align-items: center; padding: 14px 16px; background: var(--bg-tertiary); border: 1px solid var(--border-primary); border-radius: var(--radius-md); margin-bottom: 20px; font-size: 13px; }
        .modal-summary-label { font-weight: 600; color: var(--text-primary); }
        .modal-summary-stat { font-weight: 500; }
        .accordion-trigger { display: flex; justify-content: space-between; align-items: center; padding: 12px 16px; background: var(--bg-tertiary); border: 1px solid var(--border-primary); border-radius: var(--radius-sm); cursor: pointer; margin-bottom: 2px; transition: background 0.15s ease; user-select: none; }
        .accordion-trigger:hover { background: var(--bg-elevated); }
        .accordion-trigger:active { transform: scale(0.995); }
        .accordion-arrow { display: inline-block; transition: transform 0.2s ease; color: var(--text-tertiary); font-size: 12px; }
        .accordion-arrow.open { transform: rotate(90deg); }
        .accordion-content { max-height: 0; overflow: hidden; background: var(--bg-secondary); border-radius: 0 0 var(--radius-sm) var(--radius-sm); margin-bottom: 12px; transition: max-height 0.3s cubic-bezier(0.4, 0, 0.2, 1), padding 0.3s ease; padding: 0 16px; }
        .accordion-content.open { max-height: 2000px; padding: 12px 16px 16px; }
        .pagination { display: flex; justify-content: center; align-items: center; gap: 10px; margin-top: 16px; }
        .btn-page { padding: 6px 14px; background: var(--bg-tertiary); color: var(--text-secondary); border: 1px solid var(--border-primary); border-radius: var(--radius-sm); cursor: pointer; font-size: 12px; transition: all 0.15s ease; }
        .btn-page:hover { background: var(--bg-elevated); border-color: var(--border-secondary); color: var(--text-primary); }
        .btn-page:disabled { opacity: 0.35; cursor: not-allowed; }
        .btn-page:disabled:hover { background: var(--bg-tertiary); border-color: var(--border-primary); color: var(--text-secondary); }
        .page-info { font-size: 12px; color: var(--text-tertiary); }
        .empty-state { text-align: center; padding: 40px 20px; color: var(--text-tertiary); font-size: 14px; }
        .loading { text-align: center; padding: 60px 20px; color: var(--text-tertiary); font-size: 14px; }
    </style>
</head>
<body>
    <header class="page-header">
        <div>
            <div class="header-title">Upload Dashboard</div>
            <div class="header-subtitle">Sistema de upload automatizado</div>
        </div>
        <div class="live-indicator"><span class="live-dot"></span><span>Ao vivo</span></div>
    </header>
    <div class="stats-grid">
        <div class="stat-card stat-card--total" id="card-total" onclick="toggleFiltro(null)">
            <div class="stat-value stat-value--total" id="total">-</div>
            <div class="stat-label">Total de Canais</div>
        </div>
        <div class="stat-card stat-card--sucesso" id="card-sucesso" onclick="toggleFiltro('sucesso')">
            <div class="stat-value stat-value--sucesso" id="sucesso">-</div>
            <div class="stat-label">Upload com Sucesso</div>
        </div>
        <div class="stat-card stat-card--sem_video" id="card-sem_video" onclick="toggleFiltro('sem_video')">
            <div class="stat-value stat-value--sem_video" id="sem_video">-</div>
            <div class="stat-label">Sem Video</div>
        </div>
        <div class="stat-card stat-card--erro" id="card-erro" onclick="toggleFiltro('erro')">
            <div class="stat-value stat-value--erro" id="erro">-</div>
            <div class="stat-label">Com Erro</div>
        </div>
        <div class="stat-card stat-card--historico" onclick="abrirHistoricoCompleto()">
            <div class="stat-value stat-value--historico">&#x1F4DC;</div>
            <div class="stat-label">Historico Completo</div>
        </div>
    </div>
    <div class="content" id="subnichos-container"><div class="loading">Carregando dados...</div></div>
    <div class="status-bar">
        <div class="status-bar-left">
            <span>Atualizado: <span id="update-time">--:--:--</span></span>
            <span class="status-bar-sep"></span>
            <span>Refresh: 5s</span>
        </div>
        <div class="status-bar-right"><span><span id="total-monetizados">-</span> canais monetizados</span></div>
    </div>
    <div id="historicoModal" class="modal-overlay">
        <div class="modal-panel">
            <div class="modal-header">
                <h2 class="modal-title" id="modalTitle">Historico de Uploads</h2>
                <button class="btn-close" onclick="fecharModal()">&times;</button>
            </div>
            <div class="modal-body" id="modalBody"><p style="color: var(--text-tertiary); text-align: center;">Carregando...</p></div>
        </div>
    </div>
    <div id="historicoCompletoModal" class="modal-overlay">
        <div class="modal-panel">
            <div class="modal-header">
                <h2 class="modal-title" id="modalTitleCompleto">Historico Completo</h2>
                <button class="btn-close" onclick="fecharModalCompleto()">&times;</button>
            </div>
            <div class="modal-body" id="modalBodyCompleto"><p style="color: var(--text-tertiary); text-align: center;">Carregando...</p></div>
        </div>
    </div>
    <script>
        var filtroStatus = null;
        function escapeHtml(text) {
            if (!text) return '';
            return String(text).replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;').replace(/"/g,'&quot;');
        }
        function truncarTitulo(titulo) {
            if (!titulo || titulo === '-') return '-';
            var palavras = titulo.split(' ');
            if (palavras.length > 7) return palavras.slice(0, 7).join(' ') + '...';
            return titulo;
        }
        function getSiglaIdioma(lingua) {
            if (!lingua) return '';
            var l = lingua.toLowerCase();
            var mapa = {'pt':'PT','portugues':'PT','portuguese':'PT','en':'EN','ingles':'EN','english':'EN','es':'ES','espanhol':'ES','spanish':'ES','de':'DE','alemao':'DE','german':'DE','fr':'FR','frances':'FR','french':'FR','it':'IT','italiano':'IT','italian':'IT','pl':'PL','polones':'PL','polish':'PL','ru':'RU','russo':'RU','russian':'RU','ja':'JP','japones':'JP','japanese':'JP','ko':'KR','coreano':'KR','korean':'KR','tr':'TR','turco':'TR','turkish':'TR','ar':'AR','arabic':'AR','arabe':'AR'};
            return mapa[l] || '';
        }
        function toggleFiltro(status) {
            document.querySelectorAll('.stat-card').forEach(function(c) { c.classList.remove('active'); });
            if (!status || filtroStatus === status) { filtroStatus = null; } else { filtroStatus = status; var card = document.getElementById('card-' + status); if (card) card.classList.add('active'); }
            atualizar();
        }
        function toggleDia(id) {
            var el = document.getElementById(id);
            var seta = document.getElementById('seta-' + id);
            if (el) el.classList.toggle('open');
            if (seta) seta.classList.toggle('open');
        }
        var _uploadingChannelId = null;
        var _successChannelId = null;
        var _errorChannelId = null;
        var _statusBeforeUpload = null;

        function _getChannelStatus(data, channelId) {
            if (data && data.subnichos) {
                for (var sub in data.subnichos) {
                    for (var i = 0; i < data.subnichos[sub].length; i++) {
                        if (data.subnichos[sub][i].channel_id === channelId) return data.subnichos[sub][i].status;
                    }
                }
            }
            return null;
        }
        async function forcarUpload(channelId, channelName) {
            var msg = 'Forcar upload do canal ' + channelName + '?\\n\\nO proximo video "done" da planilha sera enviado.';
            if (!confirm(msg)) return;
            var botao = event.target.closest('.btn-icon');
            _uploadingChannelId = channelId;
            try {
                // Captura status ANTES do upload para comparar depois
                var preXhr = new XMLHttpRequest();
                preXhr.open('GET', '/api/dash-upload/status', false);
                preXhr.send();
                if (preXhr.status === 200) {
                    _statusBeforeUpload = _getChannelStatus(JSON.parse(preXhr.responseText), channelId);
                }
                botao.innerHTML = '\\u23F3';
                botao.classList.add('btn-icon--uploading');
                var response = await fetch('/api/yt-upload/force/' + channelId, { method: 'POST' });
                var result = await response.json();
                if (response.ok && result.status !== 'sem_video' && result.status !== 'no_video') {
                    var tentativas = 0;
                    var maxTentativas = 4;
                    var pollInterval = setInterval(function() {
                        tentativas++;
                        if (tentativas > maxTentativas) {
                            clearInterval(pollInterval);
                            _uploadingChannelId = null;
                            _statusBeforeUpload = null;
                            atualizar();
                            return;
                        }
                        var xhr = new XMLHttpRequest();
                        xhr.open('GET', '/api/dash-upload/status', true);
                        xhr.onreadystatechange = function() {
                            if (xhr.readyState === 4 && xhr.status === 200) {
                                try {
                                    var data = JSON.parse(xhr.responseText);
                                    var st = _getChannelStatus(data, channelId);
                                    if (st && st !== _statusBeforeUpload) {
                                        clearInterval(pollInterval);
                                        _uploadingChannelId = null;
                                        _statusBeforeUpload = null;
                                        if (st === 'sucesso') {
                                            _successChannelId = channelId;
                                            atualizar();
                                            setTimeout(function() {
                                                _successChannelId = null;
                                                atualizar();
                                            }, 15000);
                                        } else if (st === 'erro') {
                                            _errorChannelId = channelId;
                                            atualizar();
                                            setTimeout(function() {
                                                _errorChannelId = null;
                                                atualizar();
                                            }, 5000);
                                        } else {
                                            atualizar();
                                        }
                                    }
                                } catch(e) {}
                            }
                        };
                        xhr.send();
                    }, 3000);
                } else if (result.status === 'sem_video' || result.status === 'no_video') {
                    alert('Sem videos disponiveis na planilha de ' + channelName);
                    _uploadingChannelId = null;
                    _statusBeforeUpload = null;
                    atualizar();
                } else {
                    alert('Erro: ' + (result.detail || result.message || 'Falha ao iniciar upload'));
                    _uploadingChannelId = null;
                    _statusBeforeUpload = null;
                    atualizar();
                }
            } catch (error) {
                alert('Erro de conexao: ' + error.message);
                _uploadingChannelId = null;
                _statusBeforeUpload = null;
            }
        }
        var _historicoData = [];
        var _historicoPagina = 0;
        var _HIST_POR_PAGINA = 10;
        function renderHistoricoPagina() {
            var modalBody = document.getElementById('modalBody');
            var items = _historicoData;
            var totalPaginas = Math.ceil(items.length / _HIST_POR_PAGINA);
            var inicio = _historicoPagina * _HIST_POR_PAGINA;
            var fim = Math.min(inicio + _HIST_POR_PAGINA, items.length);
            var paginaItems = items.slice(inicio, fim);
            var countSucesso = 0, countSemVideo = 0, countErro = 0;
            items.forEach(function(item) { if (item.status === 'sucesso') countSucesso++; else if (item.status === 'sem_video') countSemVideo++; else if (item.status === 'erro') countErro++; });
            var totalRegistros = countSucesso + countSemVideo + countErro;
            var html = '<div class="modal-summary" style="background:var(--success-muted);border-color:rgba(34,197,94,0.25);">';
            html += '<span class="modal-summary-stat" style="color:var(--text-primary);font-weight:600;">Total de Registros: ' + totalRegistros + ' |</span>';
            html += '<span class="modal-summary-stat" style="color:var(--success);">&#x2705; ' + countSucesso + ' uploads</span>';
            html += '<span class="modal-summary-stat" style="color:var(--warning);">&#x26A0;&#xFE0F; ' + countSemVideo + ' sem video</span>';
            html += '<span class="modal-summary-stat" style="color:var(--error);">&#x274C; ' + countErro + ' erros</span>';
            html += '</div>';
            html += '<table class="modal-table"><thead><tr><th>Data</th><th>Video</th><th>Status</th><th>Horario</th></tr></thead><tbody>';
            if (paginaItems.length > 0) {
                paginaItems.forEach(function(item) {
                    html += '<tr>';
                    var df = item.data; if (df && df.includes('-')) { var p = df.split('-'); df = p[2] + '/' + p[1] + '/' + p[0]; }
                    html += '<td>' + df + '</td>';
                    html += '<td style="max-width:300px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;">' + escapeHtml(truncarTitulo(item.video_titulo)) + '</td>';
                    var statusColor = 'var(--text-tertiary)'; var statusText = '&#x26AA; Sem Video';
                    if (item.status === 'sucesso') { statusColor = 'var(--success)'; statusText = '&#x2705; Sucesso'; }
                    else if (item.status === 'erro') { statusColor = 'var(--error)'; statusText = '&#x274C; Erro'; }
                    html += '<td style="color:' + statusColor + ';font-weight:500;">' + statusText + '</td>';
                    html += '<td style="color:var(--text-tertiary);">' + (item.hora_processamento || '-') + '</td>';
                    html += '</tr>';
                });
            } else { html += '<tr><td colspan="4" style="text-align:center;color:var(--text-tertiary);padding:20px;">Nenhum historico</td></tr>'; }
            html += '</tbody></table>';
            if (totalPaginas > 1) {
                html += '<div class="pagination">';
                html += '<button class="btn-page" onclick="_historicoPagina--;renderHistoricoPagina();" ' + (_historicoPagina === 0 ? 'disabled' : '') + '>Anterior</button>';
                html += '<span class="page-info">Pagina ' + (_historicoPagina + 1) + ' de ' + totalPaginas + '</span>';
                html += '<button class="btn-page" onclick="_historicoPagina++;renderHistoricoPagina();" ' + (_historicoPagina >= totalPaginas - 1 ? 'disabled' : '') + '>Proxima</button>';
                html += '</div>';
            }
            modalBody.innerHTML = html;
        }
        function abrirHistorico(channelId, channelName) {
            var modal = document.getElementById('historicoModal');
            document.getElementById('modalTitle').textContent = 'Historico - ' + channelName;
            modal.classList.add('show');
            document.getElementById('modalBody').innerHTML = '<p style="color:var(--text-tertiary);text-align:center;padding:40px;">Carregando...</p>';
            fetch('/api/dash-upload/canais/' + channelId + '/historico')
                .then(function(r) { return r.json(); })
                .then(function(data) {
                    if (data.error) { document.getElementById('modalBody').innerHTML = '<p style="color:var(--error);">Erro: ' + escapeHtml(data.error) + '</p>'; return; }
                    if (data.historico && data.historico.length > 0) {
                        var vistos = new Set(); var filtrado = [];
                        data.historico.forEach(function(item) { var chave = item.data + '|' + (item.video_titulo || 'sem_video'); if (!vistos.has(chave)) { vistos.add(chave); filtrado.push(item); } });
                        data.historico = filtrado;
                    }
                    _historicoData = data.historico || []; _historicoPagina = 0; renderHistoricoPagina();
                })
                .catch(function(error) { document.getElementById('modalBody').innerHTML = '<p style="color:var(--error);">Erro: ' + error + '</p>'; });
        }
        function fecharModal() { document.getElementById('historicoModal').classList.remove('show'); }
        function fecharModalCompleto() { document.getElementById('historicoCompletoModal').classList.remove('show'); }
        async function abrirHistoricoCompleto() {
            var modal = document.getElementById('historicoCompletoModal');
            var modalBody = document.getElementById('modalBodyCompleto');
            modal.classList.add('show');
            modalBody.innerHTML = '<p style="color:var(--text-tertiary);text-align:center;padding:40px;">Carregando historico...</p>';
            try {
                var response = await fetch('/api/dash-upload/historico-completo');
                var data = await response.json();
                if (data.historico_por_data && data.historico_por_data.length > 0) {
                    data.historico_por_data.forEach(function(dia) {
                        if (dia.canais && dia.canais.length > 0) {
                            var vistos = new Set(); var filtrado = [];
                            dia.canais.forEach(function(canal) { if (!canal.nome || canal.nome.trim() === '') return; var chave = dia.data + '|' + canal.nome + '|' + (canal.video_titulo || 'sem_video'); if (!vistos.has(chave)) { vistos.add(chave); filtrado.push(canal); } });
                            dia.canais = filtrado;
                        }
                    });
                    var totalSucesso = 0, totalSemVideo = 0, totalErro = 0;
                    data.historico_por_data.forEach(function(dia) { dia.canais.forEach(function(canal) { if (canal.status === 'sucesso') totalSucesso++; else if (canal.status === 'sem_video') totalSemVideo++; else if (canal.status === 'erro') totalErro++; }); });
                    var totalRegistros = totalSucesso + totalSemVideo + totalErro;
                    var html = '<div class="modal-summary" style="background:var(--success-muted);border-color:rgba(34,197,94,0.25);">';
                    html += '<span class="modal-summary-stat" style="color:var(--text-primary);font-weight:600;">Total de Registros: ' + totalRegistros + ' |</span>';
                    html += '<span class="modal-summary-stat" style="color:var(--success);">&#x2705; ' + totalSucesso + ' uploads</span>';
                    html += '<span class="modal-summary-stat" style="color:var(--warning);">&#x26A0;&#xFE0F; ' + totalSemVideo + ' sem video</span>';
                    html += '<span class="modal-summary-stat" style="color:var(--error);">&#x274C; ' + totalErro + ' erros</span>';
                    html += '</div>';
                    html += '<div style="max-height:450px;overflow-y:auto;">';
                    data.historico_por_data.forEach(function(dia, idx) {
                        var df = dia.data; if (df && df.includes('-')) { var p = df.split('-'); if (p.length === 3) df = p[2] + '/' + p[1] + '/' + p[0]; }
                        var suc = 0, sv = 0, err = 0;
                        dia.canais.forEach(function(c) { if (c.status === 'sucesso') suc++; else if (c.status === 'sem_video') sv++; else if (c.status === 'erro') err++; });
                        var diaId = 'dia-' + idx;
                        html += '<div class="accordion-trigger" data-dia="' + diaId + '" onclick="toggleDia(this.dataset.dia)">';
                        html += '<div style="display:flex;align-items:center;gap:8px;">';
                        html += '<span id="seta-' + diaId + '" class="accordion-arrow">&#9654;</span>';
                        html += '<span style="font-weight:600;color:var(--text-primary);">' + df + '</span>';
                        html += '</div>';
                        html += '<div style="display:flex;gap:12px;font-size:12px;">';
                        html += '<span style="color:var(--success);">&#x2705; ' + suc + '</span>';
                        html += '<span style="color:var(--warning);">&#x26A0;&#xFE0F; ' + sv + '</span>';
                        html += '<span style="color:var(--error);">&#x274C; ' + err + '</span>';
                        html += '</div></div>';
                        html += '<div class="accordion-content" id="' + diaId + '">';
                        var canaisSucesso = dia.canais.filter(function(c) { return c.status === 'sucesso'; });
                        if (canaisSucesso.length > 0) {
                            html += '<table class="modal-table"><thead><tr><th>Canal</th><th>Video</th><th>Status</th><th>Horario</th></tr></thead><tbody>';
                            canaisSucesso.forEach(function(canal) {
                                var sigla = getSiglaIdioma(canal.lingua);
                                html += '<tr><td style="color:var(--text-primary);font-weight:500;">' + escapeHtml(canal.nome);
                                if (sigla) html += ' <span class="lang-tag">' + sigla + '</span>';
                                html += '</td><td style="max-width:280px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;">' + escapeHtml(truncarTitulo(canal.video_titulo)) + '</td>';
                                html += '<td style="color:var(--success);font-weight:500;">Sucesso</td>';
                                html += '<td style="color:var(--text-tertiary);">' + (canal.hora || '-') + '</td></tr>';
                            });
                            html += '</tbody></table>';
                        } else { html += '<p style="color:var(--text-tertiary);text-align:center;padding:12px;">Nenhum upload com sucesso</p>'; }
                        html += '</div>';
                    });
                    html += '</div>';
                    modalBody.innerHTML = html;
                } else { modalBody.innerHTML = '<p style="color:var(--text-tertiary);text-align:center;padding:40px;">Nenhum historico encontrado.</p>'; }
            } catch (error) { modalBody.innerHTML = '<p style="color:var(--error);text-align:center;">Erro: ' + error.message + '</p>'; }
        }
        window.addEventListener('click', function(e) { if (e.target.classList && e.target.classList.contains('modal-overlay')) e.target.classList.remove('show'); });
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') document.querySelectorAll('.modal-overlay.show').forEach(function(m) { m.classList.remove('show'); }); });
        function formatTime(timeStr) { if (!timeStr) return '-'; return timeStr; }
        function atualizar() {
            var xhr = new XMLHttpRequest();
            xhr.open('GET', '/api/dash-upload/status', true);
            xhr.onreadystatechange = function() {
                if (xhr.readyState === 4 && xhr.status === 200) {
                    try {
                        var data = JSON.parse(xhr.responseText);
                        var el;
                        el = document.getElementById('total'); if (el) el.textContent = data.stats.total || 0;
                        el = document.getElementById('sucesso'); if (el) el.textContent = data.stats.sucesso || 0;
                        el = document.getElementById('sem_video'); if (el) el.textContent = data.stats.sem_video || 0;
                        el = document.getElementById('erro'); if (el) el.textContent = data.stats.erro || 0;
                        var totalMonetizados = 0;
                        var html = '';
                        if (data.subnichos) {
                            for (var subnicho in data.subnichos) {
                                var canais = data.subnichos[subnicho];
                                var ss = {sucesso: 0, erro: 0, sem_video: 0, pendente: 0};
                                for (var i = 0; i < canais.length; i++) {
                                    if (canais[i].is_monetized) totalMonetizados++;
                                    if (canais[i].status === 'sucesso') ss.sucesso++;
                                    else if (canais[i].status === 'erro') ss.erro++;
                                    else if (canais[i].status === 'sem_video') ss.sem_video++;
                                    else ss.pendente++;
                                }
                                var accent = '#3f3f46'; var accentMuted = 'rgba(63,63,70,0.15)'; var icon = '?';
                                if (subnicho === 'Monetizados') { accent = '#22c55e'; accentMuted = 'rgba(34,197,94,0.12)'; icon = '$'; }
                                else if (subnicho === 'Historias Sombrias') { accent = '#8b5cf6'; accentMuted = 'rgba(139,92,246,0.12)'; icon = '\\u265B'; }
                                else if (subnicho === 'Relatos de Guerra') { accent = '#4a8c50'; accentMuted = 'rgba(74,140,80,0.18)'; icon = '\\u2694'; }
                                else if (subnicho === 'Guerras e Civilizacoes' || subnicho === 'Guerras e Civiliza\\u00e7\\u00f5es') { accent = '#f97316'; accentMuted = 'rgba(249,115,22,0.12)'; icon = '\\u26E8'; }
                                else if (subnicho === 'Terror') { accent = '#ef4444'; accentMuted = 'rgba(239,68,68,0.12)'; icon = '\\u2620'; }
                                else if (subnicho === 'Desmonetizados') { accent = '#71717a'; accentMuted = 'rgba(113,113,122,0.12)'; icon = '\\u25CB'; }
                                html += '<div class="section" style="--section-accent:' + accent + ';--section-accent-muted:' + accentMuted + ';">';
                                html += '<div class="section-header"><div class="section-title">';
                                html += '<span class="section-icon">' + icon + '</span>';
                                html += '<span>' + escapeHtml(subnicho) + '</span>';
                                html += '<span class="section-count">' + canais.length + ' canais</span></div>';
                                html += '<div class="section-pills">';
                                if (ss.sucesso > 0) html += '<span class="stat-pill stat-pill--success">' + ss.sucesso + ' sucesso</span>';
                                if (ss.sem_video > 0) html += '<span class="stat-pill stat-pill--warning">' + ss.sem_video + ' sem video</span>';
                                if (ss.erro > 0) html += '<span class="stat-pill stat-pill--error">' + ss.erro + ' erro</span>';
                                if (ss.pendente > 0) html += '<span class="stat-pill stat-pill--pending">' + ss.pendente + ' pendente</span>';
                                html += '</div></div>';
                                html += '<table class="channel-table"><thead><tr>';
                                html += '<th style="width:280px">Canal</th><th style="width:120px">Status</th><th>Video Enviado</th><th style="width:80px">Horario</th><th style="width:120px">Acoes</th>';
                                html += '</tr></thead><tbody>';
                                var temCanais = false;
                                for (var j = 0; j < canais.length; j++) {
                                    var canal = canais[j];
                                    if (filtroStatus && canal.status !== filtroStatus) continue;
                                    temCanais = true;
                                    var badgeClass = 'status-badge--pending'; var badgeText = 'Pendente';
                                    if (canal.status === 'sucesso') { badgeClass = 'status-badge--success'; badgeText = 'Enviado'; }
                                    else if (canal.status === 'sem_video') { badgeClass = 'status-badge--sem_video'; badgeText = 'Sem Video'; }
                                    else if (canal.status === 'erro') { badgeClass = 'status-badge--error'; badgeText = 'Erro'; }
                                    html += '<tr>';
                                    html += '<td><div class="cell-channel"><span class="channel-name">' + escapeHtml(canal.channel_name) + '</span>';
                                    var sigla = getSiglaIdioma(canal.lingua);
                                    if (sigla) html += '<span class="lang-tag">' + sigla + '</span>';
                                    if (canal.is_monetized) html += '<span class="monetized-dot"></span>';
                                    html += '</div></td>';
                                    html += '<td><span class="status-badge ' + badgeClass + '">' + badgeText + '</span></td>';
                                    html += '<td><span class="video-title">' + escapeHtml(truncarTitulo(canal.video_titulo)) + '</span></td>';
                                    html += '<td><span class="cell-time">' + formatTime(canal.hora_upload) + '</span></td>';
                                    html += '<td><div class="cell-actions">';
                                    var safeName = escapeHtml(canal.channel_name).replace(/"/g, '&quot;');
                                    if (_uploadingChannelId === canal.channel_id) {
                                        html += '<button class="btn-icon btn-icon--upload btn-icon--uploading" data-channel-id="' + canal.channel_id + '" data-channel-name="' + safeName + '" title="Uploading...">&#x23F3;</button>';
                                    } else if (_successChannelId === canal.channel_id) {
                                        html += '<button class="btn-icon btn-icon--upload btn-icon--upload-success" data-channel-id="' + canal.channel_id + '" data-channel-name="' + safeName + '" title="Upload concluido!">&#x2705;</button>';
                                    } else if (_errorChannelId === canal.channel_id) {
                                        html += '<button class="btn-icon btn-icon--upload btn-icon--upload-error" data-channel-id="' + canal.channel_id + '" data-channel-name="' + safeName + '" title="Erro no upload">&#x274C;</button>';
                                    } else {
                                        html += '<button class="btn-icon btn-icon--upload" data-channel-id="' + canal.channel_id + '" data-channel-name="' + safeName + '" title="Forcar upload">&#x1F4E4;</button>';
                                    }
                                    html += '<button class="btn-icon btn-icon--hist" data-channel-id="' + canal.channel_id + '" data-channel-name="' + safeName + '" title="Historico">&#x1F4DC;</button>';
                                    if (canal.spreadsheet_id && canal.spreadsheet_id !== '') {
                                        html += '<a href="https://docs.google.com/spreadsheets/d/' + canal.spreadsheet_id + '" target="_blank" class="btn-icon" title="Planilha">&#x1F4D1;</a>';
                                    } else { html += '<button class="btn-icon" disabled title="Sem planilha">&#x1F4D1;</button>'; }
                                    html += '</div></td></tr>';
                                }
                                if (!temCanais) html += '<tr><td colspan="5" style="text-align:center;color:var(--text-tertiary);padding:20px;">Nenhum canal com este filtro</td></tr>';
                                html += '</tbody></table></div>';
                            }
                        }
                        if (html === '') html = '<div class="empty-state">Nenhum canal encontrado</div>';
                        var container = document.getElementById('subnichos-container');
                        if (container.innerHTML !== html) container.innerHTML = html;
                        document.getElementById('total-monetizados').textContent = totalMonetizados;
                        var now = new Date();
                        document.getElementById('update-time').textContent = now.toLocaleTimeString('pt-BR', {timeZone: 'America/Sao_Paulo'});
                    } catch(e) { console.error('Erro:', e); document.getElementById('subnichos-container').innerHTML = '<div class="empty-state">Erro ao carregar dados</div>'; }
                }
            };
            xhr.send();
        }
        atualizar();
        setInterval(atualizar, 5000);
        document.addEventListener('click', function(e) {
            var btn = e.target.closest('.btn-icon--hist');
            if (btn) { abrirHistorico(btn.getAttribute('data-channel-id'), btn.getAttribute('data-channel-name')); return; }
            btn = e.target.closest('.btn-icon--upload');
            if (btn) { forcarUpload(btn.getAttribute('data-channel-id'), btn.getAttribute('data-channel-name')); return; }
        });
    </script>
</body>
</html>
'''

@app.get("/dash-upload", response_class=HTMLResponse)
async def dash_upload_page():
    """Dashboard de Upload v2 - Interface web"""
    return DASH_UPLOAD_HTML

@app.get("/api/dash-upload/status")
async def dash_upload_status():
    """Status dos canais de upload agrupados por subnicho"""
    import time as _time
    from collections import defaultdict

    now = _time.time()
    if _dash_cache['data'] and (now - _dash_cache['timestamp']) < _DASH_CACHE_TTL:
        return _dash_cache['data']

    try:
        canais = supabase.table('yt_channels')\
            .select('channel_id, channel_name, spreadsheet_id, lingua, is_monetized, subnicho')\
            .eq('is_active', True)\
            .eq('upload_automatico', True)\
            .order('subnicho, channel_name')\
            .execute()

        today = datetime.now(timezone.utc).date().isoformat()
        uploads = supabase.table('yt_canal_upload_diario')\
            .select('channel_id, status, upload_realizado, video_titulo, hora_processamento, erro_mensagem, created_at')\
            .eq('data', today)\
            .execute()

        # Prioridade: sucesso > erro > sem_video (quando canal tem multiplos registros no dia)
        # Quando mesmo status, pega o MAIS RECENTE (created_at maior)
        _status_priority = {'sucesso': 0, 'erro': 1, 'sem_video': 2}
        upload_map = {}
        for u in uploads.data:
            cid = u['channel_id']
            new_prio = _status_priority.get(u.get('status'), 9)
            if cid not in upload_map:
                upload_map[cid] = u
            else:
                old_prio = _status_priority.get(upload_map[cid].get('status'), 9)
                if new_prio < old_prio or (new_prio == old_prio and (u.get('created_at') or '') > (upload_map[cid].get('created_at') or '')):
                    upload_map[cid] = u

        subnichos_dict = defaultdict(list)
        stats = {'total': 0, 'sucesso': 0, 'erro': 0, 'sem_video': 0, 'pendente': 0}

        for canal in canais.data:
            upload = upload_map.get(canal['channel_id'])
            status = 'pendente'
            video_titulo = None
            hora_upload = None

            if upload:
                if upload.get('upload_realizado'):
                    status = 'sucesso'
                    video_titulo = upload.get('video_titulo')
                    hora_upload = _extrair_hora(upload.get('hora_processamento') or upload.get('updated_at'))
                elif upload.get('status') == 'sem_video':
                    status = 'sem_video'
                elif upload.get('erro_mensagem'):
                    status = 'erro'

            stats['total'] += 1
            stats[status] += 1

            subnicho = canal.get('subnicho', 'Sem Categoria')
            subnichos_dict[subnicho].append({
                'channel_id': canal['channel_id'],
                'channel_name': canal['channel_name'],
                'spreadsheet_id': canal.get('spreadsheet_id', ''),
                'lingua': canal.get('lingua', ''),
                'is_monetized': canal.get('is_monetized', False),
                'status': status,
                'video_titulo': video_titulo,
                'hora_upload': hora_upload
            })

        monetizados_forcados = ['UCzfZRuRHSp6erCwzuhjywFw', 'UCWYzVowgJ6LlxCcYlMGcLtA']
        for sub in subnichos_dict:
            for canal in subnichos_dict[sub]:
                if canal['channel_id'] in monetizados_forcados:
                    canal['is_monetized'] = True

        novo_dict = defaultdict(list)
        for sub, canais_list in subnichos_dict.items():
            for canal in canais_list:
                if canal['is_monetized']:
                    novo_dict['Monetizados'].append(canal)
                else:
                    novo_dict[sub].append(canal)
        subnichos_dict = novo_dict

        status_order = {'sucesso': 0, 'pendente': 1, 'erro': 2, 'sem_video': 3}
        for sub in subnichos_dict:
            subnichos_dict[sub].sort(key=lambda x: (status_order.get(x['status'], 4), not x['is_monetized'], x['channel_name']))

        # Ordenar subnichos: mais sucessos primeiro
        subnichos_ordenados = dict(sorted(
            subnichos_dict.items(),
            key=lambda item: sum(1 for c in item[1] if c['status'] == 'sucesso'),
            reverse=True
        ))

        result = {'stats': stats, 'subnichos': subnichos_ordenados}
        _dash_cache['data'] = result
        _dash_cache['timestamp'] = _time.time()
        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dash-upload/canais/{channel_id}/historico")
async def dash_upload_historico(channel_id: str):
    """Historico de uploads de um canal"""
    try:
        try:
            response = supabase.table('yt_canal_upload_historico')\
                .select('*')\
                .eq('channel_id', channel_id)\
                .order('data', desc=True)\
                .order('hora_processamento', desc=True)\
                .execute()
        except Exception as e:
            if "relation" in str(e) and "does not exist" in str(e):
                response = supabase.table('yt_canal_upload_diario')\
                    .select('*')\
                    .eq('channel_id', channel_id)\
                    .order('data', desc=True)\
                    .execute()
            else:
                raise e

        historico_data = response.data if response.data else []

        response_diario = supabase.table('yt_canal_upload_diario')\
            .select('*')\
            .eq('channel_id', channel_id)\
            .order('data', desc=True)\
            .execute()

        if response_diario.data:
            registros_unicos = {(item.get('channel_id'), item['data'], item.get('video_titulo', '')) for item in historico_data}
            for item_diario in response_diario.data:
                chave = (item_diario.get('channel_id'), item_diario['data'], item_diario.get('video_titulo', ''))
                if chave not in registros_unicos:
                    historico_data.append(item_diario)
                    registros_unicos.add(chave)

        historico_data.sort(key=lambda x: (x.get('data', ''), x.get('hora_processamento', '')), reverse=True)

        historico = []
        for item in historico_data:
            historico.append({
                'data': item['data'],
                'status': item.get('status', 'pendente'),
                'video_titulo': item.get('video_titulo', '-'),
                'hora_processamento': _extrair_hora(item.get('hora_processamento')),
                'erro_mensagem': item.get('erro_mensagem'),
                'tentativa_numero': item.get('tentativa_numero', 1),
                'upload_realizado': item.get('upload_realizado', False),
                'youtube_video_id': item.get('youtube_video_id')
            })

        return {'channel_id': channel_id, 'total_registros': len(historico), 'historico': historico}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dash-upload/historico-completo")
async def dash_upload_historico_completo():
    """Historico completo dos ultimos 30 dias"""
    try:
        from collections import defaultdict

        hoje = datetime.now(timezone.utc).date()
        data_inicio = hoje - timedelta(days=30)

        canais_info = supabase.table('yt_channels')\
            .select('channel_name, lingua')\
            .eq('is_active', True)\
            .execute()
        mapa_lingua = {c['channel_name']: c.get('lingua', '') for c in canais_info.data} if canais_info.data else {}

        response = supabase.table('yt_canal_upload_historico')\
            .select('*')\
            .gte('data', data_inicio.isoformat())\
            .order('data', desc=True)\
            .execute()

        historico_por_data = defaultdict(lambda: {'data': '', 'total': 0, 'sucesso': 0, 'erro': 0, 'sem_video': 0, 'canais': []})

        for item in response.data:
            data_str = item['data']
            historico_por_data[data_str]['data'] = data_str
            nome_canal = item.get('channel_name', '').strip()
            if nome_canal:
                historico_por_data[data_str]['total'] += 1
                status = item.get('status', 'pendente')
                if status == 'sucesso': historico_por_data[data_str]['sucesso'] += 1
                elif status == 'erro': historico_por_data[data_str]['erro'] += 1
                elif status == 'sem_video': historico_por_data[data_str]['sem_video'] += 1
                historico_por_data[data_str]['canais'].append({
                    'nome': nome_canal, 'status': status,
                    'video_titulo': item.get('video_titulo', ''),
                    'hora': _extrair_hora(item.get('hora_processamento')) or '',
                    'lingua': mapa_lingua.get(nome_canal, '')
                })

        response_diario = supabase.table('yt_canal_upload_diario')\
            .select('*')\
            .gte('data', data_inicio.isoformat())\
            .execute()

        if response_diario.data:
            for item in response_diario.data:
                data_str = item.get('data')
                if data_str not in historico_por_data:
                    historico_por_data[data_str] = {'data': data_str, 'total': 0, 'sucesso': 0, 'erro': 0, 'sem_video': 0, 'canais': []}
                nome_canal = item.get('channel_name', '').strip()
                if not nome_canal: continue
                video_ja_existe = any(c['nome'] == nome_canal and c.get('video_titulo', '') == item.get('video_titulo', '') for c in historico_por_data[data_str]['canais'])
                if not video_ja_existe:
                    status = item.get('status', 'pendente')
                    historico_por_data[data_str]['total'] += 1
                    if status == 'sucesso': historico_por_data[data_str]['sucesso'] += 1
                    elif status == 'erro': historico_por_data[data_str]['erro'] += 1
                    elif status == 'sem_video': historico_por_data[data_str]['sem_video'] += 1
                    historico_por_data[data_str]['canais'].append({
                        'nome': nome_canal, 'status': status,
                        'video_titulo': item.get('video_titulo', ''),
                        'hora': _extrair_hora(item.get('hora_processamento')) or '',
                        'lingua': mapa_lingua.get(nome_canal, '')
                    })

        historico_lista = sorted(historico_por_data.values(), key=lambda x: x['data'], reverse=True)
        dias_mostrados = min(30, len(historico_lista))
        total_registros = sum(d['total'] for d in historico_lista[:30])

        return {'historico_por_data': historico_lista[:30], 'total_dias': dias_mostrados, 'total_registros': total_registros}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ANALISE DE ESTRUTURA DE COPY - MVP
# ============================================================================

from copy_analysis_agent import (
    run_analysis as copy_run_analysis,
    get_latest_analysis as copy_get_latest,
    get_analysis_history as copy_get_history,
    get_video_mappings as copy_get_mappings,
    get_all_channels_for_analysis as copy_get_channels
)


@app.post("/api/analise-copy/{channel_id}")
async def trigger_copy_analysis(channel_id: str, background_tasks: BackgroundTasks):
    """
    Dispara analise de estrutura de copy para um canal.
    Roda em background e retorna imediatamente.
    """
    try:
        # Rodar sincrono (analise demora ~10-30s por canal)
        result = copy_run_analysis(channel_id)
        return result
    except Exception as e:
        logger.error(f"Erro analise copy {channel_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analise-copy/{channel_id}/latest")
async def get_latest_copy_analysis(channel_id: str):
    """Retorna a analise mais recente de estrutura de copy de um canal."""
    try:
        result = copy_get_latest(channel_id)
        if not result:
            raise HTTPException(status_code=404, detail=f"Nenhuma analise encontrada para canal {channel_id}")
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro buscar analise copy {channel_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analise-copy/{channel_id}/historico")
async def get_copy_analysis_history(
    channel_id: str,
    limit: int = 20,
    offset: int = 0
):
    """
    Retorna historico de analises com paginacao.
    Params: limit (default 20, max 100), offset (default 0)
    """
    try:
        limit = min(limit, 100)
        offset = max(offset, 0)
        return copy_get_history(channel_id, limit=limit, offset=offset)
    except Exception as e:
        logger.error(f"Erro historico copy {channel_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analise-copy/{channel_id}/videos")
async def get_copy_analysis_videos(
    channel_id: str,
    run_id: int = 0,
    structure: Optional[str] = None,
    limit: int = 50,
    offset: int = 0
):
    """
    Retorna mapeamento video-estrutura de uma analise com paginacao.
    Se run_id=0, usa a analise mais recente.
    Params: run_id, structure (filtro A-G), limit (max 200), offset
    """
    try:
        if run_id == 0:
            latest = copy_get_latest(channel_id)
            if not latest:
                raise HTTPException(status_code=404, detail="Nenhuma analise encontrada")
            run_id = latest["id"]

        limit = min(limit, 200)
        offset = max(offset, 0)
        return copy_get_mappings(run_id, limit=limit, offset=offset, structure_filter=structure)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erro videos copy {channel_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analise-copy/run-all")
async def run_copy_analysis_all():
    """
    Roda analise de copy para TODOS os canais nosso com spreadsheet configurado.
    Retorna resumo de cada canal.
    """
    try:
        channels = copy_get_channels()
        if not channels:
            return {"success": False, "error": "Nenhum canal encontrado com spreadsheet configurado"}

        results = []
        success_count = 0
        error_count = 0

        for ch in channels:
            try:
                result = copy_run_analysis(ch["channel_id"])
                results.append({
                    "channel_id": ch["channel_id"],
                    "channel_name": ch.get("channel_name", ""),
                    "success": result.get("success", False),
                    "summary": result.get("summary"),
                    "error": result.get("error")
                })
                if result.get("success"):
                    success_count += 1
                else:
                    error_count += 1
            except Exception as e:
                results.append({
                    "channel_id": ch["channel_id"],
                    "channel_name": ch.get("channel_name", ""),
                    "success": False,
                    "error": str(e)
                })
                error_count += 1

        return {
            "success": True,
            "total_channels": len(channels),
            "success_count": success_count,
            "error_count": error_count,
            "results": results
        }
    except Exception as e:
        logger.error(f"Erro run-all copy: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =========================================================================
# MISSION CONTROL - Escrit√≥rio Virtual
# =========================================================================
try:
    from mission_control import MISSION_CONTROL_HTML, get_mission_control_data, get_sala_detail

    @app.get("/mission-control", response_class=HTMLResponse)
    async def mission_control_page():
        """Mission Control - Escrit√≥rio Virtual com agentes"""
        return MISSION_CONTROL_HTML

    @app.get("/api/mission-control/status")
    async def mission_control_status():
        """Retorna dados de todos os setores, salas e agentes"""
        return await get_mission_control_data(db)

    @app.get("/api/mission-control/sala/{canal_id}")
    async def mission_control_sala(canal_id: int):
        """Retorna detalhes de uma sala espec√≠fica"""
        return await get_sala_detail(db, canal_id)

    logger.info("Mission Control inicializado com sucesso!")
except Exception as e:
    logger.warning(f"Mission Control nao inicializado: {e}")


# =========================================================================
# DASHBOARD ANALISE DE COPY - Visualizacao de Relatorios
# =========================================================================

@app.get("/api/dash-analise-copy/channels")
async def dash_copy_analysis_channels():
    """Lista canais com copy_spreadsheet_id agrupados por subnicho + ultima analise."""
    try:
        # 1. Busca canais com copy_spreadsheet_id
        channels = copy_get_channels()
        if not channels:
            return {"subnichos": {}, "stats": {"total": 0, "com_relatorio": 0}}

        # 2. Busca ultima analise de cada canal
        channel_ids = [c["channel_id"] for c in channels]
        last_analyses = {}

        # Buscar em batches de 20
        for i in range(0, len(channel_ids), 20):
            batch = channel_ids[i:i+20]
            batch_str = ",".join(batch)
            resp = supabase.table("copy_analysis_runs")\
                .select("channel_id,run_date,channel_avg_retention")\
                .in_("channel_id", batch)\
                .order("run_date", desc=True)\
                .execute()

            for row in resp.data:
                cid = row["channel_id"]
                if cid not in last_analyses:
                    last_analyses[cid] = {
                        "last_date": row["run_date"],
                        "avg_retention": row.get("channel_avg_retention")
                    }

        # 3. Agrupa por subnicho
        subnichos = {}
        com_relatorio = 0
        for ch in channels:
            sub = ch.get("subnicho", "Outros") or "Outros"
            if sub not in subnichos:
                subnichos[sub] = []

            analysis = last_analyses.get(ch["channel_id"])
            last_date = None
            avg_ret = None
            if analysis:
                last_date = analysis["last_date"]
                avg_ret = analysis["avg_retention"]
                com_relatorio += 1

            subnichos[sub].append({
                "channel_id": ch["channel_id"],
                "channel_name": ch.get("channel_name", ""),
                "lingua": ch.get("lingua", ""),
                "is_monetized": ch.get("is_monetized", False),
                "last_analysis_date": last_date,
                "avg_retention": avg_ret
            })

        return {
            "subnichos": subnichos,
            "stats": {
                "total": len(channels),
                "com_relatorio": com_relatorio
            }
        }
    except Exception as e:
        logger.error(f"Erro dash-analise-copy channels: {e}")
        raise HTTPException(status_code=500, detail=str(e))


DASH_COPY_ANALYSIS_HTML = '''<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Analise de Copy - Dashboard</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Plus+Jakarta+Sans:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
    --bg-primary: #0a0a0f;
    --bg-secondary: #12121a;
    --bg-tertiary: #1a1a24;
    --bg-card: #16161f;
    --text-primary: #e8e8ed;
    --text-secondary: #a0a0b0;
    --text-muted: #6b6b7b;
    --accent: #00d4aa;
    --accent-dim: rgba(0, 212, 170, 0.15);
    --warning: #ff6b6b;
    --warning-dim: rgba(255, 107, 107, 0.15);
    --highlight: #ffd93d;
    --highlight-dim: rgba(255, 217, 61, 0.1);
    --purple: #a78bfa;
    --purple-dim: rgba(167, 139, 250, 0.15);
    --orange: #ff9f43;
    --blue: #54a0ff;
    --border: rgba(255, 255, 255, 0.08);
}
* { margin:0; padding:0; box-sizing:border-box; }
body {
    font-family: 'Plus Jakarta Sans', sans-serif;
    background: var(--bg-primary);
    color: var(--text-primary);
    line-height: 1.6;
}
.container { display:flex; min-height:100vh; }

/* Sidebar */
.sidebar {
    width: 280px;
    background: var(--bg-secondary);
    border-right: 1px solid var(--border);
    padding: 1.5rem 1rem;
    position: fixed;
    height: 100vh;
    overflow-y: auto;
    z-index: 10;
}
.sidebar-header {
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border);
}
.sidebar-title {
    font-size: 0.7rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: var(--accent);
    font-weight: 600;
}
.sidebar-subtitle {
    font-size: 1rem;
    font-weight: 700;
    color: var(--text-primary);
    margin-top: 0.25rem;
}
.sidebar-stats {
    font-size: 0.75rem;
    color: var(--text-muted);
    margin-top: 0.5rem;
}
.sidebar-actions {
    display: flex;
    gap: 0.5rem;
    margin-bottom: 1.5rem;
}
.btn {
    padding: 0.5rem 1rem;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    font-size: 0.8rem;
    font-weight: 600;
    font-family: inherit;
    transition: all 0.2s;
}
.btn-accent {
    background: var(--accent);
    color: #000;
    flex: 1;
}
.btn-accent:hover { opacity: 0.85; }
.btn-accent:disabled { opacity: 0.4; cursor: not-allowed; }
.btn-secondary {
    background: var(--bg-tertiary);
    color: var(--text-secondary);
    border: 1px solid var(--border);
}
.btn-secondary:hover { border-color: var(--accent); color: var(--accent); }

.subnicho-group { margin-bottom: 1rem; }
.subnicho-label {
    font-size: 0.65rem;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    margin-bottom: 0.4rem;
    font-weight: 600;
    padding-left: 0.5rem;
    display: flex;
    align-items: center;
    gap: 0.4rem;
}
.subnicho-icon {
    font-size: 0.75rem;
}
.channel-flag {
    font-size: 0.7rem;
    margin-right: 0.2rem;
    flex-shrink: 0;
}
.channel-info {
    display: flex;
    align-items: center;
    overflow: hidden;
    min-width: 0;
}
.channel-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.5rem 0.75rem;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.15s;
    margin-bottom: 2px;
}
.channel-item:hover { background: var(--bg-tertiary); }
.channel-item.active { background: var(--accent-dim); border-left: 3px solid var(--accent); }
.channel-name {
    font-size: 0.82rem;
    color: var(--text-secondary);
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    max-width: 170px;
}
.channel-item.active .channel-name { color: var(--accent); font-weight: 600; }
.channel-date {
    font-size: 0.68rem;
    color: var(--text-muted);
    font-family: 'JetBrains Mono', monospace;
}
.channel-date.has-data { color: var(--accent); }

/* Main area */
.main {
    margin-left: 280px;
    flex: 1;
    padding: 2rem 3rem;
    min-height: 100vh;
}
.main-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 2rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border);
}
.main-title { font-size: 1.2rem; font-weight: 700; }
.main-actions { display: flex; gap: 0.5rem; }

/* Report area */
.report-container {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 2rem;
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.85rem;
    line-height: 1.8;
    white-space: pre-wrap;
    word-break: break-word;
}
.report-header-line {
    color: var(--accent);
    font-weight: 600;
}
.report-title {
    color: var(--accent);
    font-size: 1rem;
    font-weight: 700;
}
.report-meta { color: var(--text-secondary); }
.report-meta .val { color: var(--highlight); font-weight: 600; }
.section-header {
    color: var(--accent);
    font-weight: 600;
    margin-top: 1.5rem;
    margin-bottom: 0.5rem;
    font-size: 0.9rem;
}
.section-header.obs { color: var(--purple); }
.section-header.anom { color: var(--warning); }
.section-header.insuf { color: var(--text-muted); }
.section-header.comp { color: var(--blue); }
.ranking-line { color: var(--text-primary); }
.tag-acima { color: var(--accent); font-weight: 700; }
.tag-media { color: var(--highlight); font-weight: 600; }
.tag-abaixo { color: var(--warning); font-weight: 700; }
.anomaly-line { color: var(--warning); font-weight: 600; }
.anomaly-detail { color: var(--text-secondary); padding-left: 0.5rem; }
.narrative { color: var(--purple); }
.comp-positive { color: var(--accent); }
.comp-negative { color: var(--warning); }
.insuf-line { color: var(--text-muted); }
.table-header-line { color: var(--text-muted); font-size: 0.78rem; }

/* Empty state */
.empty-state {
    text-align: center;
    padding: 4rem 2rem;
    color: var(--text-muted);
}
.empty-state h2 { color: var(--text-secondary); margin-bottom: 1rem; font-size: 1.1rem; }
.empty-state p { margin-bottom: 1.5rem; }

/* Loading */
.loading { text-align: center; padding: 3rem; color: var(--accent); }
.loading-spinner {
    display: inline-block;
    width: 24px;
    height: 24px;
    border: 3px solid var(--border);
    border-top-color: var(--accent);
    border-radius: 50%;
    animation: spin 0.8s linear infinite;
    margin-right: 0.5rem;
    vertical-align: middle;
}
@keyframes spin { to { transform: rotate(360deg); } }

/* Modal */
.modal-overlay {
    display: none;
    position: fixed;
    top: 0; left: 0; right: 0; bottom: 0;
    background: rgba(0,0,0,0.7);
    z-index: 100;
    justify-content: center;
    align-items: center;
}
.modal-overlay.active { display: flex; }
.modal {
    background: var(--bg-secondary);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 2rem;
    max-width: 600px;
    width: 90%;
    max-height: 80vh;
    overflow-y: auto;
}
.modal h3 { color: var(--accent); margin-bottom: 1rem; }
.modal-close {
    float: right;
    background: none;
    border: none;
    color: var(--text-muted);
    font-size: 1.5rem;
    cursor: pointer;
}
.modal-close:hover { color: var(--text-primary); }
.history-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.75rem 1rem;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.15s;
    margin-bottom: 4px;
    border: 1px solid var(--border);
}
.history-item:hover { border-color: var(--accent); background: var(--accent-dim); }
.history-date { font-family: 'JetBrains Mono', monospace; color: var(--accent); font-weight: 600; }
.history-info { color: var(--text-muted); font-size: 0.8rem; }

@media (max-width: 1024px) {
    .sidebar { display: none; }
    .main { margin-left: 0; padding: 1.5rem; }
}
</style>
</head>
<body>
<div class="container">
    <aside class="sidebar">
        <div class="sidebar-header">
            <div class="sidebar-title">Dashboard</div>
            <div class="sidebar-subtitle">Analise de Copy</div>
            <div class="sidebar-stats" id="sidebarStats">Carregando...</div>
        </div>
        <div class="sidebar-actions">
            <button class="btn btn-accent" onclick="runAll()" id="btnRunAll">Rodar Todos</button>
        </div>
        <div id="channelList">
            <div class="loading"><span class="loading-spinner"></span> Carregando canais...</div>
        </div>
    </aside>
    <main class="main">
        <div class="main-header">
            <div class="main-title" id="mainTitle">Selecione um canal</div>
            <div class="main-actions" id="mainActions" style="display:none">
                <button class="btn btn-accent" onclick="runAnalysis()" id="btnRun">Gerar Relatorio</button>
                <button class="btn btn-secondary" onclick="showHistory()" id="btnHistory">Historico</button>
            </div>
        </div>
        <div id="reportArea">
            <div class="empty-state">
                <h2>Analise de Estrutura de Copy</h2>
                <p>Selecione um canal na sidebar para visualizar o relatorio<br>ou clique em "Rodar Todos" para gerar analises.</p>
            </div>
        </div>
    </main>
</div>

<div class="modal-overlay" id="historyModal">
    <div class="modal">
        <button class="modal-close" onclick="closeHistory()">&times;</button>
        <h3>Historico de Relatorios</h3>
        <div id="historyList">
            <div class="loading"><span class="loading-spinner"></span> Carregando...</div>
        </div>
    </div>
</div>

<script>
var _selectedChannel = null;
var _channelsData = {};

function getSubnichoStyle(sub) {
    var map = {
        'Monetizados': {color:'#22c55e', icon:'$'},
        'Historias Sombrias': {color:'#8b5cf6', icon:'\u265B'},
        'Relatos de Guerra': {color:'#4a8c50', icon:'\u2694'},
        'Guerras e Civilizacoes': {color:'#f97316', icon:'\u26E8'},
        'Guerras e Civiliza\u00e7\u00f5es': {color:'#f97316', icon:'\u26E8'},
        'Terror': {color:'#ef4444', icon:'\u2620'},
        'Desmonetizados': {color:'#71717a', icon:'\u25CB'}
    };
    return map[sub] || {color:'#64748b', icon:'\u25C6'};
}

function getFlag(lingua) {
    if (!lingua) return '';
    var l = lingua.toLowerCase();
    var map = {
        'pt':'\uD83C\uDDE7\uD83C\uDDF7','portugues':'\uD83C\uDDE7\uD83C\uDDF7','portuguese':'\uD83C\uDDE7\uD83C\uDDF7',
        'en':'\uD83C\uDDFA\uD83C\uDDF8','ingles':'\uD83C\uDDFA\uD83C\uDDF8','english':'\uD83C\uDDFA\uD83C\uDDF8',
        'es':'\uD83C\uDDEA\uD83C\uDDF8','espanhol':'\uD83C\uDDEA\uD83C\uDDF8','spanish':'\uD83C\uDDEA\uD83C\uDDF8',
        'de':'\uD83C\uDDE9\uD83C\uDDEA','alemao':'\uD83C\uDDE9\uD83C\uDDEA','german':'\uD83C\uDDE9\uD83C\uDDEA',
        'fr':'\uD83C\uDDEB\uD83C\uDDF7','frances':'\uD83C\uDDEB\uD83C\uDDF7','french':'\uD83C\uDDEB\uD83C\uDDF7',
        'it':'\uD83C\uDDEE\uD83C\uDDF9','italiano':'\uD83C\uDDEE\uD83C\uDDF9','italian':'\uD83C\uDDEE\uD83C\uDDF9',
        'pl':'\uD83C\uDDF5\uD83C\uDDF1','polones':'\uD83C\uDDF5\uD83C\uDDF1','polish':'\uD83C\uDDF5\uD83C\uDDF1',
        'ru':'\uD83C\uDDF7\uD83C\uDDFA','russo':'\uD83C\uDDF7\uD83C\uDDFA','russian':'\uD83C\uDDF7\uD83C\uDDFA',
        'ja':'\uD83C\uDDEF\uD83C\uDDF5','japones':'\uD83C\uDDEF\uD83C\uDDF5','japanese':'\uD83C\uDDEF\uD83C\uDDF5',
        'ko':'\uD83C\uDDF0\uD83C\uDDF7','coreano':'\uD83C\uDDF0\uD83C\uDDF7','korean':'\uD83C\uDDF0\uD83C\uDDF7',
        'tr':'\uD83C\uDDF9\uD83C\uDDF7','turco':'\uD83C\uDDF9\uD83C\uDDF7','turkish':'\uD83C\uDDF9\uD83C\uDDF7',
        'ar':'\uD83C\uDDF8\uD83C\uDDE6','arabic':'\uD83C\uDDF8\uD83C\uDDE6','arabe':'\uD83C\uDDF8\uD83C\uDDE6'
    };
    return map[l] || '';
}

function loadChannels() {
    fetch('/api/dash-analise-copy/channels')
        .then(function(r) { return r.json(); })
        .then(function(data) {
            var el = document.getElementById('channelList');
            var stats = document.getElementById('sidebarStats');
            var s = data.stats || {};
            stats.textContent = s.total + ' canais | ' + s.com_relatorio + ' com relatorio';

            var html = '';
            var subnichos = data.subnichos || {};
            var order = ['Monetizados','Relatos de Guerra','Historias Sombrias','Terror','Guerras e Civilizacoes','Desmonetizados'];
            var keys = Object.keys(subnichos).sort(function(a,b) {
                var ia = order.indexOf(a), ib = order.indexOf(b);
                if (ia === -1) ia = 99; if (ib === -1) ib = 99;
                return ia - ib;
            });
            for (var i = 0; i < keys.length; i++) {
                var sub = keys[i];
                var channels = subnichos[sub];
                var sStyle = getSubnichoStyle(sub);
                html += '<div class="subnicho-group">';
                html += '<div class="subnicho-label" style="color:' + sStyle.color + ';">';
                html += '<span class="subnicho-icon">' + sStyle.icon + '</span>';
                html += escHtml(sub) + ' <span style="opacity:0.5;font-size:0.6rem;">(' + channels.length + ')</span></div>';
                for (var j = 0; j < channels.length; j++) {
                    var ch = channels[j];
                    _channelsData[ch.channel_id] = ch;
                    var dateStr = '--';
                    var dateClass = '';
                    if (ch.last_analysis_date) {
                        var d = new Date(ch.last_analysis_date);
                        dateStr = pad(d.getDate()) + '/' + pad(d.getMonth()+1);
                        dateClass = ' has-data';
                    }
                    var flag = getFlag(ch.lingua || '');
                    html += '<div class="channel-item" id="ch-' + ch.channel_id + '" onclick="selectChannel(\\'' + ch.channel_id + '\\')">';
                    html += '<div class="channel-info">';
                    if (flag) html += '<span class="channel-flag">' + flag + '</span>';
                    html += '<span class="channel-name" title="' + escHtml(ch.channel_name) + '">' + escHtml(ch.channel_name) + '</span>';
                    html += '</div>';
                    html += '<span class="channel-date' + dateClass + '">' + dateStr + '</span>';
                    html += '</div>';
                }
                html += '</div>';
            }
            el.innerHTML = html;
        })
        .catch(function(e) {
            document.getElementById('channelList').innerHTML = '<div class="empty-state"><p>Erro ao carregar canais</p></div>';
        });
}

function selectChannel(channelId) {
    _selectedChannel = channelId;
    var items = document.querySelectorAll('.channel-item');
    for (var i = 0; i < items.length; i++) items[i].classList.remove('active');
    var el = document.getElementById('ch-' + channelId);
    if (el) el.classList.add('active');

    var ch = _channelsData[channelId] || {};
    document.getElementById('mainTitle').textContent = ch.channel_name || channelId;
    document.getElementById('mainActions').style.display = 'flex';

    loadLatestReport(channelId);
}

function loadLatestReport(channelId) {
    var area = document.getElementById('reportArea');
    area.innerHTML = '<div class="loading"><span class="loading-spinner"></span> Carregando relatorio...</div>';

    fetch('/api/analise-copy/' + channelId + '/latest')
        .then(function(r) {
            if (r.status === 404) return null;
            return r.json();
        })
        .then(function(data) {
            if (!data) {
                area.innerHTML = '<div class="empty-state"><h2>Nenhuma analise encontrada</h2><p>Clique em "Gerar Relatorio" para criar a primeira analise deste canal.</p></div>';
                return;
            }
            renderReport(data, area);
        })
        .catch(function(e) {
            area.innerHTML = '<div class="empty-state"><p>Erro ao carregar relatorio: ' + escHtml(e.message) + '</p></div>';
        });
}

function renderReport(data, container) {
    var text = data.report_text || '';
    if (!text) {
        container.innerHTML = '<div class="empty-state"><p>Relatorio sem conteudo</p></div>';
        return;
    }

    var runDate = data.run_date ? new Date(data.run_date).toLocaleString('pt-BR') : '';
    var infoHtml = runDate ? '<div style="color:var(--text-muted);font-size:0.75rem;margin-bottom:1rem;font-family:sans-serif;">Gerado em: ' + runDate + '</div>' : '';

    var lines = text.split('\\n');
    var html = '';
    var inSection = '';

    for (var i = 0; i < lines.length; i++) {
        var line = lines[i];
        var trimmed = line.trim();

        // Header lines (====)
        if (/^={10,}/.test(trimmed)) {
            html += '<div class="report-header-line">' + escHtml(line) + '</div>';
            continue;
        }

        // Report title (RELATORIO ...)
        if (/^RELATORIO /.test(trimmed)) {
            html += '<div class="report-title">' + escHtml(line) + '</div>';
            continue;
        }

        // Section headers (--- NAME ---)
        if (/^---\\s+(.+?)\\s+---/.test(trimmed)) {
            var secName = trimmed.replace(/^---\\s+/, '').replace(/\\s+---$/, '');
            var secClass = 'section-header';
            if (/OBSERVAC/.test(secName)) { secClass += ' obs'; inSection = 'obs'; }
            else if (/ANOMAL/.test(secName)) { secClass += ' anom'; inSection = 'anom'; }
            else if (/INSUFICIENTE/.test(secName)) { secClass += ' insuf'; inSection = 'insuf'; }
            else if (/ANTERIOR/.test(secName)) { secClass += ' comp'; inSection = 'comp'; }
            else { inSection = 'ranking'; }
            html += '<div class="' + secClass + '">' + escHtml(line) + '</div>';
            continue;
        }

        // Empty line
        if (trimmed === '') {
            html += '<br>';
            continue;
        }

        // Anomaly line (! Estrutura ...)
        if (/^!\\s+Estrutura/.test(trimmed)) {
            html += '<div class="anomaly-line">' + escHtml(line) + '</div>';
            continue;
        }

        // Anomaly detail (   Retencao: ... / Views: ... / Publicado: ... / NOTA: ...)
        if (inSection === 'anom' && /^\\s{2,}/.test(line)) {
            html += '<div class="anomaly-detail">' + escHtml(line) + '</div>';
            continue;
        }

        // Meta lines (Videos analisados / Periodo / Media geral)
        if (/^Videos analisados/.test(trimmed) || /^Periodo:/.test(trimmed) || /^Media geral/.test(trimmed)) {
            var metaHtml = escHtml(line);
            metaHtml = metaHtml.replace(/(\\d+\\.?\\d*%)/g, '<span class="val">$1</span>');
            metaHtml = metaHtml.replace(/(\\d+\\.?\\d* min)/g, '<span class="val">$1</span>');
            metaHtml = metaHtml.replace(/(\\d[\\d,]+ views)/g, '<span class="val">$1</span>');
            html += '<div class="report-meta">' + metaHtml + '</div>';
            continue;
        }

        // Table header line (# / Estr. / dashes)
        if (/^\\s*#\\s+Estr/.test(trimmed) || /^\\s*Estr\\.?\\s+/.test(trimmed) || /^\\s*[‚îÄ-]{3,}/.test(trimmed)) {
            html += '<div class="table-header-line">' + escHtml(line) + '</div>';
            continue;
        }

        // Ranking lines (contain Acima/Media/Abaixo)
        if (/Acima|Media|Abaixo/.test(trimmed) && inSection === 'ranking') {
            var rLine = escHtml(line);
            rLine = rLine.replace(/Acima(\\s*\\([^)]*\\))?/g, '<span class="tag-acima">Acima$1</span>');
            rLine = rLine.replace(/Media(\\s*\\([^)]*\\))?/g, '<span class="tag-media">Media$1</span>');
            rLine = rLine.replace(/Abaixo(\\s*\\([^)]*\\))?/g, '<span class="tag-abaixo">Abaixo$1</span>');
            html += '<div class="ranking-line">' + rLine + '</div>';
            continue;
        }

        // Comparison lines (with +X% or -X%)
        if (inSection === 'comp' && /[+-]\\d+\\.?\\d*%/.test(trimmed)) {
            var cLine = escHtml(line);
            cLine = cLine.replace(/(\\+\\d+\\.?\\d*%)/g, '<span class="comp-positive">$1</span>');
            cLine = cLine.replace(/(\\-\\d+\\.?\\d*%)/g, '<span class="comp-negative">$1</span>');
            cLine = cLine.replace(/(Subiu[^<]*)/g, '<span class="comp-positive">$1</span>');
            cLine = cLine.replace(/(Caiu[^<]*)/g, '<span class="comp-negative">$1</span>');
            html += '<div class="ranking-line">' + cLine + '</div>';
            continue;
        }

        // Insufficient data lines
        if (inSection === 'insuf') {
            html += '<div class="insuf-line">' + escHtml(line) + '</div>';
            continue;
        }

        // Observation/narrative text
        if (inSection === 'obs' || (inSection === 'comp' && !/^\\s*\\d/.test(trimmed) && !/^\\s*Estr/.test(trimmed) && !/^Estruturas novas/.test(trimmed))) {
            html += '<div class="narrative">' + escHtml(line) + '</div>';
            continue;
        }

        // Default
        html += '<div>' + escHtml(line) + '</div>';
    }

    container.innerHTML = infoHtml + '<div class="report-container">' + html + '</div>';
}

function runAnalysis() {
    if (!_selectedChannel) return;
    var ch = _channelsData[_selectedChannel] || {};
    if (!confirm('Gerar relatorio para ' + (ch.channel_name || _selectedChannel) + '?\\n\\nIsso pode demorar 10-30 segundos.')) return;

    var btn = document.getElementById('btnRun');
    btn.disabled = true;
    btn.textContent = 'Gerando...';
    var area = document.getElementById('reportArea');
    area.innerHTML = '<div class="loading"><span class="loading-spinner"></span> Gerando relatorio... (10-30s)</div>';

    fetch('/api/analise-copy/' + _selectedChannel, { method: 'POST' })
        .then(function(r) { return r.json(); })
        .then(function(data) {
            btn.disabled = false;
            btn.textContent = 'Gerar Relatorio';
            if (data.success && data.report) {
                renderReport({ report_text: data.report, run_date: new Date().toISOString() }, area);
                loadChannels();
            } else {
                area.innerHTML = '<div class="empty-state"><h2>Erro na analise</h2><p>' + escHtml(data.error || 'Erro desconhecido') + '</p></div>';
            }
        })
        .catch(function(e) {
            btn.disabled = false;
            btn.textContent = 'Gerar Relatorio';
            area.innerHTML = '<div class="empty-state"><p>Erro: ' + escHtml(e.message) + '</p></div>';
        });
}

function runAll() {
    if (!confirm('Rodar analise de TODOS os canais?\\n\\nIsso pode demorar varios minutos.')) return;
    var btn = document.getElementById('btnRunAll');
    btn.disabled = true;
    btn.textContent = 'Rodando...';

    fetch('/api/analise-copy/run-all', { method: 'POST' })
        .then(function(r) { return r.json(); })
        .then(function(data) {
            btn.disabled = false;
            btn.textContent = 'Rodar Todos';
            var msg = 'Concluido! ' + (data.success_count || 0) + ' sucesso, ' + (data.error_count || 0) + ' erros de ' + (data.total_channels || 0) + ' canais.';
            alert(msg);
            loadChannels();
            if (_selectedChannel) loadLatestReport(_selectedChannel);
        })
        .catch(function(e) {
            btn.disabled = false;
            btn.textContent = 'Rodar Todos';
            alert('Erro: ' + e.message);
        });
}

function showHistory() {
    if (!_selectedChannel) return;
    document.getElementById('historyModal').classList.add('active');
    var el = document.getElementById('historyList');
    el.innerHTML = '<div class="loading"><span class="loading-spinner"></span> Carregando...</div>';

    fetch('/api/analise-copy/' + _selectedChannel + '/historico?limit=30')
        .then(function(r) { return r.json(); })
        .then(function(data) {
            var items = data.items || [];
            if (items.length === 0) {
                el.innerHTML = '<div class="empty-state"><p>Nenhum historico encontrado</p></div>';
                return;
            }
            var html = '';
            for (var i = 0; i < items.length; i++) {
                var item = items[i];
                var d = new Date(item.run_date);
                var dateStr = pad(d.getDate()) + '/' + pad(d.getMonth()+1) + '/' + d.getFullYear() + ' ' + pad(d.getHours()) + ':' + pad(d.getMinutes());
                var ret = item.channel_avg_retention ? item.channel_avg_retention.toFixed(1) + '%' : '--';
                var vids = item.total_videos_analyzed || '--';
                html += '<div class="history-item" onclick="loadHistoryReport(' + item.id + ')">';
                html += '<span class="history-date">' + dateStr + '</span>';
                html += '<span class="history-info">' + vids + ' videos | ret: ' + ret + '</span>';
                html += '</div>';
            }
            el.innerHTML = html;
        })
        .catch(function(e) {
            el.innerHTML = '<div class="empty-state"><p>Erro: ' + e.message + '</p></div>';
        });
}

function loadHistoryReport(runId) {
    closeHistory();
    if (!_selectedChannel) return;
    var area = document.getElementById('reportArea');
    area.innerHTML = '<div class="loading"><span class="loading-spinner"></span> Carregando relatorio...</div>';

    fetch('/api/analise-copy/' + _selectedChannel + '/latest')
        .then(function(r) { return r.json(); })
        .then(function(latest) {
            // Se o runId for o mais recente, ja temos
            if (latest && latest.id === runId) {
                renderReport(latest, area);
                return;
            }
            // Buscar historico e encontrar o run especifico
            return fetch('/api/analise-copy/' + _selectedChannel + '/historico?limit=100')
                .then(function(r2) { return r2.json(); })
                .then(function(hist) {
                    var items = hist.items || [];
                    // Historico nao traz report_text (so sumario)
                    // Precisamos do report_text - buscar da tabela diretamente
                    // Como nao temos endpoint por run_id, mostramos o sumario
                    for (var i = 0; i < items.length; i++) {
                        if (items[i].id === runId) {
                            var info = items[i];
                            var html = '<div class="report-container">';
                            html += '<div class="report-title">Relatorio de ' + new Date(info.run_date).toLocaleDateString('pt-BR') + '</div><br>';
                            html += '<div class="report-meta">Videos analisados: <span class="val">' + (info.total_videos_analyzed || '--') + '</span></div>';
                            html += '<div class="report-meta">Retencao media: <span class="val">' + (info.channel_avg_retention ? info.channel_avg_retention.toFixed(1) + '%' : '--') + '</span></div>';
                            html += '<div class="report-meta">Watch time medio: <span class="val">' + (info.channel_avg_watch_time ? info.channel_avg_watch_time.toFixed(1) + ' min' : '--') + '</span></div>';
                            html += '<div class="report-meta">Views media: <span class="val">' + (info.channel_avg_views ? Math.round(info.channel_avg_views).toLocaleString() : '--') + '</span></div>';
                            html += '<br><div style="color:var(--text-muted)">Relatorio detalhado disponivel apenas para a analise mais recente.</div>';
                            html += '</div>';
                            area.innerHTML = html;
                            return;
                        }
                    }
                    area.innerHTML = '<div class="empty-state"><p>Relatorio nao encontrado</p></div>';
                });
        })
        .catch(function(e) {
            area.innerHTML = '<div class="empty-state"><p>Erro: ' + e.message + '</p></div>';
        });
}

function closeHistory() {
    document.getElementById('historyModal').classList.remove('active');
}

function escHtml(s) {
    if (!s) return '';
    return String(s).replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;').replace(/"/g,'&quot;');
}

function pad(n) { return n < 10 ? '0' + n : '' + n; }

// Init
loadChannels();
</script>
</body>
</html>'''


@app.get("/dash-analise-copy", response_class=HTMLResponse)
async def dash_copy_analysis_page():
    """Dashboard de Analise de Copy - Interface web"""
    return DASH_COPY_ANALYSIS_HTML


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
