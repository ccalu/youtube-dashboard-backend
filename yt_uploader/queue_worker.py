"""
Upload Queue Worker - Processador autom√°tico de fila de uploads

Sistema isolado que processa v√≠deos pendentes na yt_upload_queue.
Totalmente desacoplado do main.py para garantir estabilidade.

Features:
- Circuit breaker (auto-desliga ap√≥s erros consecutivos)
- Resource monitoring (mem√≥ria + disco)
- Startup protection (delay configur√°vel)
- Proper task management (sem acumula√ß√£o)
- Toggle via environment variable
- Logs detalhados
"""

import asyncio
import logging
import os
import sys
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone

logger = logging.getLogger(__name__)


class UploadQueueWorker:
    """
    Worker isolado para processar fila de uploads automaticamente.

    Responsabilidades:
    - Buscar v√≠deos com status='pending' periodicamente
    - Disparar uploads respeitando limites de concorr√™ncia
    - Monitorar recursos (mem√≥ria, disco)
    - Auto-desligar em caso de erros consecutivos
    """

    def __init__(self):
        """Inicializa worker com configura√ß√µes de ENV"""

        # Configura√ß√µes
        self.interval_seconds = int(os.getenv("UPLOAD_WORKER_INTERVAL_SECONDS", "120"))
        self.batch_size = int(os.getenv("UPLOAD_WORKER_BATCH_SIZE", "5"))  # Reduzido para seguran√ßa
        self.max_consecutive_errors = int(os.getenv("UPLOAD_WORKER_MAX_ERRORS", "5"))
        self.min_free_memory_mb = int(os.getenv("UPLOAD_WORKER_MIN_FREE_MEMORY_MB", "200"))
        self.min_free_disk_mb = int(os.getenv("UPLOAD_WORKER_MIN_FREE_DISK_MB", "500"))

        # Estado interno
        self.consecutive_errors = 0
        self.is_active = True
        self.total_processed = 0
        self.total_succeeded = 0
        self.total_failed = 0

        # Lazy initialization (evita imports circulares)
        self._db_client = None
        self._process_upload_func = None

    def _lazy_init_dependencies(self):
        """
        Inicializa depend√™ncias de forma lazy.
        Evita imports circulares e problemas de timing.
        """
        if self._db_client is None:
            try:
                # Importa DENTRO da fun√ß√£o (n√£o no topo do arquivo)
                from yt_uploader.database import get_pending_uploads
                self._get_pending_uploads = get_pending_uploads

                # Import da fun√ß√£o de processamento
                # IMPORTANTE: Vamos importar do main.py que j√° tem tudo configurado
                import sys
                if 'main' in sys.modules:
                    main_module = sys.modules['main']
                    self._process_upload_func = main_module.process_upload_task
                else:
                    logger.error("‚ùå M√≥dulo 'main' n√£o encontrado - worker n√£o pode processar uploads")
                    self.is_active = False
                    return False

                logger.info("‚úÖ Worker dependencies initialized")
                return True

            except Exception as e:
                logger.error(f"‚ùå Erro ao inicializar depend√™ncias: {e}", exc_info=True)
                self.is_active = False
                return False

        return True

    def check_resources(self) -> tuple[bool, str]:
        """
        Verifica se h√° recursos suficientes para processar uploads.

        Returns:
            (ok: bool, message: str)
        """
        try:
            # Verifica mem√≥ria dispon√≠vel
            try:
                import psutil
                memory = psutil.virtual_memory()
                available_mb = memory.available / (1024 * 1024)

                if available_mb < self.min_free_memory_mb:
                    return False, f"Mem√≥ria insuficiente: {available_mb:.0f}MB dispon√≠vel (m√≠n: {self.min_free_memory_mb}MB)"
            except ImportError:
                # psutil n√£o dispon√≠vel - skip check
                logger.warning("‚ö†Ô∏è psutil n√£o instalado - check de mem√≥ria desabilitado")

            # Verifica espa√ßo em disco (/tmp para v√≠deos)
            try:
                import shutil
                stat = shutil.disk_usage('/tmp')
                available_mb = stat.free / (1024 * 1024)

                if available_mb < self.min_free_disk_mb:
                    return False, f"Disco insuficiente: {available_mb:.0f}MB dispon√≠vel (m√≠n: {self.min_free_disk_mb}MB)"
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel verificar disco: {e}")

            return True, "OK"

        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar recursos: {e}")
            return True, "Resource check failed - continuing anyway"

    async def _process_batch(self):
        """
        Processa um batch de uploads pendentes.

        Diferente do worker anterior que criava tasks sem aguardar,
        este aguarda a conclus√£o de todos antes de continuar.
        """
        # Verifica recursos antes de processar
        resources_ok, resources_msg = self.check_resources()
        if not resources_ok:
            logger.warning(f"‚ö†Ô∏è {resources_msg} - Aguardando pr√≥ximo ciclo")
            return

        # Busca v√≠deos pendentes
        try:
            pending_uploads = self._get_pending_uploads(limit=self.batch_size)
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar uploads pendentes: {e}")
            raise

        if not pending_uploads:
            # Nenhum v√≠deo pendente - OK, n√£o √© erro
            return

        logger.info(f"üì§ Encontrados {len(pending_uploads)} v√≠deos pendentes")

        # Processa uploads de forma segura
        # IMPORTANTE: Usamos gather com return_exceptions=True para n√£o crashar se um falhar
        tasks = []
        for upload in pending_uploads:
            upload_id = upload['id']
            titulo_preview = upload.get('titulo', 'Sem t√≠tulo')[:50]
            logger.info(f"   üé¨ Disparando upload_id={upload_id}: {titulo_preview}...")

            # Cria task
            task = asyncio.create_task(self._process_upload_func(upload_id))
            tasks.append(task)

        # AGUARDA TODOS (diferente do worker anterior)
        # return_exceptions=True evita que um erro cancele os outros
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Conta sucessos/falhas
        succeeded = sum(1 for r in results if not isinstance(r, Exception))
        failed = len(results) - succeeded

        self.total_processed += len(results)
        self.total_succeeded += succeeded
        self.total_failed += failed

        logger.info(f"üìä Batch conclu√≠do: {succeeded} sucesso, {failed} falhas")

    async def run(self):
        """
        Loop principal do worker.
        Roda indefinidamente at√© ser desativado.
        """
        logger.info("=" * 80)
        logger.info("üì§ UPLOAD QUEUE WORKER INICIADO")
        logger.info(f"‚è∞ Intervalo: {self.interval_seconds}s")
        logger.info(f"üì¶ Batch size: {self.batch_size} uploads/vez")
        logger.info(f"üîí Max erros consecutivos: {self.max_consecutive_errors}")
        logger.info("=" * 80)

        # Lazy initialization
        if not self._lazy_init_dependencies():
            logger.error("‚ùå Worker n√£o p√¥de inicializar - ABORTANDO")
            return

        while self.is_active:
            try:
                # Processa batch
                await self._process_batch()

                # Reset erro counter em caso de sucesso
                self.consecutive_errors = 0

            except Exception as e:
                self.consecutive_errors += 1
                logger.error(
                    f"‚ùå Erro no worker (tentativa {self.consecutive_errors}/{self.max_consecutive_errors}): {e}",
                    exc_info=True
                )

                # Circuit breaker
                if self.consecutive_errors >= self.max_consecutive_errors:
                    self.is_active = False
                    logger.critical("=" * 80)
                    logger.critical(f"üö® WORKER DESATIVADO ap√≥s {self.max_consecutive_errors} erros consecutivos")
                    logger.critical(f"üìä Estat√≠sticas finais:")
                    logger.critical(f"   Total processado: {self.total_processed}")
                    logger.critical(f"   Sucessos: {self.total_succeeded}")
                    logger.critical(f"   Falhas: {self.total_failed}")
                    logger.critical("=" * 80)
                    return

            # Aguarda pr√≥ximo ciclo
            if self.is_active:
                await asyncio.sleep(self.interval_seconds)

        logger.info("üì§ Worker encerrado gracefully")


async def start_queue_worker():
    """
    Entry point do worker com startup protection.

    Fun√ß√£o ass√≠ncrona que pode ser chamada via asyncio.create_task()
    no startup do main.py sem bloquear outros schedulers.
    """
    # Verifica se est√° habilitado
    enabled = os.getenv("UPLOAD_WORKER_ENABLED", "true").lower() == "true"
    if not enabled:
        logger.info("üì§ Upload queue worker DESABILITADO (UPLOAD_WORKER_ENABLED=false)")
        return

    # Startup protection (como outros schedulers)
    startup_delay = int(os.getenv("UPLOAD_WORKER_STARTUP_DELAY", "180"))  # 3 min padr√£o
    logger.info(f"üì§ Upload worker aguardando {startup_delay}s (startup protection)...")
    await asyncio.sleep(startup_delay)

    # Inicializa e roda worker
    try:
        worker = UploadQueueWorker()
        await worker.run()
    except Exception as e:
        logger.error(f"‚ùå Upload worker crashou: {e}", exc_info=True)
        # N√£o propaga exce√ß√£o - worker isolado n√£o deve afetar main app
