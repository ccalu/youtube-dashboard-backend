import os
import re
import asyncio
import logging
import html
import urllib.parse
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Any, Set
from collections import deque
import aiohttp
from aiohttp import ClientConnectionError
import json

logger = logging.getLogger(__name__)

# FUN√á√ÉO PARA DECODIFICAR HTML ENTITIES
def decode_html_entities(text: str) -> str:
    """Decodifica HTML entities em texto (ex: &#39; -> ')"""
    if not text:
        return text
    return html.unescape(text)


class RateLimiter:
    """
    Rate Limiter para respeitar o limite de 100 req/100s do YouTube
    Mant√©m hist√≥rico de requisi√ß√µes e calcula automaticamente quando pode fazer nova requisi√ß√£o
    """
    def __init__(self, max_requests: int = 90, time_window: int = 100):
        """
        max_requests: M√°ximo de requisi√ß√µes permitidas (90 para margem de seguran√ßa)
        time_window: Janela de tempo em segundos (100s)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()  # Armazena timestamps das requisi√ß√µes

    def _clean_old_requests(self):
        """Remove requisi√ß√µes antigas (fora da janela de 100s)"""
        now = datetime.now(timezone.utc)
        cutoff = now - timedelta(seconds=self.time_window)

        while self.requests and self.requests[0] < cutoff:
            self.requests.popleft()

    def can_make_request(self) -> bool:
        """Verifica se pode fazer uma nova requisi√ß√£o"""
        self._clean_old_requests()
        return len(self.requests) < self.max_requests

    def get_wait_time(self) -> float:
        """Calcula quanto tempo deve aguardar antes da pr√≥xima requisi√ß√£o"""
        self._clean_old_requests()

        if len(self.requests) < self.max_requests:
            return 0.0

        oldest = self.requests[0]
        now = datetime.now(timezone.utc)
        wait = (oldest + timedelta(seconds=self.time_window)) - now
        return max(0.0, wait.total_seconds())

    def record_request(self):
        """Registra que uma requisi√ß√£o foi feita"""
        self._clean_old_requests()
        self.requests.append(datetime.now(timezone.utc))

    async def wait_if_needed(self):
        """Aguarda automaticamente se necess√°rio antes de fazer requisi√ß√£o"""
        wait_time = self.get_wait_time()
        if wait_time > 0:
            logger.info(f"‚è≥ Rate limit pr√≥ximo - aguardando {wait_time:.1f}s")
            await asyncio.sleep(wait_time + 0.5)

    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas do rate limiter"""
        self._clean_old_requests()
        return {
            "requests_in_window": len(self.requests),
            "max_requests": self.max_requests,
            "utilization_pct": (len(self.requests) / self.max_requests) * 100
        }


class YouTubeCollector:
    def __init__(self):
        # üÜï SUPORTE PARA 20 CHAVES (KEY_3 a KEY_10 + KEY_21 a KEY_32)
        self.api_keys = [
            os.environ.get("YOUTUBE_API_KEY_3"),
            os.environ.get("YOUTUBE_API_KEY_4"),
            os.environ.get("YOUTUBE_API_KEY_5"),
            os.environ.get("YOUTUBE_API_KEY_6"),
            os.environ.get("YOUTUBE_API_KEY_7"),
            os.environ.get("YOUTUBE_API_KEY_8"),
            os.environ.get("YOUTUBE_API_KEY_9"),
            os.environ.get("YOUTUBE_API_KEY_10"),
            os.environ.get("YOUTUBE_API_KEY_21"),
            os.environ.get("YOUTUBE_API_KEY_22"),
            os.environ.get("YOUTUBE_API_KEY_23"),
            os.environ.get("YOUTUBE_API_KEY_24"),
            os.environ.get("YOUTUBE_API_KEY_25"),
            os.environ.get("YOUTUBE_API_KEY_26"),
            os.environ.get("YOUTUBE_API_KEY_27"),
            os.environ.get("YOUTUBE_API_KEY_28"),
            os.environ.get("YOUTUBE_API_KEY_29"),
            os.environ.get("YOUTUBE_API_KEY_30"),
            os.environ.get("YOUTUBE_API_KEY_31"),
            os.environ.get("YOUTUBE_API_KEY_32")
        ]

        self.api_keys = [key for key in self.api_keys if key]

        if not self.api_keys:
            raise ValueError("At least one YouTube API key is required")

        self.rate_limiters = {i: RateLimiter() for i in range(len(self.api_keys))}

        self.current_key_index = 0

        # RASTREAR DIA UTC QUE CADA CHAVE FOI ESGOTADA
        self.exhausted_keys_date: Dict[int, datetime.date] = {}

        # üÜï RASTREAR CHAVES SUSPENSAS (reseta no restart)
        self.suspended_keys: Set[int] = set()

        self.base_url = "https://www.googleapis.com/youtube/v3"

        # üÜï CONTADOR DE UNITS (CORRETO AGORA!)
        self.total_quota_units = 0  # Total de units gastos
        self.quota_units_per_key = {i: 0 for i in range(len(self.api_keys))}
        self.quota_units_per_canal: Dict[str, int] = {}
        self.failed_canals: Set[str] = set()

        # RETRY CONFIG
        self.max_retries = 3
        self.base_delay = 0.8

        # üöÄ OTIMIZA√á√ÉO: Cache de channel_id para evitar requisi√ß√µes duplicadas
        self.channel_id_cache: Dict[str, str] = {}  # {url_canal: channel_id}

        logger.info(f"üöÄ YouTube collector initialized with {len(self.api_keys)} API keys")
        logger.info(f"üìä Total quota dispon√≠vel: {len(self.api_keys) * 10000:,} units/dia")
        logger.info(f"üìä Rate limiter: {self.rate_limiters[0].max_requests} req/{self.rate_limiters[0].time_window}s per key")

    def reset_for_new_collection(self):
        """Reset collector state - LIMPA CHAVES SE J√Å MUDOU DE DIA UTC"""
        self.failed_canals = set()
        self.total_quota_units = 0
        self.quota_units_per_key = {i: 0 for i in range(len(self.api_keys))}
        self.quota_units_per_canal = {}

        # üöÄ OTIMIZA√á√ÉO: N√ÉO limpar channel_id_cache - pode reusar entre coletas
        # Cache persiste at√© restart do servidor (economiza requisi√ß√µes)

        # üÜï RESETAR CHAVES SUSPENSAS (podem ter voltado)
        if self.suspended_keys:
            logger.info("=" * 80)
            logger.info(f"üîÑ RESETANDO {len(self.suspended_keys)} CHAVES SUSPENSAS")
            for key_idx in self.suspended_keys:
                logger.info(f"‚úÖ Key {key_idx + 2} ser√° testada novamente")
            self.suspended_keys = set()
            logger.info("=" * 80)

        # LIMPAR CHAVES ESGOTADAS SE J√Å √â OUTRO DIA UTC
        today_utc = datetime.now(timezone.utc).date()

        keys_to_reset = []
        for key_index, exhausted_date in list(self.exhausted_keys_date.items()):
            if exhausted_date < today_utc:
                keys_to_reset.append(key_index)

        if keys_to_reset:
            logger.info("=" * 80)
            logger.info(f"üîÑ RESETANDO {len(keys_to_reset)} CHAVES (novo dia UTC)")
            for key_index in keys_to_reset:
                del self.exhausted_keys_date[key_index]
                logger.info(f"‚úÖ Key {key_index + 2} dispon√≠vel novamente")
            logger.info("=" * 80)

        # Log status das chaves
        logger.info("=" * 80)
        logger.info("üîÑ COLLECTOR RESET")
        logger.info(f"üìÖ Dia UTC atual: {today_utc}")
        logger.info(f"üîë Chaves dispon√≠veis: {len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)}/{len(self.api_keys)}")
        logger.info(f"üí∞ Quota total dispon√≠vel: {(len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)) * 10000:,} units")

        if self.exhausted_keys_date:
            logger.warning(f"‚ö†Ô∏è  Chaves esgotadas hoje:")
            for key_idx, date in self.exhausted_keys_date.items():
                logger.warning(f"   Key {key_idx + 2}: esgotada em {date}")

        if self.suspended_keys:
            logger.warning(f"‚ö†Ô∏è  Chaves suspensas:")
            for key_idx in self.suspended_keys:
                logger.warning(f"   Key {key_idx + 2}: suspensa temporariamente")

        logger.info(f"üìä Chave inicial: {self.current_key_index + 2}")
        logger.info("=" * 80)

    def get_request_cost(self, url: str) -> int:
        """
        üÜï CALCULA O CUSTO REAL EM UNITS DE CADA REQUISI√á√ÉO
        - search.list = 100 units (CARA!)
        - channels.list = 1 unit
        - videos.list = 1 unit
        """
        if "/search" in url:
            return 100  # Search √© MUITO caro!
        elif "/channels" in url:
            return 1
        elif "/videos" in url:
            return 1
        else:
            return 1

    def increment_quota_counter(self, canal_name: str, cost: int):
        """
        üÜï INCREMENTA CONTADOR DE QUOTA UNITS (CORRETO!)
        Agora usa o CUSTO REAL da requisi√ß√£o
        """
        self.total_quota_units += cost
        self.quota_units_per_key[self.current_key_index] += cost

        if canal_name not in self.quota_units_per_canal:
            self.quota_units_per_canal[canal_name] = 0
        self.quota_units_per_canal[canal_name] += cost

    def get_request_stats(self) -> Dict[str, Any]:
        """Get request statistics"""
        return {
            "total_quota_units": self.total_quota_units,  # Nome correto agora
            "quota_units_per_key": self.quota_units_per_key.copy(),
            "quota_units_per_canal": self.quota_units_per_canal.copy(),
            "failed_canals": list(self.failed_canals),
            "exhausted_keys": len(self.exhausted_keys_date),
            "suspended_keys": len(self.suspended_keys),
            "active_keys": len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys),
            "total_available_quota": (len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)) * 10000
        }

    def get_current_api_key(self) -> Optional[str]:
        """Get current API key - PULA chaves esgotadas E SUSPENSAS"""
        if self.all_keys_exhausted():
            return None

        attempts = 0
        while (self.current_key_index in self.exhausted_keys_date or self.current_key_index in self.suspended_keys) and attempts < len(self.api_keys):
            self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
            attempts += 1

        if attempts >= len(self.api_keys):
            return None

        return self.api_keys[self.current_key_index]

    def rotate_to_next_key(self):
        """Rotaciona para pr√≥xima chave dispon√≠vel - PULA SUSPENSAS E ESGOTADAS"""
        old_index = self.current_key_index
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)

        attempts = 0
        while (self.current_key_index in self.exhausted_keys_date or self.current_key_index in self.suspended_keys) and attempts < len(self.api_keys):
            self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
            attempts += 1

        if old_index != self.current_key_index:
            stats = self.rate_limiters[self.current_key_index].get_stats()
            logger.info(f"üîÑ Rotated: Key {old_index + 2} ‚Üí Key {self.current_key_index + 2} (load: {stats['requests_in_window']}/{stats['max_requests']})")

    def mark_key_as_exhausted(self):
        """Marca chave atual como esgotada AT√â MEIA-NOITE UTC"""
        today_utc = datetime.now(timezone.utc).date()
        self.exhausted_keys_date[self.current_key_index] = today_utc

        logger.error(f"üö® QUOTA EXCEEDED - Key {self.current_key_index + 2} EXHAUSTED at√© meia-noite UTC ({today_utc})")
        logger.error(f"üîë Chaves restantes: {len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)}/{len(self.api_keys)}")
        logger.error(f"üí∞ Quota restante: {(len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)) * 10000:,} units")

        self.rotate_to_next_key()

    def mark_key_as_suspended(self):
        """üÜï Marca chave atual como SUSPENSA (reseta no restart)"""
        self.suspended_keys.add(self.current_key_index)

        logger.error(f"‚ùå KEY SUSPENDED - Key {self.current_key_index + 2} marcada como suspensa at√© restart")
        logger.error(f"üîë Chaves restantes: {len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)}/{len(self.api_keys)}")
        logger.error(f"üí∞ Quota restante: {(len(self.api_keys) - len(self.exhausted_keys_date) - len(self.suspended_keys)) * 10000:,} units")

        self.rotate_to_next_key()

    def all_keys_exhausted(self) -> bool:
        """Check if all API keys are exhausted or suspended"""
        unavailable = len(self.exhausted_keys_date) + len(self.suspended_keys)
        return unavailable >= len(self.api_keys)

    def reset_suspended_keys(self):
        """üÜï Limpa lista de keys suspensas (para testar novamente)"""
        count = len(self.suspended_keys)
        self.suspended_keys = set()
        logger.info(f"‚úÖ {count} chaves suspensas resetadas - ser√£o testadas novamente")
        return count

    def mark_canal_as_failed(self, canal_url: str):
        """Mark a canal as failed"""
        self.failed_canals.add(canal_url)

    def is_canal_failed(self, canal_url: str) -> bool:
        """Check if canal already failed"""
        return canal_url in self.failed_canals

    async def make_api_request(self, url: str, params: dict, canal_name: str = "system", retry_count: int = 0) -> Optional[dict]:
        """Fun√ß√£o para fazer requisi√ß√µes √† API do YouTube"""
        if self.all_keys_exhausted():
            logger.error("‚ùå All keys exhausted or suspended!")
            return None

        current_key = self.get_current_api_key()
        if not current_key:
            return None

        params['key'] = current_key

        await self.rate_limiters[self.current_key_index].wait_if_needed()

        try:
            async with aiohttp.ClientSession() as session:
                # üÜï CALCULAR CUSTO REAL E INCREMENTAR CORRETAMENTE
                request_cost = self.get_request_cost(url)
                self.increment_quota_counter(canal_name, request_cost)
                self.rate_limiters[self.current_key_index].record_request()

                # üöÄ OTIMIZA√á√ÉO: Removido base_delay - RateLimiter j√° controla requisi√ß√µes
                # if self.total_quota_units > 0:
                #     await asyncio.sleep(self.base_delay)

                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=60)) as response:

                    if response.status == 200:
                        data = await response.json()
                        return data

                    elif response.status == 403:
                        error_data = await response.json()
                        error_obj = error_data.get('error', {})
                        error_msg = error_obj.get('message', '').lower()
                        error_reason = ''
                        if error_obj.get('errors'):
                            error_reason = error_obj['errors'][0].get('reason', '').lower()

                        logger.warning(f"‚ö†Ô∏è 403 Error - Message: '{error_msg}' | Reason: '{error_reason}'")

                        # CASO 1: Quota Excedida
                        if 'quota' in error_msg or 'quota' in error_reason or 'dailylimit' in error_reason:
                            logger.error(f"üö® QUOTA EXCEEDED on key {self.current_key_index + 2}")
                            self.mark_key_as_exhausted()

                            if retry_count < self.max_retries and not self.all_keys_exhausted():
                                logger.info(f"‚ôªÔ∏è Tentando com pr√≥xima chave dispon√≠vel...")
                                return await self.make_api_request(url, params, canal_name, retry_count + 1)
                            return None

                        # CASO 2: Rate Limit
                        elif 'ratelimit' in error_msg or 'ratelimit' in error_reason or 'usageratelimit' in error_reason:
                            if retry_count < self.max_retries:
                                wait_time = (2 ** retry_count) * 30
                                logger.warning(f"‚è±Ô∏è RATE LIMIT hit on key {self.current_key_index + 2}")
                                logger.info(f"‚ôªÔ∏è Retry {retry_count + 1}/{self.max_retries} ap√≥s {wait_time}s")
                                await asyncio.sleep(wait_time)
                                return await self.make_api_request(url, params, canal_name, retry_count + 1)
                            else:
                                logger.error(f"‚ùå Max retries atingido ap√≥s rate limit")
                                return None

                        # CASO 3: üÜï Key Suspensa (403 gen√©rico) - AGORA ROTACIONA!
                        else:
                            logger.error(f"‚ùå KEY SUSPENDED (403 gen√©rico) on key {self.current_key_index + 2}: {error_msg}")
                            self.mark_key_as_suspended()

                            if retry_count < self.max_retries and not self.all_keys_exhausted():
                                logger.info(f"‚ôªÔ∏è Tentando com pr√≥xima chave dispon√≠vel...")
                                return await self.make_api_request(url, params, canal_name, retry_count + 1)
                            else:
                                logger.error(f"‚ùå Todas as chaves esgotadas ou suspensas")
                                return None

                    else:
                        logger.warning(f"‚ö†Ô∏è HTTP {response.status}: {await response.text()}")
                        return None

        except asyncio.TimeoutError:
            logger.warning(f"‚è±Ô∏è Timeout na requisi√ß√£o")
            if retry_count < self.max_retries:
                await asyncio.sleep(5)
                return await self.make_api_request(url, params, canal_name, retry_count + 1)
            return None

        except ClientConnectionError as e:
            # Tratamento especial para ConnectionTerminated (limite HTTP/2)
            error_str = str(e)
            if 'ConnectionTerminated' in error_str or 'ConnectionReset' in error_str:
                logger.warning(f"‚ö†Ô∏è Conex√£o terminada pelo servidor YouTube (limite HTTP/2 atingido)")
                logger.info(f"   Erro: {error_str[:100]}...")

                if retry_count < self.max_retries:
                    # Aguardar um pouco mais para dar tempo do servidor resetar
                    wait_time = 3 + (retry_count * 2)
                    logger.info(f"‚ôªÔ∏è Tentando novamente em {wait_time}s (tentativa {retry_count + 1}/{self.max_retries})")
                    await asyncio.sleep(wait_time)
                    return await self.make_api_request(url, params, canal_name, retry_count + 1)
                else:
                    logger.error(f"‚ùå Max retries atingido ap√≥s ConnectionTerminated")
                    return None
            else:
                # Outros erros de conex√£o
                logger.error(f"‚ùå Erro de conex√£o: {e}")
                if retry_count < self.max_retries:
                    await asyncio.sleep(2)
                    return await self.make_api_request(url, params, canal_name, retry_count + 1)
                return None

        except Exception as e:
            logger.error(f"‚ùå Exception na requisi√ß√£o: {e}")
            return None

    def clean_youtube_url(self, url: str) -> str:
        """Remove extra paths from YouTube URL"""
        url = re.sub(r'/(videos|channel-analytics|about|featured|playlists|community|channels|streams|shorts).*$', '', url)
        return url

    def is_valid_channel_id(self, channel_id: str) -> bool:
        """Check if a string is a valid YouTube channel ID"""
        if not channel_id:
            return False
        return channel_id.startswith('UC') and len(channel_id) == 24

    def extract_channel_identifier(self, url: str) -> tuple[Optional[str], str]:
        """Extract channel identifier from YouTube URL - SUPORTA CARACTERES UNICODE"""
        url = self.clean_youtube_url(url)

        # 1. Channel ID direto (mais confi√°vel)
        channel_id_match = re.search(r'youtube\.com/channel/([a-zA-Z0-9_-]+)', url)
        if channel_id_match:
            channel_id = channel_id_match.group(1)
            if self.is_valid_channel_id(channel_id):
                return (channel_id, 'id')

        # 2. Handle (@...) - ACEITA QUALQUER CARACTERE
        handle_match = re.search(r'youtube\.com/@([^/?&#]+)', url)
        if handle_match:
            handle = handle_match.group(1)
            # Decodifica URL encoding (%C4%B1 etc)
            handle = urllib.parse.unquote(handle)
            logger.debug(f"Handle extra√≠do: {handle}")
            return (handle, 'handle')

        # 3. Custom URL (/c/)
        custom_match = re.search(r'youtube\.com/c/([a-zA-Z0-9._-]+)', url)
        if custom_match:
            username = custom_match.group(1)
            return (username, 'username')

        # 4. Old style (/user/)
        user_match = re.search(r'youtube\.com/user/([a-zA-Z0-9._-]+)', url)
        if user_match:
            username = user_match.group(1)
            return (username, 'username')

        return (None, 'unknown')

    async def get_channel_id_from_handle(self, handle: str, canal_name: str) -> Optional[str]:
        """Convert handle to channel ID"""
        if self.all_keys_exhausted():
            return None

        if handle.startswith('@'):
            handle = handle[1:]

        logger.info(f"üîç {canal_name}: Buscando channel ID para handle '{handle}'")

        # Try forHandle
        url = f"{self.base_url}/channels"
        params = {'part': 'id', 'forHandle': handle}

        data = await self.make_api_request(url, params, canal_name)
        if data and data.get('items'):
            channel_id = data['items'][0]['id']
            logger.info(f"‚úÖ {canal_name}: Channel ID encontrado via forHandle: {channel_id}")
            return channel_id

        # Try forUsername
        params = {'part': 'id', 'forUsername': handle}
        data = await self.make_api_request(url, params, canal_name)
        if data and data.get('items'):
            channel_id = data['items'][0]['id']
            logger.info(f"‚úÖ {canal_name}: Channel ID encontrado via forUsername: {channel_id}")
            return channel_id

        logger.warning(f"‚ùå {canal_name}: N√£o foi poss√≠vel encontrar channel ID para handle '{handle}'")
        return None

    async def get_channel_id(self, url: str, canal_name: str) -> Optional[str]:
        """
        Get channel ID from URL

        üöÄ OTIMIZA√á√ÉO: Usa cache para evitar requisi√ß√µes duplicadas
        - Se j√° resolvemos este URL antes, retorna do cache (0 requisi√ß√µes)
        - Cache persiste entre coletas at√© restart do servidor
        """
        # 1. Verificar cache primeiro
        if url in self.channel_id_cache:
            logger.debug(f"‚ö° {canal_name}: Cache hit para channel_id")
            return self.channel_id_cache[url]

        # 2. Resolver normalmente (c√≥digo existente)
        identifier, id_type = self.extract_channel_identifier(url)

        if not identifier:
            logger.error(f"‚ùå {canal_name}: N√£o foi poss√≠vel extrair identificador da URL: {url}")
            return None

        channel_id = None

        if id_type == 'id' and self.is_valid_channel_id(identifier):
            channel_id = identifier

        elif id_type in ['handle', 'username']:
            channel_id = await self.get_channel_id_from_handle(identifier, canal_name)

        # 3. Adicionar ao cache se resolveu com sucesso
        if channel_id:
            self.channel_id_cache[url] = channel_id
            logger.debug(f"üíæ {canal_name}: Channel ID cacheado: {channel_id}")

        return channel_id

    async def get_channel_info(self, channel_id: str, canal_name: str) -> Optional[Dict[str, Any]]:
        """Get channel info - EXPANDIDO com publishedAt e customUrl"""
        if not self.is_valid_channel_id(channel_id):
            return None

        url = f"{self.base_url}/channels"
        # Expandido para incluir mais dados (sem custo extra - mesma requisi√ß√£o)
        params = {'part': 'statistics,snippet', 'id': channel_id}

        data = await self.make_api_request(url, params, canal_name)

        if data and data.get('items'):
            channel = data['items'][0]
            stats = channel.get('statistics', {})
            snippet = channel.get('snippet', {})

            return {
                'channel_id': channel_id,
                'title': snippet.get('title'),
                'subscriber_count': int(stats.get('subscriberCount', 0)),
                'video_count': int(stats.get('videoCount', 0)),
                'view_count': int(stats.get('viewCount', 0)),
                # NOVOS CAMPOS para Analytics
                'published_at': snippet.get('publishedAt'),  # Data de cria√ß√£o do canal
                'custom_url': snippet.get('customUrl'),      # URL customizada (@handle)
                'country': snippet.get('country'),           # Pa√≠s do canal (opcional)
                'description': snippet.get('description')    # Descri√ß√£o do canal (opcional)
            }

        return None

    async def get_channel_videos(self, channel_id: str, canal_name: str, days: int = 30) -> List[Dict[str, Any]]:
        """
        üÜï Get channel videos - AGORA BUSCA APENAS 30 DIAS (em vez de 60)
        Isso economiza ~40-50% de quota!
        """
        if not self.is_valid_channel_id(channel_id):
            logger.warning(f"‚ùå {canal_name}: Invalid channel ID")
            return []

        if self.all_keys_exhausted():
            logger.warning(f"‚ùå {canal_name}: All keys exhausted")
            return []

        videos = []
        page_token = None
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

        logger.info(f"üîç {canal_name}: Buscando v√≠deos desde {cutoff_date.date()} (√∫ltimos {days} dias)")

        while True:
            if self.all_keys_exhausted():
                logger.warning(f"‚ö†Ô∏è {canal_name}: Keys exhausted during video fetch")
                break

            url = f"{self.base_url}/search"
            params = {
                'part': 'id,snippet',
                'channelId': channel_id,
                'type': 'video',
                'order': 'date',
                'maxResults': 50,
                'publishedAfter': cutoff_date.isoformat()
            }

            if page_token:
                params['pageToken'] = page_token

            data = await self.make_api_request(url, params, canal_name)

            if not data:
                logger.warning(f"‚ö†Ô∏è {canal_name}: API request returned None")
                break

            if not data.get('items'):
                logger.info(f"‚ÑπÔ∏è {canal_name}: No more videos found")
                break

            video_ids = [item['id']['videoId'] for item in data['items']]
            video_details = await self.get_video_details(video_ids, canal_name)

            for item, details in zip(data['items'], video_details):
                if details:
                    video_info = {
                        'video_id': item['id']['videoId'],
                        'titulo': decode_html_entities(item['snippet']['title']),
                        'url_video': f"https://www.youtube.com/watch?v={item['id']['videoId']}",
                        'data_publicacao': item['snippet']['publishedAt'],
                        'views_atuais': details.get('view_count', 0),
                        'likes': details.get('like_count', 0),
                        'comentarios': details.get('comment_count', 0),
                        'duracao': details.get('duration_seconds', 0)
                    }
                    videos.append(video_info)

            page_token = data.get('nextPageToken')
            if not page_token:
                break

        logger.info(f"‚úÖ {canal_name}: Encontrados {len(videos)} v√≠deos nos √∫ltimos {days} dias")
        return videos

    async def get_video_details(self, video_ids: List[str], canal_name: str) -> List[Optional[Dict[str, Any]]]:
        """Get video details"""
        if self.all_keys_exhausted():
            return [None] * len(video_ids)

        if not video_ids:
            return []

        details = []

        for i in range(0, len(video_ids), 50):
            if self.all_keys_exhausted():
                details.extend([None] * (len(video_ids) - i))
                break

            batch_ids = video_ids[i:i+50]

            url = f"{self.base_url}/videos"
            params = {
                'part': 'statistics,contentDetails',
                'id': ','.join(batch_ids)
            }

            data = await self.make_api_request(url, params, canal_name)

            if data and data.get('items'):
                for item in data['items']:
                    stats = item.get('statistics', {})
                    content = item.get('contentDetails', {})

                    video_detail = {
                        'view_count': int(stats.get('viewCount', 0)),
                        'like_count': int(stats.get('likeCount', 0)),
                        'comment_count': int(stats.get('commentCount', 0)),
                        'duration_seconds': self.parse_duration(content.get('duration', 'PT0S'))
                    }
                    details.append(video_detail)
            else:
                details.extend([None] * len(batch_ids))

        return details

    def parse_duration(self, duration_str: str) -> int:
        """Parse YouTube duration format to seconds"""
        try:
            pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
            match = re.match(pattern, duration_str)

            if match:
                hours = int(match.group(1) or 0)
                minutes = int(match.group(2) or 0)
                seconds = int(match.group(3) or 0)
                return hours * 3600 + minutes * 60 + seconds
        except:
            pass

        return 0

    def calculate_views_by_period(self, videos: List[Dict], current_date: datetime) -> Dict[str, int]:
        """
        üÜï Calculate views for different periods - SEM views_60d agora!
        Calcula apenas: views_30d, views_15d, views_7d
        """
        views_30d = views_15d = views_7d = 0

        if current_date.tzinfo is None:
            current_date = current_date.replace(tzinfo=timezone.utc)

        count_30d = count_15d = count_7d = 0

        for video in videos:
            try:
                pub_date_str = video['data_publicacao']
                pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))

                # Usar total_seconds() para precis√£o
                time_diff = current_date - pub_date
                days_ago = time_diff.total_seconds() / 86400

                if days_ago <= 30:
                    views_30d += video['views_atuais']
                    count_30d += 1
                if days_ago <= 15:
                    views_15d += video['views_atuais']
                    count_15d += 1
                if days_ago <= 7:
                    views_7d += video['views_atuais']
                    count_7d += 1

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao calcular views: {e}")
                continue

        logger.debug(f"üìä Views: 7d={views_7d} ({count_7d} v√≠deos), 30d={views_30d} ({count_30d} v√≠deos)")

        return {
            'views_30d': views_30d,
            'views_15d': views_15d,
            'views_7d': views_7d
        }

    async def get_canal_data(self, url_canal: str, canal_name: str) -> tuple[Optional[Dict[str, Any]], Optional[List[Dict[str, Any]]]]:
        """
        Get complete canal data AND videos in a single call.

        üöÄ OTIMIZA√á√ÉO: Retorna (stats, videos) para evitar buscar v√≠deos duas vezes.
        Economiza ~50% da quota de API!

        Returns:
            tuple: (canal_stats, videos_list) ou (None, None) em caso de erro
        """
        try:
            if self.is_canal_failed(url_canal):
                logger.warning(f"‚è≠Ô∏è Skipping {canal_name} - already failed")
                return None, None

            if self.all_keys_exhausted():
                logger.error(f"‚ùå {canal_name}: All keys exhausted")
                return None, None

            logger.info(f"üé¨ Iniciando coleta: {canal_name}")

            self.rotate_to_next_key()

            channel_id = await self.get_channel_id(url_canal, canal_name)

            if not channel_id:
                logger.error(f"‚ùå {canal_name}: N√£o foi poss√≠vel obter channel_id")
                self.mark_canal_as_failed(url_canal)
                return None, None

            logger.info(f"‚úÖ {canal_name}: Channel ID = {channel_id}")

            channel_info = await self.get_channel_info(channel_id, canal_name)
            if not channel_info:
                logger.error(f"‚ùå {canal_name}: N√£o foi poss√≠vel obter info do canal")
                self.mark_canal_as_failed(url_canal)
                return None, None

            logger.info(f"‚úÖ {canal_name}: {channel_info['subscriber_count']:,} inscritos")

            # üÜï BUSCA APENAS 30 DIAS (em vez de 60)
            videos = await self.get_channel_videos(channel_id, canal_name, days=30)

            if not videos:
                logger.warning(f"‚ö†Ô∏è {canal_name}: NENHUM v√≠deo encontrado nos √∫ltimos 30 dias!")

            current_date = datetime.now(timezone.utc)
            views_by_period = self.calculate_views_by_period(videos, current_date)

            videos_7d = sum(1 for v in videos if (current_date - datetime.fromisoformat(v['data_publicacao'].replace('Z', '+00:00'))).total_seconds() / 86400 <= 7)

            total_engagement = sum(v['likes'] + v['comentarios'] for v in videos)
            total_views = sum(v['views_atuais'] for v in videos)
            engagement_rate = (total_engagement / total_views * 100) if total_views > 0 else 0

            result = {
                'inscritos': channel_info['subscriber_count'],
                'videos_publicados_7d': videos_7d,
                'engagement_rate': round(engagement_rate, 2),
                **views_by_period,  # Agora s√≥ tem views_30d, views_15d, views_7d
                # NOVOS CAMPOS para Analytics
                'published_at': channel_info.get('published_at'),
                'custom_url': channel_info.get('custom_url'),
                'video_count': channel_info.get('video_count'),
                'view_count': channel_info.get('view_count')
            }

            logger.info(f"‚úÖ {canal_name}: Coleta conclu√≠da - 7d={views_by_period['views_7d']:,} views, {len(videos)} v√≠deos")

            # üöÄ OTIMIZA√á√ÉO: Retorna stats E v√≠deos para evitar busca duplicada
            return result, videos

        except Exception as e:
            logger.error(f"‚ùå Error for {canal_name}: {e}")
            self.mark_canal_as_failed(url_canal)
            return None, None

    async def get_videos_data(self, url_canal: str, canal_name: str) -> Optional[List[Dict[str, Any]]]:
        """Get videos data for a canal"""
        try:
            if self.is_canal_failed(url_canal):
                return None

            if self.all_keys_exhausted():
                return None

            channel_id = await self.get_channel_id(url_canal, canal_name)

            if not channel_id:
                return None

            # üÜï BUSCA APENAS 30 DIAS (em vez de 60)
            videos = await self.get_channel_videos(channel_id, canal_name, days=30)
            return videos

        except Exception as e:
            logger.error(f"‚ùå Error getting videos for {canal_name}: {e}")
            return None

    # ===============================================
    # üÜï SISTEMA DE COLETA DE COMENT√ÅRIOS
    # ===============================================

    async def get_video_comments(self, video_id: str, video_title: str = "", max_results: int = 100) -> Optional[List[Dict[str, Any]]]:
        """
        Busca coment√°rios de um v√≠deo espec√≠fico
        - Custo: 1 unit por request (at√© 100 coment√°rios)
        - Suporta pagina√ß√£o para buscar TODOS os coment√°rios
        """
        try:
            if not video_id:
                return []

            url = f"{self.base_url}/commentThreads"
            all_comments = []
            next_page_token = None
            total_fetched = 0

            logger.info(f"üí¨ Buscando coment√°rios do v√≠deo: {video_title[:50]}...")

            while True:
                params = {
                    'part': 'snippet,replies',
                    'videoId': video_id,
                    'maxResults': min(max_results, 100),  # M√°ximo 100 por request
                    'order': 'relevance',
                    'textFormat': 'plainText'
                }

                if next_page_token:
                    params['pageToken'] = next_page_token

                data = await self.make_api_request(url, params, f"Comments-{video_id[:10]}")

                if not data or 'items' not in data:
                    break

                # Processar coment√°rios principais
                for item in data.get('items', []):
                    snippet = item['snippet']['topLevelComment']['snippet']

                    comment = {
                        'comment_id': item['id'],
                        'video_id': video_id,
                        'video_title': video_title,
                        'author_name': snippet['authorDisplayName'],
                        'author_channel_id': snippet.get('authorChannelId', {}).get('value', ''),
                        'comment_text_original': decode_html_entities(snippet['textDisplay']),
                        'like_count': snippet.get('likeCount', 0),
                        'published_at': snippet['publishedAt'],
                        'is_reply': False,
                        'reply_count': item['snippet'].get('totalReplyCount', 0)
                    }

                    all_comments.append(comment)

                    # Processar replies se existirem
                    if 'replies' in item and item['replies'].get('comments'):
                        for reply_item in item['replies']['comments']:
                            reply_snippet = reply_item['snippet']
                            reply = {
                                'comment_id': reply_item['id'],
                                'video_id': video_id,
                                'video_title': video_title,
                                'author_name': reply_snippet['authorDisplayName'],
                                'author_channel_id': reply_snippet.get('authorChannelId', {}).get('value', ''),
                                'comment_text_original': decode_html_entities(reply_snippet['textDisplay']),
                                'like_count': reply_snippet.get('likeCount', 0),
                                'published_at': reply_snippet['publishedAt'],
                                'is_reply': True,
                                'parent_comment_id': item['id'],
                                'reply_count': 0
                            }
                            all_comments.append(reply)

                total_fetched = len(all_comments)

                # Verificar se tem pr√≥xima p√°gina e se ainda n√£o atingiu o limite
                next_page_token = data.get('nextPageToken')
                if not next_page_token or total_fetched >= max_results:
                    break

                # Limitar a 1000 coment√°rios por v√≠deo para n√£o sobrecarregar
                if total_fetched >= 1000:
                    logger.info(f"‚ö†Ô∏è Limite de 1000 coment√°rios atingido para v√≠deo {video_title[:30]}")
                    break

            logger.info(f"‚úÖ {total_fetched} coment√°rios coletados para: {video_title[:50]}")
            return all_comments

        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar coment√°rios do v√≠deo {video_id}: {e}")
            return []

    async def get_all_channel_comments(self, channel_id: str, canal_name: str, videos: List[Dict]) -> Dict[str, Any]:
        """
        Busca coment√°rios de todos os v√≠deos de um canal
        Otimizado para canais 'nossos' apenas
        """
        try:
            if not videos:
                return {'total_comments': 0, 'comments_by_video': {}}

            total_comments = 0
            comments_by_video = {}

            # Limitar aos v√≠deos mais recentes (√∫ltimos 30 dias ou top 20 v√≠deos)
            recent_videos = sorted(videos, key=lambda x: x.get('publishedAt', ''), reverse=True)[:20]

            logger.info(f"üìä Coletando coment√°rios de {len(recent_videos)} v√≠deos recentes de {canal_name}")

            for video in recent_videos:
                video_id = video.get('videoId')
                video_title = video.get('title', 'Sem t√≠tulo')

                if not video_id:
                    continue

                # Buscar coment√°rios (limitado a 500 por v√≠deo para economizar)
                comments = await self.get_video_comments(video_id, video_title, max_results=500)

                if comments:
                    comments_by_video[video_id] = {
                        'video_title': video_title,
                        'video_views': video.get('viewCount', 0),
                        'video_published': video.get('publishedAt'),
                        'comments': comments,
                        'total_count': len(comments)
                    }
                    total_comments += len(comments)

                # Pequena pausa entre v√≠deos para n√£o sobrecarregar
                await asyncio.sleep(0.5)

            logger.info(f"‚úÖ Total de {total_comments} coment√°rios coletados de {canal_name}")

            return {
                'canal_name': canal_name,
                'total_videos_analyzed': len(recent_videos),
                'total_comments': total_comments,
                'comments_by_video': comments_by_video
            }

        except Exception as e:
            logger.error(f"‚ùå Erro ao coletar coment√°rios do canal {canal_name}: {e}")
            return {'total_comments': 0, 'comments_by_video': {}}
