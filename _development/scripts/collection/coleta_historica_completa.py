"""
Script de Coleta Hist√≥rica Completa com Tradu√ß√£o
================================================
Este script:
1. Coleta TODOS os coment√°rios de TODOS os v√≠deos
2. Salva primeiro SEM traduzir
3. Depois traduz em lote usando o sistema existente

Autor: Claude Code para Cellibs
Data: 12/02/2026
"""

import os
import sys
import json
import time
import logging
import asyncio
import argparse
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Set
from dotenv import load_dotenv
from supabase import create_client, Client
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from urllib.parse import unquote

# Configurar encoding UTF-8 para Windows
sys.stdout.reconfigure(encoding='utf-8')

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('coleta_historica_completa.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Carregar vari√°veis de ambiente
load_dotenv()

# Configurar Supabase
supabase_url = os.getenv('SUPABASE_URL')
supabase_service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

if not supabase_url or not supabase_service_key:
    logger.error("‚ùå Credenciais Supabase n√£o configuradas!")
    sys.exit(1)

supabase: Client = create_client(supabase_url, supabase_service_key)

# Configurar YouTube API com rota√ß√£o de chaves
class YouTubeAPIManager:
    """Gerenciador de API Keys com rota√ß√£o autom√°tica"""
    def __init__(self):
        # Lista de todas as API keys dispon√≠veis
        self.api_keys = []

        # Keys 3 a 10
        for i in range(3, 11):
            key = os.getenv(f'YOUTUBE_API_KEY_{i}')
            if key:
                self.api_keys.append(key)

        # Keys 21 a 32
        for i in range(21, 33):
            key = os.getenv(f'YOUTUBE_API_KEY_{i}')
            if key:
                self.api_keys.append(key)

        if not self.api_keys:
            logger.error("‚ùå Nenhuma YouTube API Key configurada!")
            sys.exit(1)

        logger.info(f"‚úÖ {len(self.api_keys)} API Keys configuradas para rota√ß√£o")

        self.current_key_index = 0
        self.youtube = build('youtube', 'v3', developerKey=self.api_keys[0])
        self.requests_count = 0
        self.max_requests_per_key = 500  # Mudar de chave a cada 500 requests para distribuir uso

    def rotate_key(self):
        """Rotaciona para pr√≥xima API key"""
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        new_key = self.api_keys[self.current_key_index]
        self.youtube = build('youtube', 'v3', developerKey=new_key)
        logger.info(f"üîÑ Rotacionando para API Key {self.current_key_index + 1}/{len(self.api_keys)}")
        self.requests_count = 0

    def execute_request(self, request):
        """Executa request com tratamento de erro e rota√ß√£o de chave"""
        max_retries = len(self.api_keys)
        retry_count = 0

        while retry_count < max_retries:
            try:
                # Rotacionar preventivamente a cada N requests
                self.requests_count += 1
                if self.requests_count >= self.max_requests_per_key:
                    self.rotate_key()

                return request.execute()

            except HttpError as e:
                if e.resp.status == 403 and 'quotaExceeded' in str(e):
                    logger.warning(f"‚ö†Ô∏è Quota excedida na chave {self.current_key_index + 1}")
                    self.rotate_key()
                    retry_count += 1
                    if retry_count < max_retries:
                        logger.info(f"üîÑ Tentando com nova chave...")
                        # Reconstruir request com nova API
                        continue
                else:
                    raise e
            except Exception as e:
                logger.error(f"‚ùå Erro na requisi√ß√£o: {e}")
                raise e

        logger.error("‚ùå Todas as API keys est√£o com quota excedida!")
        raise Exception("Todas as API keys est√£o com quota excedida")

# Inicializar gerenciador de API
api_manager = YouTubeAPIManager()
youtube = api_manager.youtube

# Arquivo de checkpoint
CHECKPOINT_FILE = 'coleta_checkpoint.json'

# Estat√≠sticas globais
stats = {
    'canais_processados': 0,
    'videos_processados': 0,
    'comentarios_novos': 0,
    'comentarios_existentes': 0,
    'comentarios_para_traduzir': 0,
    'erros': 0,
    'inicio': datetime.now(timezone.utc)
}


def load_checkpoint() -> Dict:
    """Carrega checkpoint se existir"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_checkpoint(data: Dict):
    """Salva checkpoint"""
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)


def get_existing_comment_ids(canal_id: int) -> Set[str]:
    """Retorna IDs de coment√°rios j√° existentes"""
    logger.info(f"  üìä Verificando coment√°rios existentes...")

    existing_ids = set()
    offset = 0
    limit = 1000

    while True:
        try:
            response = supabase.table('video_comments').select('comment_id').eq(
                'canal_id', canal_id
            ).range(offset, offset + limit - 1).execute()

            if not response.data:
                break

            for comment in response.data:
                existing_ids.add(comment['comment_id'])

            if len(response.data) < limit:
                break

            offset += limit

        except Exception as e:
            logger.error(f"  ‚ùå Erro ao buscar IDs: {e}")
            break

    logger.info(f"  ‚úÖ {len(existing_ids)} coment√°rios j√° existem")
    return existing_ids


def get_channel_info(channel_url: str) -> Optional[str]:
    """Extrai channel ID da URL"""
    try:
        # Decodificar URL
        channel_url = unquote(channel_url)

        if '@' in channel_url:
            handle = channel_url.split('@')[-1].split('/')[0]
            request = api_manager.youtube.search().list(
                part="snippet",
                q=handle,
                type="channel",
                maxResults=1
            )
            response = api_manager.execute_request(request)

            if 'items' in response and response['items']:
                return response['items'][0]['snippet']['channelId']

        elif '/channel/' in channel_url:
            return channel_url.split('/channel/')[-1].split('/')[0]

    except Exception as e:
        logger.error(f"Erro ao extrair channel ID: {e}")

    return None


def get_all_channel_videos(channel_id: str) -> List[Dict]:
    """Busca TODOS os v√≠deos do canal"""
    videos = []
    next_page_token = None

    try:
        while True:
            request = api_manager.youtube.search().list(
                part="id,snippet",
                channelId=channel_id,
                type="video",
                maxResults=50,
                pageToken=next_page_token,
                order="date"
            )
            response = api_manager.execute_request(request)

            for item in response.get('items', []):
                videos.append({
                    'id': item['id']['videoId'],
                    'title': item['snippet']['title'],
                    'publishedAt': item['snippet']['publishedAt']
                })

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

            time.sleep(0.1)

    except Exception as e:
        logger.error(f"Erro ao buscar v√≠deos: {e}")

    return videos


def get_video_comments(video_id: str, max_results: int = 100) -> List[Dict]:
    """Busca at√© 100 coment√°rios de um v√≠deo"""
    comments = []
    next_page_token = None

    try:
        while len(comments) < max_results:
            request = api_manager.youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=min(100, max_results - len(comments)),
                order='time',
                pageToken=next_page_token
            )
            response = api_manager.execute_request(request)

            for item in response.get('items', []):
                comment = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'id': item['snippet']['topLevelComment']['id'],
                    'author': comment['authorDisplayName'],
                    'author_channel_id': comment['authorChannelId']['value'],
                    'text': comment['textDisplay'],
                    'published_at': comment['publishedAt'],
                    'like_count': comment.get('likeCount', 0),
                    'reply_count': item['snippet'].get('totalReplyCount', 0)
                })

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        return comments[:max_results]

    except HttpError as e:
        if e.resp.status == 403 and 'commentsDisabled' in str(e):
            logger.debug(f"    Coment√°rios desabilitados")
        else:
            logger.error(f"    Erro HTTP: {e}")
    except Exception as e:
        logger.error(f"    Erro: {e}")

    return []


def save_comments_batch(comments_data: List[Dict]) -> bool:
    """Salva lote de coment√°rios (INSERT apenas novos)"""
    if not comments_data:
        return True

    try:
        # INSERT (n√£o UPSERT) para garantir que n√£o sobrescreve
        response = supabase.table('video_comments').insert(comments_data).execute()

        if response.data:
            logger.info(f"    ‚úÖ {len(comments_data)} coment√°rios salvos")
            return True

    except Exception as e:
        # Se der erro de duplicata, tenta um por um
        if 'duplicate key' in str(e).lower():
            salvos = 0
            for comment in comments_data:
                try:
                    response = supabase.table('video_comments').insert(comment).execute()
                    if response.data:
                        salvos += 1
                except:
                    pass  # J√° existe, pular
            logger.info(f"    ‚úÖ {salvos}/{len(comments_data)} coment√°rios salvos")
            return salvos > 0
        else:
            logger.error(f"    ‚ùå Erro ao salvar: {e}")

    return False


def process_channel(canal: Dict, dry_run: bool = False) -> Dict:
    """Processa um canal - FASE 1: Apenas coleta, SEM tradu√ß√£o"""
    canal_id = canal['id']
    canal_nome = canal['nome_canal']
    canal_lingua = canal.get('lingua', 'unknown')
    canal_url = canal['url_canal']

    logger.info(f"\n{'='*80}")
    logger.info(f"üì∫ CANAL: {canal_nome} (ID: {canal_id})")
    logger.info(f"   Subnicho: {canal['subnicho']}")
    logger.info(f"   L√≠ngua: {canal_lingua}")
    logger.info(f"{'='*80}")

    canal_stats = {
        'videos': 0,
        'comentarios_novos': 0,
        'comentarios_existentes': 0,
        'precisa_traducao': 0
    }

    # 1. Buscar coment√°rios existentes
    existing_ids = get_existing_comment_ids(canal_id) if not dry_run else set()

    # 2. Extrair channel ID
    channel_id = get_channel_info(canal_url)
    if not channel_id:
        logger.error(f"‚ùå N√£o foi poss√≠vel extrair channel ID")
        return canal_stats

    # 3. Buscar TODOS os v√≠deos
    logger.info(f"  üîç Buscando v√≠deos...")
    videos = get_all_channel_videos(channel_id)
    logger.info(f"  ‚úÖ {len(videos)} v√≠deos encontrados")

    if not videos:
        return canal_stats

    # 4. Processar cada v√≠deo
    for i, video in enumerate(videos, 1):
        video_id = video['id']
        video_title = video['title'][:50]

        # Mostrar progresso
        if i % 10 == 0 or i == 1:
            logger.info(f"\n  üìπ [{i}/{len(videos)}] {video_title}...")

        # Buscar coment√°rios
        comments = get_video_comments(video_id, max_results=100)

        if not comments:
            continue

        # Processar apenas coment√°rios NOVOS
        new_comments = []
        for comment in comments:
            if comment['id'] in existing_ids:
                canal_stats['comentarios_existentes'] += 1
                continue

            # Preparar dados do coment√°rio
            comment_data = {
                'comment_id': comment['id'],
                'video_id': video_id,
                'canal_id': canal_id,
                'author_name': comment['author'],
                'author_channel_id': comment['author_channel_id'],
                'comment_text_original': comment['text'],
                'published_at': comment['published_at'],
                'collected_at': datetime.now(timezone.utc).isoformat(),
                'like_count': comment['like_count'],
                'reply_count': comment['reply_count'],
                'is_reply': False,
                'parent_comment_id': None  # Campo correto √© parent_comment_id
                # Removido canal_lingua - n√£o existe na tabela!
            }

            # FASE 1: Salvar SEM tradu√ß√£o
            if canal_lingua == 'portuguese':
                # Portugu√™s: copiar original
                comment_data['comment_text_pt'] = comment['text']
                comment_data['is_translated'] = True
            else:
                # Outros idiomas: marcar para traduzir depois
                comment_data['comment_text_pt'] = None  # Ser√° traduzido na FASE 2
                comment_data['is_translated'] = False
                canal_stats['precisa_traducao'] += 1

            new_comments.append(comment_data)
            canal_stats['comentarios_novos'] += 1

        # Salvar em lotes de 50
        if len(new_comments) >= 50:
            if not dry_run:
                success = save_comments_batch(new_comments[:50])
                if success:
                    for c in new_comments[:50]:
                        existing_ids.add(c['comment_id'])
            new_comments = new_comments[50:]

        canal_stats['videos'] += 1

        # Checkpoint peri√≥dico
        if i % 20 == 0 and not dry_run:
            checkpoint = {
                'canal_id': canal_id,
                'video_index': i,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            save_checkpoint(checkpoint)

        # Pausa pequena
        time.sleep(0.2)

    # Salvar coment√°rios restantes
    if new_comments and not dry_run:
        save_comments_batch(new_comments)

    logger.info(f"\n  üìä RESUMO - {canal_nome}:")
    logger.info(f"     V√≠deos: {canal_stats['videos']}")
    logger.info(f"     Novos: {canal_stats['comentarios_novos']}")
    logger.info(f"     Existentes: {canal_stats['comentarios_existentes']}")
    logger.info(f"     Para traduzir: {canal_stats['precisa_traducao']}")

    return canal_stats


async def traduzir_comentarios_pendentes():
    """FASE 2: Traduz todos os coment√°rios pendentes"""
    logger.info("\n" + "="*80)
    logger.info("üåç FASE 2: TRADU√á√ÉO DOS COMENT√ÅRIOS")
    logger.info("="*80)

    # Importar o sistema de tradu√ß√£o existente
    try:
        from main import traduzir_comentarios_canal

        # Buscar canais com coment√°rios n√£o traduzidos
        response = supabase.table('canais_monitorados').select(
            'id, nome_canal, lingua'
        ).eq('tipo', 'nosso').neq('lingua', 'portuguese').execute()

        canais_para_traduzir = response.data

        logger.info(f"\nüìã {len(canais_para_traduzir)} canais para traduzir")

        for canal in canais_para_traduzir:
            # Contar coment√°rios n√£o traduzidos
            count_response = supabase.table('video_comments').select(
                'id', count='exact'
            ).eq('canal_id', canal['id']).eq('is_translated', False).execute()

            if count_response.count > 0:
                logger.info(f"\nüîÑ Traduzindo {count_response.count} coment√°rios de {canal['nome_canal']}...")

                # Usar fun√ß√£o existente do sistema
                await traduzir_comentarios_canal(canal['id'])

                logger.info(f"‚úÖ Tradu√ß√£o conclu√≠da para {canal['nome_canal']}")

        logger.info("\n‚úÖ FASE 2 CONCLU√çDA: Todos os coment√°rios traduzidos!")

    except ImportError:
        logger.warning("‚ö†Ô∏è Sistema de tradu√ß√£o n√£o dispon√≠vel localmente")
        logger.info("Execute no Railway: POST /api/traduzir-comentarios-todos")


def main():
    """Fun√ß√£o principal"""
    parser = argparse.ArgumentParser(description='Coleta Hist√≥rica Completa')
    parser.add_argument('--canal', type=str, help='Nome do canal espec√≠fico')
    parser.add_argument('--todos', action='store_true', help='Processar TODOS os canais')
    parser.add_argument('--dry-run', action='store_true', help='Modo teste (n√£o salva)')
    parser.add_argument('--traduzir', action='store_true', help='Executar tradu√ß√£o ap√≥s coleta')

    args = parser.parse_args()

    logger.info("\n" + "="*80)
    logger.info("üöÄ COLETA HIST√ìRICA COMPLETA - INICIANDO")
    logger.info("="*80)

    if args.dry_run:
        logger.warning("‚ö†Ô∏è MODO DRY-RUN: Nada ser√° salvo!")

    # Buscar canais
    if args.canal:
        response = supabase.table('canais_monitorados').select('*').eq(
            'tipo', 'nosso'
        ).eq('nome_canal', args.canal).execute()
        canais = response.data
    elif args.todos:
        response = supabase.table('canais_monitorados').select('*').eq(
            'tipo', 'nosso'
        ).order('subnicho').execute()
        canais = response.data
    else:
        logger.error("‚ùå Use --canal NOME ou --todos")
        return

    logger.info(f"üìã {len(canais)} canais para processar\n")

    # FASE 1: COLETA
    logger.info("="*80)
    logger.info("üì• FASE 1: COLETA DE COMENT√ÅRIOS")
    logger.info("="*80)

    # Processar cada canal
    for i, canal in enumerate(canais, 1):
        logger.info(f"\n[{i}/{len(canais)}] Processando...")

        try:
            canal_stats = process_channel(canal, dry_run=args.dry_run)

            stats['canais_processados'] += 1
            stats['videos_processados'] += canal_stats['videos']
            stats['comentarios_novos'] += canal_stats['comentarios_novos']
            stats['comentarios_existentes'] += canal_stats['comentarios_existentes']
            stats['comentarios_para_traduzir'] += canal_stats['precisa_traducao']

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è Interrompido pelo usu√°rio!")
            break
        except Exception as e:
            logger.error(f"‚ùå Erro: {e}")
            stats['erros'] += 1

    # FASE 2: TRADU√á√ÉO (se solicitado)
    if args.traduzir and not args.dry_run:
        asyncio.run(traduzir_comentarios_pendentes())

    # Relat√≥rio final
    tempo_total = datetime.now(timezone.utc) - stats['inicio']
    logger.info("\n" + "="*80)
    logger.info("üìä RELAT√ìRIO FINAL")
    logger.info("="*80)
    logger.info(f"‚è±Ô∏è Tempo: {tempo_total}")
    logger.info(f"üì∫ Canais: {stats['canais_processados']}")
    logger.info(f"üé¨ V√≠deos: {stats['videos_processados']}")
    logger.info(f"‚úÖ Novos: {stats['comentarios_novos']}")
    logger.info(f"‚è≠Ô∏è Existentes: {stats['comentarios_existentes']}")
    logger.info(f"üåç Para traduzir: {stats['comentarios_para_traduzir']}")
    logger.info(f"‚ùå Erros: {stats['erros']}")

    if not args.dry_run and os.path.exists(CHECKPOINT_FILE):
        os.remove(CHECKPOINT_FILE)
        logger.info("üóëÔ∏è Checkpoint removido")

    logger.info("\n‚úÖ COLETA CONCLU√çDA!")

    if stats['comentarios_para_traduzir'] > 0 and not args.traduzir:
        logger.info("\nüí° DICA: Execute com --traduzir para traduzir os coment√°rios")


if __name__ == "__main__":
    main()